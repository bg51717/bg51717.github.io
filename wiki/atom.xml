<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Blogs</title>
  
  
  <link href="https://bg51717.github.io/wiki/atom.xml" rel="self"/>
  
  <link href="https://bg51717.github.io/wiki/"/>
  <updated>2024-11-04T12:53:21.660Z</updated>
  <id>https://bg51717.github.io/wiki/</id>
  
  <author>
    <name>bg51717</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>梯度估计STE</title>
    <link href="https://bg51717.github.io/wiki/13277/"/>
    <id>https://bg51717.github.io/wiki/13277/</id>
    <published>2024-11-04T12:15:47.000Z</published>
    <updated>2024-11-04T12:53:21.660Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景">背景</h2><p>反向传播是现在训练模型的重要方法，但是在部分场景下，会遇到不可微分的函数，从而导致梯度传播失败。比如量化里的取整函数。因此，需要对梯度进行估计然后反向传播。</p><p><strong>STE(Straight-Through Estimator)</strong>是2013年YoshuaBengio等人针对梯度估计进行了研究，那篇论文提出了几种梯度估计的方法，并推导出了一些理论性质，然后通过实验证明，STE是效果最好的方法。</p><blockquote><p>由于那篇论文很多篇幅在介绍较为复杂的估计方法，且理论推导也极为复杂，效果没有简洁的STE好，因此不对其进行详细介绍。</p></blockquote><h2 id="应用">应用</h2><h3 id="恒等函数">恒等函数</h3><p>STE在反向传播理论推导的时候，把不可微的原子函数（比如量化函数里有放缩和取整两部分，其中取整是不可微的原子函数）替换为恒等函数。</p><p>这种应用可以大大减少理论推导的难度，但是在代码里应用反向传播的时候不太方便，以pytorch框架为例，可能需要自己手写函数类<code>torch.autograd.Function</code>，具体文档可以查看：<a href="https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html">PyTorch:Defining New autograd Functions — PyTorch Tutorials 2.5.0+cu124documentation</a> 。</p><h3 id="sg函数">SG函数</h3><p>苏神在他的博客（<a href="https://spaces.ac.cn/archives/6760">VQ-VAE的简明介绍：量子化自编码器- 科学空间|Scientific Spaces</a>）中提出了一个函数<code>sg(stop gradient)</code>，代表梯度反向传播终止的恒等函数。</p><p>所以原来的不可微的原子函数可以写为<span class="math inline">\(f(x)=x+sg(x'-x)\)</span>，在前向传播的时候和x'相同，在反向传播的时候梯度等于直接对<code>x</code>反向传播梯度。</p><p>这个式子也能用于理论推导，但是不如视为恒等函数麻烦，但是在代码方面容易完成，可以使用pytorch的默认反向传播函数，不需要自定义<code>torch.autograd.Function</code>（当然自定义也可以完成任务）。</p><p><a href="https://github.com/kyegomez/BitNet/blob/main/bitnet/bitlinear.py">BitNet/bitnet/bitlinear.pyat main · kyegomez/BitNet</a> 代码示例：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># w_quant 是 w 量化（不可微）后的值</span></span><br><span class="line"><span class="comment"># STE using detach</span></span><br><span class="line">w_quant = w + (weight_quant(w) - w).detach()</span><br></pre></td></tr></tbody></table></figure><h2 id="参考资料">参考资料</h2><blockquote><ul><li><a href="https://arxiv.org/abs/1308.3432">[1308.3432] Estimating orPropagating Gradients Through Stochastic Neurons for ConditionalComputation</a></li><li><a href="https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html">PyTorch:Defining New autograd Functions — PyTorch Tutorials 2.5.0+cu124documentation</a></li><li><a href="https://spaces.ac.cn/archives/6760">VQ-VAE的简明介绍：量子化自编码器- 科学空间|Scientific Spaces</a></li><li><a href="https://github.com/kyegomez/BitNet">kyegomez/BitNet:Implementation of "BitNet: Scaling 1-bit Transformers for Large LanguageModels" in pytorch</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;
&lt;p&gt;反向传播是现在训练模型的重要方法，但是在部分场景下，会遇到不可微分的函数，从而导致梯度传播失败。比如量化里的取整函数。因此，需要对梯度进行估计然后反向传播。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;STE(Straight-Through Es</summary>
      
    
    
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="经典模块" scheme="https://bg51717.github.io/wiki/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9D%97/"/>
    
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="梯度估计" scheme="https://bg51717.github.io/wiki/tags/%E6%A2%AF%E5%BA%A6%E4%BC%B0%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>Welcome to bg51717&#39;s Wiki and Blog</title>
    <link href="https://bg51717.github.io/wiki/14261/"/>
    <id>https://bg51717.github.io/wiki/14261/</id>
    <published>2024-11-02T02:55:49.874Z</published>
    <updated>2024-11-02T02:55:49.874Z</updated>
    
    <content type="html"><![CDATA[<p>这是bg51717的个人Wiki和Blog站点，主要是把知识系统的罗列出来以及存放一些特殊bug的处理，当然也会更一些游戏或者二次元相关东西，也希望在成长的过程中可以认识许多志同道合的人。</p><p>本人一直认为互联网的开源是社会发展的重要原因之一，因为开源使得技术知识和解决问题的经验可以被记录和传承下去，很多时候在需要的时候可以被人们所发掘。</p><p>也希望可以通过博客让自己的<strong>思维有条理</strong>。很多时候我喜欢观察别人的思路，发现其实人与人的很多思路差距可能没有那么多。除开经验上的差别，很多人能成功的做成一件事很多原因是思维非常有条理，时时刻刻明白自己的应该做什么，下一步思路是什么。不会让自己的思维局限在某个步骤或者门槛上。从而即使在逆境中，也能实现把烂牌打出最好的效果。</p><p>在偶尔反思自己的不足的时候，深刻的发现<strong>拖延症是致命的</strong>，很多事情只要按照条理有计划的进行，结果其实都可以完成。但是拖延容易导致事情出现计划外的变故，进而导致完成的质量。这对于个人的成长来说是极为不利的。很早以前就看到了<strong>知行合一</strong>这个词，但是一直没有理解其重要性。后来发现，缺少行动力也会导致很多计划的失败。很多事物是需要我们用心去做，而不是用敷衍的态度去进行。在实践中不断地提升自己，革新自己。</p><p>不知道此刻在阅读这个博客的你是在何时何地打开的，也不知道你是为了探究什么而点开了这个链接。但是祝你身体健康，万事如意。大家一起学习，互相交流，共同成长！</p><p>最后，附上我喜欢的一句话：</p><blockquote><p>世界靠现实主义者维系，靠理想主义者发展。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这是bg51717的个人Wiki和Blog站点，主要是把知识系统的罗列出来以及存放一些特殊bug的处理，当然也会更一些游戏或者二次元相关东西，也希望在成长的过程中可以认识许多志同道合的人。&lt;/p&gt;
&lt;p&gt;本人一直认为互联网的开源是社会发展的重要原因之一，因为开源使得技术知识</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</title>
    <link href="https://bg51717.github.io/wiki/14592/"/>
    <id>https://bg51717.github.io/wiki/14592/</id>
    <published>2024-10-27T05:35:56.000Z</published>
    <updated>2024-11-02T06:12:26.130Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p>本篇博客介绍论文<a href="https://arxiv.org/abs/2306.00978">[2306.00978] AWQ:Activation-aware Weight Quantization for LLM Compression andAcceleration</a>提出的一种针对权重的训练后量化方法。该方法基于一个发现：<strong>模型的权重不是同等重要的，只保护1%的突出权重可以大大减少量化误差</strong>。但是混合精度会导致算法对硬件不友好，具体保护的方法是对模型的权重进行扩大，输入也进行对应的缩小。具体的比例根据激活通过网格搜索获得。由于没有进行反向传播，因此量化模型在与校准数据集不同分布的数据上也能表现良好。并最终实现了推理加速效果。</p><p>下图是PTQ大致流程图，和GPTQ算法一样，AWQ算法针对的主要是量化步骤中的参数调整部分：</p><p><img src="/wiki/wiki/14592/1730525025940.png"></p><h2 id="发现">发现</h2><p>首先介绍作者根据经验提出的假设：<strong>模型的权重不是同等重要的，只保护1%的突出权重可以大大减少量化误差</strong>。下图是示例：</p><p><img src="/wiki/wiki/14592/1730009795566.png"></p><p>从图里可以看出，在某一层layer的输入中，不同token的异常维度应该是集中在某几个维度中的。在LLM.int8中也有类似的结论，并且因此设计了llm.int8算法：</p><p><img src="/wiki/wiki/14592/1730258455859.png"></p><p>为了验证这个假设，作者进行了实验，尝试对少部分显著的权重保持原来精度，别的权重量化为int3，然后进行了多组测试。RTN是直接量化，不做调整，作为对比实验。作者分别尝试了使用权重的大小，激活的大小以及随机选择来确定哪些权重的显著的，结果发现根据激活选择的效果最好。</p><p><img src="/wiki/wiki/14592/1730010346827.png"></p><h2 id="方法">方法</h2><p>根据发现，只需要根据激活选择少部分权重保持为高精度即可实现优秀的量化算法，但是混合精度是硬件不友好的操作。因此，作者选择别的方法来保护这些权重：<strong>通过激活感知缩放来保护显着权重</strong>。</p><p>考虑权重 <span class="math inline">\(\mathbf{w}\)</span>，线性层操作可以记为<span class="math inline">\(y = \mathbf{w} \mathbf{x}\)</span>,对应的量化操作（包含反量化）为<span class="math inline">\(y =Q(\mathbf{w}) \mathbf{x}\)</span>，<span class="math inline">\(N\)</span>是量化的比特数，<span class="math inline">\(\Delta\)</span>是缩放因子，量化函数为:</p><p><span class="math display">\[Q(\mathbf{w}) = \Delta \cdot\text{Round}\left(\frac{\mathbf{w}}{\Delta}\right), \quad \Delta =\frac{\max(|\mathbf{w}|)}{2^{N-1}}\]</span></p><p>考虑针对权重中的一个向量 $w $ 乘以 $s &gt; 1 $ ，然后按同等比例缩小$x $, 我们有 $Q(w s)(x / s) $， 具体为:</p><p><span class="math display">\[Q(w \cdot s) \cdot \frac{x}{s} = \Delta' \cdot\text{Round}\left(\frac{w s}{\Delta}\right) \cdot x \cdot \frac{1}{s}\]</span></p><p><span class="math inline">\(\Delta'\)</span>是新的缩放因子。</p><p>这里解释一下为什么放缩针对是一行（这里假设乘号左边是X，右边是W）：</p><p><img src="/wiki/wiki/14592/1730271763378.png"></p><p>然后量化的时候，以列或者行作为量化函数的基本单位：</p><p><img src="/wiki/wiki/14592/1730272140732.png"></p><p>作者根据经验判断：</p><ul><li>量化为整数（Round函数）的时候，误差在0~0.5之间均匀分布，因此，平均误差为0.25。因此在对权重放大后，相对误差会减小，比如之前值是1，相对误差为0.25，1放大2倍后，相对误差只有0.125。</li><li>对部分<span class="math inline">\(w\)</span>进行放缩对量化时分组里的极值改变较少，因此<span class="math inline">\(\Delta' \approx \Delta\)</span></li><li>第二点的误差可以表示为<span class="math inline">\(\text{Err}' =\Delta' \cdot \text{RoundErr} \cdot\frac{1}{s}\)</span>，与原始误差的比例为<span class="math inline">\(\frac{\Delta'}{\Delta} \cdot\frac{1}{s}\)</span>。如果<span class="math inline">\(\Delta'\approx \Delta,s&gt;1\)</span>，那么就会降低误差</li></ul><p>然后作者选择了一部分参数进行了验证：</p><p><img src="/wiki/wiki/14592/1730011375988.png"></p><p>根据结果发现，在s=2的时候PPL达到了最小。在s不超过2的时候，基本符合根据经验得到的结论。</p><p>对于AWQ算法中的<span class="math inline">\(s\)</span>，作者认为这是一个最优化问题：</p><p><span class="math display">\[\mathbf{s}^* = \arg \min_{\mathbf{s}} \, \mathcal{L}(\mathbf{s}), \quad\mathcal{L}(\mathbf{s}) = \left\| Q(\mathbf{W} \cdot \mathbf{s})\left(\mathbf{s}^{-1} \cdot \mathbf{X}\right) - \mathbf{W} \mathbf{X}\right\|\]</span></p><p>由于量化函数不可微分，别的近似方法存在收敛不稳定的情况，作者最终根据激活使用网格搜索获得：</p><p><span class="math display">\[\mathbf{s} = \mathbf{s}_{\mathbf{X}}^{\alpha}, \quad \alpha^* = \arg\min_{\alpha} \mathcal{L}(\mathbf{s}_{\mathbf{X}}^{\alpha})\]</span></p><p><span class="math inline">\(\mathbf{s}_{\mathbf{X}}\)</span>和输入<span class="math inline">\(\mathbf{X}\)</span>有关，<span class="math inline">\(\alpha\)</span>搜索的范围在[0,1]之间。</p><p>作者通过实验验证了搜索的有效性：</p><p>此外，作者还进行了以下优化：</p><ul><li>对扩大化的权重进行裁剪</li><li>把对输入的缩小和前一个算子融合（比如，以前一个算子是矩阵乘法作为例子：前一个矩阵权重权重缩小<span class="math inline">\(s\)</span>，对前一个算子来说，扩大比例和缩小的比例大小不一样，并且可能方向也不一样，比如一个是列方向，另一个是行方向，因此不会完全抵消</li></ul><h2 id="实验">实验</h2><p>作者首先在LlaMa家族模型中进行了实验，比较在WikiText-2上的困惑度，发现AWQ的效果比GPTQ要好一点。</p><p><img src="/wiki/wiki/14592/1730012063950.png"></p><p>然后作者选取了80个样本问题，把量化前后模型的回答连接起来，让GPT-4打分判断哪个更好（交换次序后再重复一次）。结果也比之前的方法好。</p><p><img src="/wiki/wiki/14592/1730012673547.png"></p><p>作者还测试了在多模态大模型上表现：</p><p><img src="/wiki/wiki/14592/1730012744431.png"></p><p>还测试了在2比特量化下的表现，作者还提出AWQ和GPTQ是可以一起使用的：</p><p><img src="/wiki/wiki/14592/1730013136175.png"></p><p>然后作者罗列了一下推理加速的效果，可以看到每秒生成的token数目增长了很多：</p><p><img src="/wiki/wiki/14592/1730013299656.png"></p><p>作者认为，由于AWQ在使用校准数据进行量化的时候没有进行反向传播，因此可以过拟合校准集合：</p><p><img src="/wiki/wiki/14592/1730013388948.png"></p><h2 id="代码">代码</h2><p>论文源码：<a href="https://github.com/mit-han-lab/llm-awq">mit-han-lab/llm-awq:[MLSys 2024 Best Paper Award] AWQ: Activation-aware Weight Quantizationfor LLM Compression and Acceleration</a></p><p>算法工具包：<a href="https://github.com/AutoGPTQ/AutoGPTQ">AutoGPTQ/AutoGPTQ: Aneasy-to-use LLMs quantization package with user-friendly apis, based onGPTQ algorithm.</a></p><h2 id="参考资料">参考资料</h2><blockquote><ul><li><a href="https://arxiv.org/abs/2306.00978">[2306.00978] AWQ:Activation-aware Weight Quantization for LLM Compression andAcceleration</a></li><li><a href="https://github.com/mit-han-lab/llm-awq">mit-han-lab/llm-awq:[MLSys 2024 Best Paper Award] AWQ: Activation-aware Weight Quantizationfor LLM Compression and Acceleration</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;本篇博客介绍论文&lt;a href=&quot;https://arxiv.org/abs/2306.00978&quot;&gt;[2306.00978] AWQ:
Activation-aware Weight Quantization for LLM Com</summary>
      
    
    
    
    <category term="科研" scheme="https://bg51717.github.io/wiki/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文阅读" scheme="https://bg51717.github.io/wiki/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>拉格朗日乘数法解条件极值</title>
    <link href="https://bg51717.github.io/wiki/49444/"/>
    <id>https://bg51717.github.io/wiki/49444/</id>
    <published>2024-10-24T09:10:04.000Z</published>
    <updated>2024-11-02T02:55:49.880Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p>求解最优化是一类十分常见且难以求解的问题，因此，考虑开一个博客系统性的介绍一下重要解法：拉格朗日乘数法（LagrangeMultiplier Method）。之后再扩展到广义的拉格朗日乘数法。</p><p>拉格朗日乘数法的重点是<strong>在一些列约束条件下，构造包含隐藏条件的拉格朗日函数等同于优化的目标函数。</strong></p><blockquote><p>隐藏条件指的是在限制条件和拉格朗日函数共同作用下实际暗含的一些条件或方程。</p></blockquote><p>最优化问题通常是指对于给定的某一函数，<strong>求其在指定作用域上的全局最小值(因为最小值与最大值可以很容易转化，即最大值问题可以转化成最小值问题)</strong>。</p><h2 id="无约束条件">无约束条件</h2><p>对所有变量求偏导计算极值点后，把极值点代回原函数验证即可。</p><h2 id="等式约束条件">等式约束条件</h2><p>求一个函数在等式约束下的最值，即</p><p><span class="math display">\[\begin{aligned}    \min &amp; \quad f(\mathbf{x}) \\    \text{s.t.} &amp; \quad h_j(\mathbf{x}) = 0, \quad j = 1, 2, \dots,l\end{aligned}\]</span></p><p>定义拉格朗日函数<span class="math inline">\(F(x)\)</span>，其中<span class="math inline">\(\lambda_k\)</span>为引入的变量：</p><p><span class="math display">\[F(\mathbf{x}, \lambda) = f(\mathbf{x}) + \sum_{j=1}^{l} \lambda_jh_j(\mathbf{x})\]</span></p><p>求解极值点只需求解方程组：</p><p><span class="math display">\[\frac{\partial F}{\partial x_i} = 0 \\\frac{\partial F}{\partial \lambda_j} = 0\]</span></p><p>下面关于不等式约束下的最优值求解给了通用的证明过程，不过，维基百科列出了一个形象的解释：</p><p>考虑有个优化问题：</p><p><span class="math display">\[\begin{aligned}    \min &amp; \quad f(x, y) \\    \text{s.t.} &amp; \quad g(x, y) = c\end{aligned}\]</span></p><p><img src="/wiki/wiki/49444/1729772448154.png"></p><p>图上画的是两个函数的等高线，箭头表示梯度方向（暂时不考虑梯度正负）。绿色的线是约束条件，代表需要在绿色的线上寻找最优点。最优点显然在两个函数相切的地方，即两个函数梯度共线，<span class="math inline">\(\nabla f(x, y) = \lambda \left( \nabla g(x, y) - C\right)\)</span>（<span class="math inline">\(\nabla\)</span>代表梯度算子，<span class="math inline">\(\lambda\)</span>是非0实数），这就是等式约束下拉格朗日乘数法的<strong>隐藏条件</strong>。</p><p>从反证法考虑，假如两个梯度不共线，那么沿着约束函数的切向，即与约束函数梯度垂直的方向，一定可以找到一个向量，与约束函数梯度垂直且在目标函数梯度上有分量。那么沿着该向量方向修改变量，可以在约束函数值不变的条件下继续优化目标函数。</p><h2 id="不等式条件约束">不等式条件约束</h2><p>添加上不等式约束条件，就得到了广义拉格朗日问题，此时的最优化问题为，</p><p><span class="math display">\[\begin{aligned}    \min &amp; \quad f(\mathbf{\mathbf{x}}) \\    \text{s.t.} &amp; \quad h_j(\mathbf{\mathbf{x}}) = 0, \quad j = 1,2, \dots, p \\                 &amp; \quad g_k(\mathbf{\mathbf{x}}) \leq 0, \quad k =1, 2, \dots, q\end{aligned}\]</span></p><p>对应的拉格朗日函数L为，</p><p><span class="math display">\[L(\mathbf{x}, \lambda, \mu) = f(\mathbf{x}) + \sum_{j=1}^{p} \lambda_jh_j(\mathbf{x}) + \sum_{k=1}^{q} \mu_k g_k(\mathbf{x})\]</span></p><p>常用的方法是KKT条件，即最优值必须满足：</p><p><span class="math display">\[\begin{aligned}    \frac{\partial F}{\partial x_i} = 0 \quad &amp;(1)\\    \frac{\partial F}{\partial \lambda_j} = 0 \quad &amp;(2)\\    \lambda_j \neq 0 \quad &amp;(3)\\    \mu_k g_k(\mathbf{x}) = 0 \quad &amp;(4)\\    g_k(\mathbf{x}) \leq 0 \quad &amp;(5)\\    \mu_k \geq 0 \quad  &amp;(6)\end{aligned}\]</span></p><p>其中前三个式子是等式条件约束里的，后三个式子是不等式条件引入的。</p><h2 id="kkt推导">KKT推导</h2><blockquote><p><span class="math inline">\(\max_\mu L(x,\mu)\)</span>代表调整<span class="math inline">\(\mu\)</span>最大化目标函数<span class="math inline">\(L(x,\mu)\)</span>。</p></blockquote><p>首先，令（这里的几个变量都是多元变量，即向量，为了书写方便，没有引入<code>\mathbf</code>）</p><p><span class="math display">\[\ L(x, \lambda, \mu) = f(x) + \sum_{k=1}^{q} \mu_k g_k(x)\]</span></p><p>引入约束条件，</p><p><span class="math display">\[\because\begin{cases}    \mu_k \geq 0 \\    g_k(x) \leq 0\end{cases} \\\\\therefore \mu_k g_k(x) \leq 0 \\\therefore 根据非负得\ \max_\mu L(x,\mu)=f(x) \\\therefore \min_ x f(x) = \min_x \max_ \mu L(x,\mu)\]</span></p><p>所以我们发现调整<span class="math inline">\(\mu\)</span>最大化<span class="math inline">\(L\)</span>就等于<span class="math inline">\(f(x)\)</span>，所以</p><p><span class="math display">\[\min_ x f(x) = \min_x \max_ \mu L(x,\mu)\]</span></p><p>另一方面，</p><p><span class="math display">\[\max_\mu \min_x L(x, \mu) = \max_\mu \left[ \min_x f(x) + \min_x \mug(x) \right]= \max_\mu \min_x f(x) + \max_\mu \min_x \mu g(x)= \min_x f(x) + \max_\mu \min_x \mu g(x) \\又\because \begin{aligned}    \mu_k \geq 0, \quad g_k(x) \leq 0 \quad &amp;\Rightarrow \quad\min_x \mu g(x) = \begin{cases}        0, &amp; \text{if } \mu = 0 \text{ or } g(x) = 0 \\        -\infty, &amp; \text{if } \mu &gt; 0 \text{ and } g(x) &lt; 0    \end{cases}\end{aligned}\]</span></p><p>所以，引入约束条件<span class="math inline">\(\mug(X)=0\)</span>，我们得到，</p><p><span class="math display">\[\max_\mu \min_x \mu g(x) = 0 \\\therefore \max_\mu \min_x L(x, \mu) = \min_x f(x) + \max_\mu \min_x \mug(x) = \min_x f(x)\]</span></p><p>综上所述，</p><p><span class="math display">\[\begin{aligned}\begin{rcases}L(x, \lambda, \mu) &amp;= f(x) + \sum_{k=1}^{q} \mu_k g_k(x) \\\mu_k &amp;\geq 0 \\g_k(x) &amp;\leq 0 \\\mu g(X)&amp;=0\end{rcases} \end{aligned}\Rightarrow\max_\mu \min_x L(x, \mu)= \min_x \max_ \mu L(x,\mu)=\min_ x f(x)\]</span></p><p>引入等式约束的条件，我们得到了，</p><p><span class="math display">\[\begin{aligned}\begin{rcases}    \frac{\partial F}{\partial x_i} = 0\\    \frac{\partial F}{\partial \lambda_j} = 0\\    \lambda_j \neq 0\\    \mu_k g_k(\mathbf{x}) = 0\\    g_k(\mathbf{x}) \leq 0\\    \mu_k \geq 0\end{rcases} \end{aligned}\Rightarrow\max_\mu \min_x L(x, \mu)= \min_x \max_ \mu L(x,\mu)=\min_ x f(x)\]</span></p><p>所以我们完成了我们需要证明的：<strong>在一些列约束条件下，拉格朗日函数等同于优化的目标函数。</strong></p><p>补充KTT下的隐藏条件：</p><p><span class="math display">\[\frac{\partial L(x, \lambda, \mu)}{\partial x} \Bigg|_{x = x^*} = 0\quad \text{表明} \quad f(x) \text{在极值点} x^* \text{处的梯度是含有}h_j(x^*) \text{和} g_k(x^*) \text{梯度的线性组合。}\]</span></p><h2 id="参考资料">参考资料</h2><blockquote><ul><li><a href="https://zh.wikipedia.org/zh-cn/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0">拉格朗日乘数- 维基百科，自由的百科全书</a></li><li><a href="https://www.cnblogs.com/mo-wang/p/4775548.html">【整理】深入理解拉格朗日乘子法（LagrangeMultiplier) 和KKT条件 - mo_wang - 博客园</a></li><li><a href="https://blog.csdn.net/johnnyconstantine/article/details/46335763">KKT条件介绍-CSDN博客</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;求解最优化是一类十分常见且难以求解的问题，因此，考虑开一个博客系统性的介绍一下重要解法：拉格朗日乘数法（Lagrange
Multiplier Method）。之后再扩展到广义的拉格朗日乘数法。&lt;/p&gt;
&lt;p&gt;拉格朗日乘数法的重点是&lt;</summary>
      
    
    
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="前置数学" scheme="https://bg51717.github.io/wiki/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%89%8D%E7%BD%AE%E6%95%B0%E5%AD%A6/"/>
    
    
    <category term="数学" scheme="https://bg51717.github.io/wiki/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>大模型量化~GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</title>
    <link href="https://bg51717.github.io/wiki/31769/"/>
    <id>https://bg51717.github.io/wiki/31769/</id>
    <published>2024-10-24T08:12:19.000Z</published>
    <updated>2024-11-02T02:57:00.290Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p>GPTQ算法的原理从数学公式出发，推导出权重的量化顺序和其余参数的调整值，然后根据这些值对block里的所有参数以列为单位进行量化，每次量化可以量化多个列，同时调整其余未量化的列的参数减小量化误差。</p><p>GPTQ算法是只针对权重的量化方式。在计算的时候，对应的内核会把需要计算的权重还原回原来的数据类型（比如int4-&gt;fp16）从而保证计算的稳定。</p><p>下图是PTQ大致流程图，GPTQ算法针对的主要是量化步骤中的参数调整部分：、</p><p><img src="/wiki/wiki/31769/1730268555276.png"></p><p>GPTQ量化算法最早来自1990年YannLeCun的OBD算法，然后按照OBS，OBC（OBQ）的顺序进行演化，最终到GPTQ算法。当然，这里的演化算法不都是量化算法，由于量化和剪枝是很像的两个技术，量化调整参数为距离最近的整数，剪枝调整参数为0，所以量化算法和剪枝算法有很多共同点。</p><h2 id="obdoptimal-brain-damage">OBD：Optimal Brain Damage</h2><p>OBD是一种剪枝方法，考虑目标函数为<span class="math inline">\(E\)</span>，模型参数为<span class="math inline">\(w_i\)</span>，对目标函数进行泰勒展开，有</p><p><span class="math display">\[\Delta E = \sum_i g_i \Delta w_i + \frac{1}{2} \sum_i h_{ii} \Deltaw_i^2 + \frac{1}{2} \sum_{i \neq j} h_{ij} \Delta w_i \Delta w_j +O(\Delta w^3)\]</span></p><p>其中<span class="math inline">\(g_i = \frac{\partial E}{\partialw_i}\)</span>是一阶偏导，<span class="math inline">\(h_{ij} =\frac{\partial^2 E}{\partial w_i \partialw_j}\)</span>是海森矩阵元素。</p><p>OBD假设：</p><ul><li>目标函数为二阶，不考虑高阶项<span class="math inline">\(O(\Deltaw^3)\)</span></li><li>模型训练充分收敛，一阶偏导为0，<span class="math inline">\(g_i=0,\forall i\)</span></li><li>每个参数对目标函数影响是独立的，海森矩阵中的交叉项为0，即<span class="math inline">\(h_{ij} = 0, \forall i, j, i \neq j\)</span></li></ul><p>于是，上式可以优化为，</p><p><span class="math display">\[\Delta E = \frac{1}{2} \sum_i h_{ii} \Delta w_i^2\]</span></p><p>因此，OBD算法只需要根据每个参数对目标的影响从小到大进行排序，然后进行剪枝即可。</p><h2 id="obsoptimal-brain-surgeon">OBS：Optimal Brain Surgeon</h2><p><img src="/wiki/wiki/31769/1729833758876.png"></p><p>OBS算法认为参数之间的独立性不成立，因此交叉项需要考虑，因此上式变为，</p><p><span class="math display">\[\Delta E = \frac{1}{2} \sum_i h_{ii} \Delta w_i^2 + \frac{1}{2} \sum_{i\neq j} h_{ij} \Delta w_i \Delta w_j\]</span></p><p>向量，矩阵形式为，</p><p><span class="math display">\[\Delta E = \frac{1}{2} \Delta \mathbf{w}^\mathrm{T} \mathbf{H} \Delta\mathbf{w}\]</span></p><p>剪枝删除一个权重，那么<span class="math inline">\(\Delta\mathbf{w}\)</span>的第 q 维固定为 <span class="math inline">\(-w_q\)</span>，但其他维度的值不固定，可以用于减少删除该权重带来的目标偏离。即约束条件为，</p><p><span class="math display">\[\mathbf{e}_q^\mathrm{T} \cdot \Delta \mathbf{w} + w_q = 0\]</span></p><p>其中，<span class="math inline">\(\mathbf{e}_q\)</span>是one-hot向量，第q个位置是1，其余位置为0。</p><p>所以剪枝转化为最优化问题，</p><p><span class="math display">\[\min_{\Delta \mathbf{w}, q} \frac{1}{2} \Delta \mathbf{w}^\mathrm{T}\mathbf{H} \Delta \mathbf{w} \quad \text{s.t.} \quad\mathbf{e}_q^\mathrm{T} \cdot \Delta \mathbf{w} + w_q = 0\]</span></p><p>构造拉格朗日函数，</p><p><span class="math display">\[L = \frac{1}{2} \Delta \mathbf{w}^\mathrm{T} \mathbf{H} \Delta\mathbf{w} + \lambda (\mathbf{e}_q^\mathrm{T} \cdot \Delta \mathbf{w} +w_q)\]</span></p><p>求解，得</p><p><span class="math display">\[\Delta \mathbf{w} = -\frac{w_q}{[\mathbf{H}^{-1}]_{qq}} \mathbf{H}^{-1}\cdot \mathbf{e}_q \quad \text{and} \quad L = \frac{1}{2}\frac{w_q^2}{[\mathbf{H}^{-1}]_{qq}}\]</span></p><p>因此，我们可以根据海森矩阵求得下一个剪枝的参数和其他参数调整的值。</p><blockquote><p>每次更新参数都需要重新求解海森矩阵，原论文中提到了数学上的优化方法，但是较为复杂且在后续算法没有使用，此处不予列出。</p></blockquote><h2 id="obc">OBC</h2><p>由于需要求解全参数的海森矩阵甚至逆矩阵，复杂度极高，在参数规模上去后求解是不现实的。</p><p>因此假设：</p><ul><li>在一个矩阵中只有同一行的参数是相关的</li></ul><p>同时，为了更好的求解，对于一个维度为<span class="math inline">\((d_{row},d_{col})\)</span>参数矩阵设置一个优化的目标函数，<span class="math inline">\(\mathbf{W}_{i,\cdot},\hat{\mathbf{W_{i,\cdot}}}\)</span>分别是参数调整前后的权重向量</p><p><span class="math display">\[E = \sum_{i=1}^{d_{\text{row}}} \| \mathbf{W}_{i,\cdot} \mathbf{X} -\hat{\mathbf{W}}_{i,\cdot} \mathbf{X} \|_2^2\]</span></p><p>求每行的海森矩阵，</p><p><span class="math display">\[\frac{\partial E}{\partial \hat{\mathbf{W}}_{i,\cdot}^2} = \mathbf{H}_i= 2 \mathbf{X} \mathbf{X}^\mathrm{T}, \quad \forall i = 1, 2, \dots,d_{\text{row}}\]</span></p><p>之后就可以求得矩阵每一行的剪枝顺序。</p><p>算法伪代码：</p><p><img src="/wiki/wiki/31769/1729836598356.png"></p><p>首先遍历每一行，然后重复k次，每次找到对目标函数影响最小的参数p，对p进行剪枝同时更新其他参数，删除海森矩阵的p行p列（不包括<span class="math inline">\(H_{pp}\)</span>），再求逆。（这里罗列的好像是降低复杂度后的数学等效。）</p><blockquote><p>OBC论文也罗列了一些别的性能上的优化，在后续算法没有使用，此处不予列出。</p><p>此处数学等效的方法是高斯消元，海森矩阵移除k行k列但包括<span class="math inline">\(H_{kk}\)</span>，因为左乘矩阵代表对行向量线性组合对，右乘矩阵代表对列向量线性组合，因此通过高斯消元即可实现对0的操作。证明可以从GPTQ的Cholesky证明中基本无条件的迁移。</p></blockquote><h2 id="obq">OBQ</h2><p>OBQ（和OBC是同一篇文章）指出，剪枝是一种特殊的量化（即剪枝的参数等价于量化到0 点），因此只需要修改一下 OBC 的约束条件即可：</p><p><span class="math display">\[\mathbf{e}_p^\mathrm{T} \cdot \Delta \mathbf{w} + w_p =\text{quant}(w_p)\]</span></p><p>相应的，权重的调整更新为，</p><p><span class="math display">\[\Delta \mathbf{w} = -\frac{w_p -\text{quant}(w_p)}{[\mathbf{H}^{-1}]_{pp}} \mathbf{H}^{-1} \cdot\mathbf{e}_p\quad \text{and} \quadL = \frac{1}{2} \frac{(w_p -\text{quant}(w_p))^2}{[\mathbf{H}^{-1}]_{pp}}\]</span></p><p>OBQ的算法伪代码，同OBC的很相似：</p><p><img src="/wiki/wiki/31769/1729837217525.png"></p><h2 id="gptq">GPTQ</h2><p>OBQ的算法复杂度还是太高了，GPTQ继续进行优化。</p><p>GPTQ是逐层量化的，同时做出以下假设：</p><ul><li>参数调整前量化网络（量化函数）是确定的，比如量化的缩放因子和零点等</li><li>权重参数可以在量化网格内进行调整</li></ul><h3 id="按索引顺序量化">按索引顺序量化</h3><p>将每一行的量化权重选择方式从贪心策略改成按索引顺序选择。</p><p>修改索引顺序的好处有两个，首先多个行的量化顺序是一样的，因此可以多行并行处理；第二个是<span class="math inline">\(\mathbf{H} = 2 \mathbf{X}\mathbf{X}^\mathrm{T}\)</span>说明，海森矩阵只与输入相关，因此不同行可以共用一个海森矩阵。<img src="/wiki/wiki/31769/1729841730501.png"></p><p>行里第p个权重调整公式可以为改为，</p><p><span class="math display">\[\Delta \mathbf{w} = -\frac{w_p -\text{quant}(w_p)}{[\mathbf{H}_{p:,p:}^{-1}]_{0,0}} \left([\mathbf{H}_{p:,p:}^{-1}]_{:,0} \right)^\top\]</span></p><p>加入多行并行后变为，</p><p><span class="math display">\[\Delta W_{:,p:} = -\frac{W_{:,p:} -\text{quant}(W_{:,p:})}{[\mathbf{H}_{p:,p:}^{-1}]_{0,0}} \left([\mathbf{H}_{p:,p:}^{-1}]_{:,0} \right)^\top=-\frac{\left( [\mathbf{H}_{p:,p:}^{-1}]_{:,0}\right)^\top}{[\mathbf{H}_{p:,p:}^{-1}]_{0,0}} ( W_{:,p:} -\text{quant}(W_{:,p:}))\]</span></p><p>逆矩阵的更新公式变为，</p><p><span class="math display">\[[H_{p:,p:}]^{-1} = \left( [H_{p-1:,p-1:}]^{-1}  -\frac{1}{[H_{p-1:,p-1:}]^{-1}_{0,0}} [H_{p-1:,p-1:}]^{-1}_{:,0}[H_{p-1:,p-1:}]^{-1}_{0,:}\right)_{1:,1:}\]</span></p><h3 id="cholesky-分解">Cholesky 分解</h3><blockquote><p>关于Cholesky分解的数学介绍可以看：<a href="https://blog.csdn.net/xbinworld/article/details/104663481">三十分钟理解：矩阵Cholesky分解，及其在求解线性方程组、矩阵逆的应用_cholesky分解法求解线性方程组-CSDN博客</a></p></blockquote><p>实验过程中发现：在大规模参数矩阵上重复使用海森矩阵逆矩阵的更新公式会产生非正定的海森矩阵逆矩阵，原因可能是数值误差的积累。</p><p>作者对初始的<span class="math inline">\(H^{-1}\)</span>进行Cholesky分解，得到一个上三角矩阵<span class="math inline">\(T\)</span>，它的每一行刚好就等于使用更新公式得到的逆矩阵序列的第一行乘以一个常数，即，</p><p><span class="math display">\[C_p T_{p,p:} = \left[ H_{p:,p:} \right]^{-1}_{0,:}\]</span></p><p>图示，</p><p><img src="/wiki/wiki/31769/1729847766438.png"></p><p>所以，权重的调整更新为，</p><p><span class="math display">\[\Delta W_{:,p:} = - \frac{W_{:,p} - \text{quant}(W_{:,p})}{C_p T_{pp}}C_p T_{p,p:} = - \frac{W_{:,p} - \text{quant}(W_{:,p})}{T_{pp}} T_{p,p:}\]</span></p><p>发现很多资料都没有系统的罗列大致的推导过程，甚至论文基本也是一笔带过，因此考虑在此给出一些浅薄的证明（笔者不是数学系的。</p><p>首先介绍一下教程中的Cholesky分解：给定一个<span class="math inline">\(n\times n\)</span>的实数对称正定矩阵<span class="math inline">\(A\)</span>，存在一个对角元全为正数的下三角矩阵，是的<span class="math inline">\(A=LL^\top\)</span>成立。</p><p><span class="math display">\[\begin{align*}\mathbf{A} &amp;= \begin{bmatrix} a_{11} &amp; \mathbf{A}_{21}^T \\\mathbf{A}_{21} &amp; \mathbf{A}_{22} \end{bmatrix}, \quad\mathbf{L} = \begin{bmatrix} l_{11} &amp; 0 \\ L_{21} &amp; L_{22}\end{bmatrix}, \quad\mathbf{L}^T = \begin{bmatrix} l_{11} &amp; L_{21}^T \\ 0 &amp; L_{22}^T\end{bmatrix}\end{align*}\]</span></p><p>其中 $a_{11} $ 和 $ l_{11} $是一个标量， <span class="math inline">\(\mathbf{A}_{21}\)</span> 和 <span class="math inline">\(L_{21}\)</span> 是一个列向量，$<em>{22} $是一个 (n-1 ) 阶的方阵，而 $ L</em>{22} $ 是一个 ( n-1 )阶的下三角矩阵。我们有，</p><p><span class="math display">\[\begin{align*}\begin{bmatrix} a_{11} &amp; \mathbf{A}_{21}^T \\ \mathbf{A}_{21} &amp;\mathbf{A}_{22} \end{bmatrix}&amp;= \begin{bmatrix} l_{11} &amp; 0 \\ L_{21} &amp; L_{22}\end{bmatrix}\begin{bmatrix} l_{11} &amp; L_{21}^T \\ 0 &amp; L_{22}^T \end{bmatrix}&amp;= \begin{bmatrix} l_{11}^2 &amp; l_{11} L_{21}^T \\l_{11}L_{21}  &amp; L_{21} L_{21}^T + L_{22} L_{22}^T \end{bmatrix}\end{align*}\]</span></p><p>我们易得，</p><p><span class="math display">\[\begin{align*}l_{11} &amp;= \sqrt{a_{11}} \\[10pt]L_{21} &amp;= \frac{1}{l_{11}} \mathbf{A}_{21} \\[10pt]L_{22} L_{22}^T &amp;= \mathbf{A}_{22} - L_{21} L_{21}^T\end{align*}\]</span></p><p>其中<span class="math inline">\(l_{11}\)</span>和<span class="math inline">\(L_{21}\)</span>我们直接得到，对于<span class="math inline">\(L_{22}\)</span>，我们发现又是一个Cholesky分解的过程，递归求解即可。</p><p>我们根据公式可以发现，</p><p><span class="math display">\[(\mathbf{L}^\top)_{1,:}=\begin{bmatrix} l_{11} &amp; L^{\top}_{21} \end{bmatrix}= \frac{1}{\sqrt{a}} \begin{bmatrix} a_{11} &amp; \mathbf{A}^{\top}_{21}\end{bmatrix}= \frac{1}{\sqrt{a}}\mathbf{A}_{1,:}\]</span></p><p>所以我们证明了两个矩阵第一列的比例关系，根据分解的递推性和矩阵的对称性，我们就完整证明了这个结论。</p><h2 id="海森逆矩阵更新公式证明">海森逆矩阵更新公式证明</h2><p>首先我们需要证明<strong>在海森矩阵删除p行p列参数（不包括<span class="math inline">\(H_{pp}\)</span>）的时候，在海森矩阵上会发生类似的操作。</strong></p><p>首先，我们需要知道<span class="math inline">\((H^{-1})^\top=(H^T)^{-1}=H^{-1}\)</span>，即海森矩阵的逆矩阵是对称矩阵。</p><p><span class="math display">\[\begin{align*}\mathbf{H}= \begin{bmatrix} a &amp; \mathbf{z}^\top \\ \mathbf{z} &amp;\mathbf{A} \end{bmatrix}, \quad构造矩阵\mathbf{B}=\begin{bmatrix} \frac{1}{a} &amp; \mathbf{z}^\top \\\mathbf{z} &amp; \mathbf{A}^{-1} \end{bmatrix}\end{align*}\]</span></p><p>其中，<span class="math inline">\(\mathbf{H}\)</span>和<span class="math inline">\(\mathbf{B}\)</span>是n维的对称正定方阵，a<span class="math inline">\(是标量，\)</span><span class="math inline">\(z\)</span>为n-1维的零向量，那么</p><p><span class="math display">\[\mathbf{H} \times \mathbf{B}=\begin{bmatrix} a &amp; \mathbf{z}^\top \\ \mathbf{z} &amp; \mathbf{A}\end{bmatrix} \times\begin{bmatrix} \frac{1}{a} &amp; \mathbf{z}^\top \\ \mathbf{z} &amp;\mathbf{A}^{-1} \end{bmatrix}=\begin{bmatrix} 1 &amp; a\mathbf{z}^\top+\mathbf{z}^\top\mathbf{A}^{-1}\\ \frac{1}{a}\mathbf{z}+\mathbf{A}\mathbf{z} &amp;\mathbf{z}\mathbf{z}^\top+\mathbf{A}\mathbf{A}^{-1} \end{bmatrix}=\begin{bmatrix} 1 &amp; \mathbf{z}^\top \\ \mathbf{z} &amp;I_{n-1\times n-1} \end{bmatrix}=I_{n\times n}\]</span></p><p>说明，说明<span class="math inline">\(\mathbf{B}\)</span>就是<span class="math inline">\(\mathbf{H}\)</span>的逆矩阵，因此，得证。我们就可以直接对海森矩阵的逆矩阵进行删除参数即可。</p><p>接下来，我们需要证明删除行参数和列参数的公式。为了方便，假设我们有个n维的对称正定方阵<span class="math inline">\(\mathbf{A}\)</span>，令</p><p><span class="math display">\[\mathbf{B}=\mathbf{A}-\frac{1}{\mathbf{A}_{11}}\mathbf{A}_{:,1}\mathbf{A}_{1,:}\]</span></p><p>那么，</p><p><span class="math display">\[\mathbf{B}_{i,1}=\mathbf{A}_{i,1}-\frac{1}{\mathbf{A}_{11}}\mathbf{A}_{i1}\mathbf{A}_{11}=0,i\neq 1\]</span></p><p>所以<span class="math inline">\(\mathbf{B}\)</span>的第一列除了第一个元素全部为0，根据对称性，第一行也全部为0，即实现了对第一行第一列元素的删除。</p><blockquote><p>尽管这里推导的时候是针对第一行或者第一列的，但是很容易外推到任意位置。</p></blockquote><p>到此，我们证明了海森逆矩阵更新公式的正确性，分为两步：首先证明删除参数后海森逆矩阵的格式，然后根据格式证明了更新公式的正确性。</p><h3 id="lazy-batch-updates">Lazy Batch-Updates</h3><p>在量化某个参数矩阵的情况下，每次量化一个参数，其他所有未量化的参数都要按公式全都要更新一遍。如果每行的量化并行计算，那么每次更新过程就需要read + write 一次参数矩阵。如果参数矩阵的维度为<span class="math inline">\(k \timesk\)</span>，那么量化这个参数矩阵就需要读写 k 次参数，总共的 IO 量为<span class="math inline">\(k^3\)</span>个元素。当 k 比较大时（&gt;=4096），需要读写的元素就非常多了，运行时间大都被 IO 占据。</p><p><img src="/wiki/wiki/31769/1729848176700.png"></p><p><span class="math display">\[\Delta W_{:,p:} = -\frac{W_{:,p:} -\text{quant}(W_{:,p:})}{[\mathbf{H}_{p:,p:}^{-1}]_{0,0}} \left([\mathbf{H}_{p:,p:}^{-1}]_{:,0} \right)^\top=-\frac{\left( [\mathbf{H}_{p:,p:}^{-1}]_{:,0}\right)^\top}{[\mathbf{H}_{p:,p:}^{-1}]_{0,0}} ( W_{:,p:} -\text{quant}(W_{:,p:}))\]</span></p><p>将参数矩阵按每若干列划分为一个个 group，量化某一列时，group内的参数立即更新，而 group 后面的列只记录更新量，延迟更新。当一个 group的参数全部量化完成，再统一对后面的所有参数做一次更新。这就是 LazyBatch-Updates。根据更新公式我们可以发现，这个更新可以是可以合并同类项的，把一个group的第一项提出来求和，可以一次读写后面的所有group。</p><p><img src="/wiki/wiki/31769/1729848182654.png"></p><p>对应的算法伪代码为</p><p><img src="/wiki/wiki/31769/1729922822263.png"></p><h2 id="实验">实验</h2><p>1.小模型测试</p><p>首先在ResNet上面进行了测试。选择AdaQuant、基于贪心策略的OBQ在精度上保持持平。相比量化前的模型来说，性能也没有下降太多。</p><p><img src="/wiki/wiki/31769/1729943142457.png"></p><p>然后在小型语言模型上（这是OBQ能使用的最大的模型之一）进行了测试。在4bit得分比之前的方法稍微差一点，在3bit要低的多一点。</p><p><img src="/wiki/wiki/31769/1729943292624.png"></p><p>但是量化需要时间从1h降到1min。</p><p>2.测试了量化时间</p><p>提供了一个参考的标准：</p><p>"For reference, the straight-through based method ZeroQuant-LKD (Yaoet al., 2022) reports a 3 hour runtime (on the same hardware) for a 1.3Bmodel, which would linearly extrapolate to several hundred hours (a fewweeks) for 175B models."</p><p><img src="/wiki/wiki/31769/1729943743510.png"></p><p>3.测试了量化模型在文本生成任务的表现</p><p>和不进行优化的直接量化（RTN）进行了对比。</p><p><img src="/wiki/wiki/31769/1729944238282.png"></p><p>4.测试了OPT-175B文本生成的效率</p><p>开发了相关kernel（推理时权重会反量化），量化只针对权重，激活不量化。</p><p><img src="/wiki/wiki/31769/1729944960522.png"></p><p>5.零样本任务评测</p><p>测试了和基线相比，在零样本任务上的准确率。</p><p><img src="/wiki/wiki/31769/1729945727934.png"></p><p>该方法很容易和别的量化网络相结合。量化网络指的是量化的操作，是权重调整后的具体量化步骤，需要确定比如缩放因子，零点，量化的粒度等。</p><h2 id="代码解析">代码解析</h2><p>论文对应的代码仓库为：<a href="https://github.com/IST-DASLab/gptq">IST-DASLab/gptq: Code for theICLR 2023 paper "GPTQ: Accurate Post-training Quantization of GenerativePretrained Transformers".</a></p><p>基于改算法开发的工具包：<a href="https://github.com/AutoGPTQ/AutoGPTQ">AutoGPTQ/AutoGPTQ: Aneasy-to-use LLMs quantization package with user-friendly apis, based onGPTQ algorithm.</a></p><h2 id="参考资料">参考资料</h2><blockquote><ul><li><a href="https://zhuanlan.zhihu.com/p/646210009">QLoRA、GPTQ：模型量化概述- 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/690834228">LLM 推理加速技术 ——GPTQ 量化技术演进 - 知乎</a></li><li><a href="https://arxiv.org/abs/2210.17323">[2210.17323] GPTQ:Accurate Post-Training Quantization for Generative Pre-trainedTransformers</a></li><li><a href="https://github.com/AutoGPTQ/AutoGPTQ">AutoGPTQ/AutoGPTQ: Aneasy-to-use LLMs quantization package with user-friendly apis, based onGPTQ algorithm.</a></li><li><a href="https://github.com/IST-DASLab/gptq">IST-DASLab/gptq: Codefor the ICLR 2023 paper "GPTQ: Accurate Post-training Quantization ofGenerative Pretrained Transformers".</a></li><li><a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=17c0a7de3c17d31f79589d245852b57d083d386e">OptimalBrain Damage</a></li><li><a href="https://blog.csdn.net/xbinworld/article/details/104663481">三十分钟理解：矩阵Cholesky分解，及其在求解线性方程组、矩阵逆的应用_cholesky分解法求解线性方程组-CSDN博客</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;GPTQ算法的原理从数学公式出发，推导出权重的量化顺序和其余参数的调整值，然后根据这些值对block里的所有参数以列为单位进行量化，每次量化可以量化多个列，同时调整其余未量化的列的参数减小量化误差。&lt;/p&gt;
&lt;p&gt;GPTQ算法是只针对</summary>
      
    
    
    
    <category term="科研" scheme="https://bg51717.github.io/wiki/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文阅读" scheme="https://bg51717.github.io/wiki/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="量化" scheme="https://bg51717.github.io/wiki/tags/%E9%87%8F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>llm.int8</title>
    <link href="https://bg51717.github.io/wiki/21264/"/>
    <id>https://bg51717.github.io/wiki/21264/</id>
    <published>2024-10-24T07:39:30.000Z</published>
    <updated>2024-11-02T02:57:00.281Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>todo：本文还在施工中.......</p></blockquote><h2 id="介绍">介绍</h2><p><code>llm.int8</code>是第一批针对大模型进行量化的算法，并且其算法也被集成在<code>bitsandbytes</code>库中，该库也已经被<code>huggingface</code>集成到代码库当中作为最基本的量化算法之一。</p><p>论文地址为：<a href="https://arxiv.org/abs/2208.07339">[2208.07339]LLM.int8(): 8-bit Matrix Multiplication for Transformers atScale</a></p><p>对应的源码为：<a href="https://github.com/bitsandbytes-foundation/bitsandbytes">bitsandbytes-foundation/bitsandbytes:Accessible large language models via k-bit quantization forPyTorch.</a></p><h2 id="摘要">摘要</h2><p>LLM.in8()量化针对的是transformer的FNN模块和Attn模块的矩阵乘法。</p><p>该量化方法把矩阵中的异常值抽出来保持16位精度，对于其余值量化为8位精度，计算的时候恢复16精度。量化常数的选择是在向量维度进行的（矩阵最后维度）。</p><h2 id="介绍-1">介绍</h2><p>FFN和Attn的矩阵参数在模型总参数占比较高，且矩阵乘法占据了较多的计算资源。以往的量化方法降低了模型的表现并且需要量化后训练。同时没有对超过350M参数进行量化研究。这篇论文第一次提出针对百万级参数量的模型的量化方法且模型性能下降有限。应该也是第一篇把大模型量化到8比特的论文。</p><h2 id="量化前置知识">量化前置知识</h2><p>论文这里简单说明了非对称量化理论上能提供更高的精度，但是由于实际限制，用的最多的还是对称量化。</p><h3 id="对称量化">对称量化</h3><p>对于fp16格式的输入<span class="math inline">\(\mathbf{X}_{f16} \in\mathbb{R}^{s \times h}\)</span>，8比特的量化为：</p><p><span class="math display">\[\mathbf{X}_{i8} = \left\lfloor \frac{127 \cdot\mathbf{X}_{f16}}{\max_{ij} \left( |\mathbf{X}_{f16_{ij}}| \right)}\right\rceil= \left\lfloor \frac{127}{\|\mathbf{X}_{f16}\|_{\infty}}\mathbf{X}_{f16} \right\rceil= \left\lfloor s_{x_{f16}} \mathbf{X}_{f16} \right\rceil\]</span></p><p>其中<span class="math inline">\(\left\lfloor  \right\rceil\)</span>，代表四舍五入取整，即距离最近的整数。</p><h2 id="原理介绍">原理介绍</h2><p>这个量化的方法原理很简单，该方案先做了一个矩阵分解，对绝大部分权重和激活用8bit量化（<strong>vector</strong>-wise）。对离群特征的几个维度保留16bit，对其做高精度的矩阵乘法。</p><p><img src="/wiki/wiki/21264/1729756835272.png"></p><p>LLM.int8() 通过三个步骤完成矩阵乘法计算:</p><ol type="1"><li>从输入的隐含状态中，按列提取异常维度(离群特征，即大于某个阈值的值)。</li><li>对离群特征进行 FP16 矩阵运算，对非离群特征进行量化，做 INT8矩阵运算；</li><li>反量化非离群值的矩阵乘结果，并与离群值矩阵乘结果相加，获得最终的FP16 结果。</li></ol><h2 id="原理分析">原理分析</h2><p>这个方法能成功的一个重要原因是作者发现<strong>同一个序列的不同token异常维度集中在部分维度且异常维度数目较少</strong>。</p><p>首先，论文定义异常值的标准为：</p><ul><li>值需要大于等于6</li><li>影响25%的层</li><li>至少出现一个序列在6%的token中</li></ul><p>作者对此进行了解释：</p><ul><li>实验发现把大于等于6的值设置为异常值，困惑度退化就会停止</li><li>异常值特征是在大模型中系统出现的，要么出现在大多数层中，要么不出现；在小模型中概率出现。设置该阈值保证125M的最小模型中异常值只有一个（小模型中第二个出现最多的异常值仅仅出现2%的层）</li><li>使用和第二点相同的过程来选择异常值在序列维度的最小出现比例</li></ul><p>作者进行了实验，将满足条件的维度设置为0，比较最高可能性输出类别所分配的Softmax概率值。对比是随机选择相同数目的维度进行对比。</p><p><img src="/wiki/wiki/21264/1729962190147.png"></p><h2 id="实验">实验</h2><p>todo：量化效果</p><p><img src="/wiki/wiki/21264/1729961938672.png"></p><p>todo：加速效果</p><p>由于多余的反量化操作，在小模型上推理速度有所下降。</p><p><img src="/wiki/wiki/21264/1729962553598.png"></p><h2 id="bitsandbytes介绍">bitsandbytes介绍</h2><p>todo：介绍库的大致使用方法和部分疑点。</p><p><code>bitsandbytes</code>库在量化模型的时候，不需要数据首先直接把权重量化为8bit。在推理的时候，根据输入确定异常维度。输入的异常维度保持fp16，别的维度量化为int8。而量化后的权重，会把异常维度进行反量化为fp16（可能会有轻微损失），别的保持int8。之后的计算就和原理图保持一致。</p><h2 id="参考资料">参考资料</h2><blockquote><ul><li><a href="https://arxiv.org/abs/2208.07339">[2208.07339] LLM.int8():8-bit Matrix Multiplication for Transformers at Scale</a></li><li><a href="https://blog.csdn.net/scgaliguodong123_/article/details/136176382">大模型量化技术原理-LLM.int8()、GPTQ-CSDN博客</a></li><li><a href="https://github.com/bitsandbytes-foundation/bitsandbytes">bitsandbytes-foundation/bitsandbytes:Accessible large language models via k-bit quantization forPyTorch.</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;todo：本文还在施工中.......&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;llm.int8&lt;/code&gt;是第一批针对大模型进行量化的算法，并且其算法也被集成在
&lt;code&gt;bitsandb</summary>
      
    
    
    
    <category term="科研" scheme="https://bg51717.github.io/wiki/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文阅读" scheme="https://bg51717.github.io/wiki/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="量化" scheme="https://bg51717.github.io/wiki/tags/%E9%87%8F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>pytorch分布式-ddp</title>
    <link href="https://bg51717.github.io/wiki/45014/"/>
    <id>https://bg51717.github.io/wiki/45014/</id>
    <published>2024-10-09T05:30:13.000Z</published>
    <updated>2024-11-02T02:55:49.879Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍">介绍</h1><p>这篇博客主要是关于pytorch分布式ddp（DistributedDataParallel）的介绍和大概的食用（<del>这不是错别字</del>）教程。</p><p>数据并行DistributedDataParallel指的是在数据集层面进行多进程的切分，对于模型参数和训练状态等其他部分切分。</p><p>首先会介绍一下通信。在pytorch分布式ddp中，各个进程的代码是单独运行的。彼此之间在没有显式通信的时候，是不知道对方的的信息的。因此分布式的重点，所以了解通信的情况，也就了解了分布式的原理和使用的方法。</p><p>主要通信的方式有：</p><ul><li>环境变量</li><li>同步</li><li>tensor操作</li></ul><p>然后会介绍一下在数据并行下数据如何进行切分。</p><p>最后介绍整体pytorch分布式大概的流程和使用方法。</p><h1 id="通信">通信</h1><h2 id="环境变量">环境变量</h2><p>常用的环境变量有：</p><ul><li>WORLD_SIZE 全局进程数</li><li>RANK 当前进程全局标识符</li><li>LOCAL_RANK 在单个节点中的进程标识符</li><li>MASTER_ADDR 主节点IP地址</li><li>MASTER_PORT 主节点端口</li></ul><p>常用的为前三个，还有一些使用更加少的暂时没有罗列。</p><p>环境变量的获取可以：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"path"</span>]</span><br><span class="line">os.environ.get(<span class="string">'KEY_THAT_MIGHT_EXIST'</span>)</span><br><span class="line">os.getenv(<span class="string">'KEY_THAT_MIGHT_EXIST'</span>, default_value) <span class="comment"># 推荐</span></span><br></pre></td></tr></tbody></table></figure><h2 id="同步">同步</h2><p>引入pytorch分布式包</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br></pre></td></tr></tbody></table></figure><p>同步所有进程进度</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dist.barrier()</span><br></pre></td></tr></tbody></table></figure><p>在tensor通信的时候，也会起到同步进程的作用。很容易理解，不同步的话tensor的值都没有求得。</p><h2 id="tensor通信">tensor通信</h2><p>广播broadcast，收集gather，分发scatter，全收集all-gather，规约reduce，全规约all-reduce，全对称all-to-all，批量广播broadcast_object_list</p><h1 id="数据">数据</h1><p>在ddp中，只考虑的数据的剪切。那么对于某个进程，只需要计算部分数据即可。某个进程根据<strong>LOCAL_RANK</strong>获取自己所需的数据的方法有两种：</p><ol type="1"><li>数据集定义中加入offset，根据offset获取自己只需要的数据，那么进程只能看到自己的数据，比如<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, segment <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">open</span>(file)):</span><br><span class="line">  <span class="keyword">if</span> i % n_gpus != offset:</span><br><span class="line">    <span class="keyword">continue</span></span><br></pre></td></tr></tbody></table></figure></li><li>通过设置DataLoader中的sampler控制数据集采样实现数据切分，比如：<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, DistributedSampler</span><br><span class="line">sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>, sampler=sampler)</span><br></pre></td></tr></tbody></table></figure></li></ol><h1 id="使用方法">使用方法</h1><p>在通信前，需要进行初始化操作（如果init_process_group不指定部分参数，也会自动从环境变量中获取）：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化分布式进程</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">setup</span>():</span><br><span class="line">    rank = <span class="built_in">int</span>(os.environ[<span class="string">"RANK"</span>])</span><br><span class="line">    world_size = <span class="built_in">int</span>(os.environ[<span class="string">"WORLD_SIZE"</span>])</span><br><span class="line">    dist.init_process_group(<span class="string">"nccl"</span>, rank=rank, world_size=world_size)</span><br></pre></td></tr></tbody></table></figure><p>设置使用的GPU（可以灵活设置，比如每若干个进程共享GPU）:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rank = <span class="built_in">int</span>(os.environ[<span class="string">"RANK"</span>])</span><br><span class="line">torch.cuda.set_device(rank) <span class="comment"># 设置默认GPU</span></span><br><span class="line">device = torch.device(<span class="string">f"cuda:<span class="subst">{rank}</span>"</span>) <span class="comment"># 显式指定设备</span></span><br></pre></td></tr></tbody></table></figure><blockquote><p>使用的GPU还会收到环境变量CUDA_VISIBLE_DEVICES的限制</p><p>设置默认GPU可以让部分CUDA操作默认在该设备执行</p></blockquote><p>然后包装模型，隐式的进行<code>tensor</code>的同步和通信（在模型之外计算某些量（如精度、损失值等），可能需要同步）：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line">model = SimpleCNN().to(device)</span><br><span class="line">model = DDP(model, device_ids=[rank])</span><br></pre></td></tr></tbody></table></figure><blockquote><p>当然，这里也可以切换成别的过程，比如如果不是模型的训练和推理，也可以进行tensor别的计算方法，但是需要手动的进行通信等。</p></blockquote><p>对于一些多个进程只需要完成一次的操作，比如保存模型或者日志记录等，只需要一个进程一般是主进程完成即可：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> dist.get_rank() == <span class="number">0</span>:</span><br><span class="line">    torch.save(model.state_dict(), <span class="string">"model_checkpoint.pth"</span>)</span><br></pre></td></tr></tbody></table></figure><p>代码执行完需要进程组的销毁：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cleanup</span>():</span><br><span class="line">    dist.destroy_process_group()</span><br></pre></td></tr></tbody></table></figure><h1 id="代码执行">代码执行</h1><p>如果执行代码直接使用python，那么需要使用pytorch的包启动多进程：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.multiprocessing <span class="keyword">as</span> mp</span><br><span class="line">mp.spawn(train, nprocs=world_size, join=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure><p>如果直接使用 <code>torchrun</code>命令执行代码，则不需要使用<code>torch.multiprocessing</code>，但需要在命令里添加部分参数，等于调用<code>torch.multiprocessing</code>的任务交给<code>torchrun</code>完成：</p><figure class="highlight sh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchrun --nproc_per_node=4 your_script.py</span><br></pre></td></tr></tbody></table></figure><h1 id="参考资料">参考资料</h1><blockquote><ul><li><a href="https://pytorch.org/tutorials/distributed/home.html">Distributedand Parallel Training Tutorials — PyTorch Tutorials 2.4.0+cu121documentation</a></li><li><a href="https://blog.csdn.net/qq_39131062/article/details/109206804">“最全“PyTorch分布式训练教程来了！_pytorch训练-CSDN博客</a></li><li><a href="https://blog.csdn.net/qq_40185847/article/details/115074443">Pytorch中基于NCCL多GPU训练_pytorchnccl-CSDN博客</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;介绍&quot;&gt;介绍&lt;/h1&gt;
&lt;p&gt;这篇博客主要是关于pytorch分布式ddp（DistributedDataParallel）的介绍和大概的食用（&lt;del&gt;这不是错别字&lt;/del&gt;）教程。&lt;/p&gt;
&lt;p&gt;数据并行DistributedDataParallel指的是在</summary>
      
    
    
    
    <category term="模板" scheme="https://bg51717.github.io/wiki/categories/%E6%A8%A1%E6%9D%BF/"/>
    
    
    <category term="模板" scheme="https://bg51717.github.io/wiki/tags/%E6%A8%A1%E6%9D%BF/"/>
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="PyTorch" scheme="https://bg51717.github.io/wiki/tags/PyTorch/"/>
    
    <category term="分布式" scheme="https://bg51717.github.io/wiki/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    <category term="ddq" scheme="https://bg51717.github.io/wiki/tags/ddq/"/>
    
    <category term="数据并行" scheme="https://bg51717.github.io/wiki/tags/%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C/"/>
    
    <category term="Data Parallel" scheme="https://bg51717.github.io/wiki/tags/Data-Parallel/"/>
    
  </entry>
  
  <entry>
    <title>hexo博客2:双主题</title>
    <link href="https://bg51717.github.io/wiki/31680/"/>
    <id>https://bg51717.github.io/wiki/31680/</id>
    <published>2024-10-03T10:55:45.000Z</published>
    <updated>2024-11-02T05:32:24.133Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍">介绍</h1><p>后来发现单纯的wiki风格博客可能确实有些单调了（<del>绝对不是因为我想弄二次元风格的</del>），因此在考虑以后，决定搭建一个双主题的博客，外层是个华丽一点的主题<a href="https://github.com/blinkfox/hexo-theme-matery">matery</a>，内层是个wiki风格主题<a href="https://github.com/zthxxx/hexo-theme-Wikitten">Wikitten</a>。</p><p><img src="/wiki/wiki/31680/1727953154709.png"></p><p>同时可以通过外层的wiki标志点击进入内层的wiki主题。</p><p><img src="/wiki/wiki/31680/1727953220606.png"></p><p><img src="/wiki/wiki/31680/1727953171329.png"></p><h1 id="双主题安装">双主题安装</h1><p>参考文章：<a href="https://masantu.com/blog/2020-05-23/hello-hexo-wiki/">Hexo同时使用两种主题（博客与 wiki 页面实现统一管理） | 别院牧志(masantu.com)</a></p><ol type="1"><li><p>把原来wiki主题的 <code>_config.yml</code>复制并重命名为<code>_config_wiki.yml</code>（wiki主题下的站点文件</p></li><li><p>安装 <code>hexo-theme-matery</code>主题</p><figure class="highlight powershell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> your<span class="literal">-hexo-directory</span>/themes</span><br><span class="line">git clone https://github.com/blinkfox/hexo<span class="literal">-theme-matery</span></span><br></pre></td></tr></tbody></table></figure></li><li><p>按照 <code>matery</code>教程配置新的<code>_config_wiki.yml</code>文件</p></li><li><p>修改 <code>_config_wiki.yml</code>文件：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># URL</span></span><br><span class="line"><span class="comment">## Set your site url here. For example, if you use GitHub Page, set url as 'https://username.github.io/project'</span></span><br><span class="line"><span class="attr">url:</span> <span class="string">https://bg51717.github.io/wiki/</span></span><br><span class="line"><span class="attr">root:</span> <span class="string">/wiki/</span></span><br><span class="line"><span class="comment"># permalink: :year/:month/:day/:title/</span></span><br><span class="line"><span class="comment"># permalink_defaults:</span></span><br><span class="line"><span class="attr">permalink:</span> <span class="string">/:abbrlink/</span></span><br><span class="line"><span class="attr">abbrlink:</span></span><br><span class="line">    <span class="attr">alg:</span> <span class="string">crc16</span>   <span class="comment">#算法： crc16(default) and crc32</span></span><br><span class="line">    <span class="attr">rep:</span> <span class="string">dec</span>     <span class="comment">#进制： dec(default) and hex</span></span><br><span class="line"></span><br><span class="line"><span class="attr">permalink_defaults:</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># Directory</span></span><br><span class="line"><span class="attr">source_dir:</span> <span class="string">source</span></span><br><span class="line"><span class="attr">public_dir:</span> <span class="string">public/wiki/</span></span><br></pre></td></tr></tbody></table></figure></li></ol><h1 id="代码块行高问题">代码块行高问题</h1><p>在使用<code>prismjs</code>渲染代码块的时候，可能会遇到图示问题，行高不匹配，其实主要是位置没有对准：<img src="/wiki/wiki/31680/1730525332224.png"></p><p>可以参考：<a href="https://github.com/blinkfox/hexo-theme-matery/issues/928">hexo 7.3代码块显示问题 · Issue #928 · blinkfox/hexo-theme-matery</a></p><p>解决办法是把<code>themes\hexo-theme-matery\source\libs\prism\prism.min.css</code>文件里的：</p><figure class="highlight css"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-attr">[class*=language-]</span><span class="selector-class">.line-numbers</span>&gt;<span class="selector-tag">code</span>{<span class="attribute">position</span>:relative;<span class="attribute">white-space</span>:inherit}</span><br></pre></td></tr></tbody></table></figure><p>修改为：</p><figure class="highlight css"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-attr">[class*=language-]</span><span class="selector-class">.line-numbers</span>&gt;<span class="selector-tag">code</span>{<span class="attribute">position</span>:sticky;<span class="attribute">white-space</span>:inherit}</span><br></pre></td></tr></tbody></table></figure><h1 id="wiki图片链接问题">wiki图片链接问题</h1><p>由于渲染的站点 <code>url</code>和 <code>root</code>里都有<code>wiki</code>，因此会导致渲染出来的图片出现两次<code>/wiki</code>，因此需要代码进行处理，核心函数如下所示：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">post_html_file</span>(<span class="params">file_path</span>):</span><br><span class="line">    <span class="string">"""处理html文件，只修改图片链接中的重复/wiki/"""</span></span><br><span class="line">    <span class="comment"># 如果不是html文件, 返回</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> file_path.endswith(<span class="string">".html"</span>):</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读取文件内容</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">"r"</span>, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> file:</span><br><span class="line">        content = file.read()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义正则表达式来匹配图片链接，处理 &lt;img&gt; 标签中的 src 属性</span></span><br><span class="line">    img_pattern = <span class="string">r'&lt;img[^&gt;]+src="([^"]+)"'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查找所有匹配的图片链接</span></span><br><span class="line">    matches = re.findall(img_pattern, content)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历所有匹配的图片链接，替换掉重复的 /wiki/</span></span><br><span class="line">    modified_content = content</span><br><span class="line">    <span class="keyword">for</span> <span class="keyword">match</span> <span class="keyword">in</span> matches:</span><br><span class="line">        <span class="comment"># 如果链接中有重复的 /wiki/，替换为单一的 /wiki/</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">"/wiki/wiki/"</span> <span class="keyword">in</span> <span class="keyword">match</span>:</span><br><span class="line">            corrected_url = <span class="keyword">match</span>.replace(<span class="string">"/wiki/wiki/"</span>, <span class="string">"/wiki/"</span>)</span><br><span class="line">            <span class="comment"># 替换内容中的旧链接为新的链接</span></span><br><span class="line">            modified_content = modified_content.replace(<span class="keyword">match</span>, corrected_url)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将修改后的内容写回文件</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">"w"</span>, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> file:</span><br><span class="line">        file.write(modified_content)</span><br></pre></td></tr></tbody></table></figure><h1 id="图库api设置">图库APi设置</h1><p>对于外层主题，需要自己设置相关的图片，笔者从<a href="https://t.alcy.cc/">栗次元API-举个栗子图库API</a>爬取部分图片作为图片。也有别的食用方法可以参考该网站具体的教程。</p><h1 id="搜索链接跳转错误">搜索链接跳转错误</h1><p>由于把每篇博客链接到了一个数字，从而起到了缩短博客网址长度和增加搜索引擎爬取的概率，但是可能会导致原有的搜索跳转到随机IP。原因是：网址返回的是纯数字相对链接，比如<code>/7369</code>，完整的链接为<code>https://bg51717.github.io/7369/</code>，但是浏览器可能会把链接解析为ip地址，变成<code>0.0.28.201</code>，等同于256进制下的<code>7369</code>，从而导致跳转错误。因此需要调整搜索代码，每次返回完整链接，具体如下：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">if (data_url.indexOf('http') !== 0) {</span><br><span class="line">    // 生成绝对路径，确保基于网站根目录</span><br><span class="line">    data_url =  window.location.host +'/'+root+'/'+ data_url;</span><br><span class="line">    data_url = data_url.replace(/\/+/g, '/'); // 去掉连续的斜杠</span><br><span class="line">    data_url = data_url.replace(/^\//, '');   // 去掉开头的斜杠</span><br><span class="line">    data_url = window.location.protocol + '//' + data_url;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h1 id="渲染页面">渲染页面</h1><p>渲染部分的参考主要来自：<a href="https://masantu.com/blog/2020-05-23/hello-hexo-wiki/">Hexo同时使用两种主题（博客与 wiki 页面实现统一管理） | 别院牧志</a></p><ol type="1"><li><p>渲染主页面，生成到/public/</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br></pre></td></tr></tbody></table></figure></li><li><p>删除 db.json 及旧的 public/wiki</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo --config _config_wiki.yml clean</span><br></pre></td></tr></tbody></table></figure></li><li><p>渲染wiki页面</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo --config _config_wiki.yml g</span><br></pre></td></tr></tbody></table></figure></li><li><p>删除db.json</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm db.json</span><br></pre></td></tr></tbody></table></figure></li></ol><p>如果每次执行上述步骤都会稍显复杂，因此考虑整理脚本文件<code>run.py</code>，执行渲染时 <code>python run.py</code>即可：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> platform</span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_command</span>(<span class="params">command</span>):</span><br><span class="line">    <span class="string">"""实时执行命令并将输出打印到控制台"""</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># 使用 Popen 来执行命令并实时读取输出</span></span><br><span class="line">        process = subprocess.Popen(command, shell=<span class="literal">True</span>, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=<span class="literal">True</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 实时读取 stdout 和 stderr</span></span><br><span class="line">        <span class="keyword">for</span> stdout_line <span class="keyword">in</span> <span class="built_in">iter</span>(process.stdout.readline, <span class="string">""</span>):</span><br><span class="line">            <span class="built_in">print</span>(stdout_line, end=<span class="string">''</span>)  <span class="comment"># 实时输出 stdout</span></span><br><span class="line"></span><br><span class="line">        process.stdout.close()</span><br><span class="line">        return_code = process.wait()  <span class="comment"># 等待命令执行结束</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出 stderr（如果有）</span></span><br><span class="line">        <span class="keyword">if</span> return_code != <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">for</span> stderr_line <span class="keyword">in</span> <span class="built_in">iter</span>(process.stderr.readline, <span class="string">""</span>):</span><br><span class="line">                <span class="built_in">print</span>(stderr_line, end=<span class="string">''</span>)  <span class="comment"># 实时输出 stderr</span></span><br><span class="line">        process.stderr.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span> subprocess.CalledProcessError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"命令 '<span class="subst">{command}</span>' 执行失败，错误信息: <span class="subst">{e.stderr}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clean_generate_hexo</span>():</span><br><span class="line">    <span class="string">"""执行 hexo clean 和 hexo g 命令，以及相关的配置命令"""</span></span><br><span class="line">    os_type = platform.system()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据操作系统选择 rm 或 del</span></span><br><span class="line">    remove_command = <span class="string">"rm db.json"</span> <span class="keyword">if</span> os_type != <span class="string">"Windows"</span> <span class="keyword">else</span> <span class="string">"del db.json"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义需要执行的命令列表</span></span><br><span class="line">    commands = [</span><br><span class="line">        <span class="string">"python -m process.run_before"</span>,</span><br><span class="line">        <span class="string">"hexo clean"</span>,</span><br><span class="line">        <span class="string">"hexo g"</span>,</span><br><span class="line">        <span class="string">"hexo --config _config_wiki.yml clean"</span>,</span><br><span class="line">        <span class="string">"hexo --config _config_wiki.yml g"</span>,</span><br><span class="line">        remove_command,</span><br><span class="line">        <span class="string">"python -m process.run_post"</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 逐一执行命令</span></span><br><span class="line">    <span class="keyword">for</span> command <span class="keyword">in</span> commands:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"\n正在执行命令: <span class="subst">{command}</span>"</span>)</span><br><span class="line">        run_command(command)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    clean_generate_hexo()</span><br></pre></td></tr></tbody></table></figure><h1 id="参考资料">参考资料</h1><blockquote><ul><li><a href="https://github.com/blinkfox/hexo-theme-matery">blinkfox/hexo-theme-matery:A beautiful hexo blog theme with material design and responsivedesign.一个基于材料设计和响应式设计而成的全面、美观的Hexo主题。国内访问：http://blinkfox.com(github.com)</a></li><li><a href="https://github.com/zthxxx/hexo-theme-Wikitten">zthxxx/hexo-theme-Wikitten:A theme of Hexo for personal wiki which seems like Wikitten style.(github.com)</a></li><li><a href="https://masantu.com/blog/2020-05-23/hello-hexo-wiki/">Hexo同时使用两种主题（博客与 wiki 页面实现统一管理） | 别院牧志(masantu.com)</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;介绍&quot;&gt;介绍&lt;/h1&gt;
&lt;p&gt;后来发现单纯的wiki风格博客可能确实有些单调了（&lt;del&gt;绝对不是因为我想弄二次元风格的&lt;/del&gt;），因此在考虑以后，决定搭建一个双主题的博客，外层是个华丽一点的主题&lt;a href=&quot;https://github.com/blin</summary>
      
    
    
    
    <category term="SmallProjects" scheme="https://bg51717.github.io/wiki/categories/SmallProjects/"/>
    
    <category term="博客搭建" scheme="https://bg51717.github.io/wiki/categories/SmallProjects/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    
    
    <category term="博客" scheme="https://bg51717.github.io/wiki/tags/%E5%8D%9A%E5%AE%A2/"/>
    
    <category term="Hexo" scheme="https://bg51717.github.io/wiki/tags/Hexo/"/>
    
    <category term="Wiki" scheme="https://bg51717.github.io/wiki/tags/Wiki/"/>
    
  </entry>
  
  <entry>
    <title>使用dotbot快速同步Linux配置</title>
    <link href="https://bg51717.github.io/wiki/13162/"/>
    <id>https://bg51717.github.io/wiki/13162/</id>
    <published>2024-10-03T07:37:15.000Z</published>
    <updated>2024-11-02T02:55:49.875Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍">介绍</h1><p><strong>dotfiles</strong>指的是<code>.</code>开头的隐藏文件夹，一般是用户的配置或者软件信息。使用服务器或者Linux的时候，安装一些软件配置自己的使用环境是十分常见的场景。一个优秀的配置和各种软件不仅可以大幅提升工作效率，还可以美化工作环境，加强工作的动力。但是很多时候一个完整的配置是十分复杂且繁琐的，也难以去记忆每次配置时的信息。</p><p>因此很多人尝试收集配置文件，创建软连接，然后整理安装脚本，上传到github。实现难度较低。</p><p>这篇博客推荐使用基于Git的<a href="https://github.com/anishathalye/dotbot">dotbot</a>来管理dotfiles。自己编写管理脚本可能会导致脚本经常需要修改来使用不同的场合。框架在设计的时候会考虑到大部分场景，因此需要的修改和可能导致的错误较少。</p><h1 id="git子模块">Git子模块</h1><p>在介绍 <code>dotbot</code>之前需要了解一下<code>Git子模块(submodule)</code>的观念。当你的仓库依赖于别的仓库的时候，你可以添加一个链接指向被依赖的仓库的某个版本，而不需要去额外复制这些文件。</p><h2 id="基本操作">基本操作</h2><p>添加子模块：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git submodule add &lt;仓库地址&gt; &lt;子模块路径&gt;</span><br></pre></td></tr></tbody></table></figure><blockquote><p>&lt;仓库地址&gt;是仓库的网络地址，&lt;子模块路径&gt;指的是相当于这个仓库的路径<!--子模块路径--><!--仓库地址--></p><p>示例：git submodule add https://github.com/example/library.gitlibs/library</p></blockquote><p>初始化子模块，当你克隆具有子模块的仓库的时候，需要手动初始化并更新子模块：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git submodule init</span><br><span class="line">git submodule update</span><br></pre></td></tr></tbody></table></figure><blockquote><p>也可以一次更新</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --recurse-submodules &lt;仓库地址&gt;</span><br></pre></td></tr></tbody></table></figure></blockquote><p>子模块管理，子模块可以像正常的仓库一样进行管理：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> &lt;子模块路径&gt;</span><br><span class="line">git pull</span><br><span class="line">git add .</span><br><span class="line">git commit -m <span class="string">"Update submodule"</span></span><br><span class="line">git push</span><br></pre></td></tr></tbody></table></figure><blockquote><p>如果需要对子模块更新且不是子模块的作者的话，建议fork仓库作为子模块，fork仓库的管理此处不再阐述。</p></blockquote><p>删除子模块，删除子模块需要调整 <code>.gitmodules</code>和<code>.git/config</code>配置：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git submodule deinit &lt;子模块路径&gt;</span><br><span class="line">git <span class="built_in">rm</span> --cached &lt;子模块路径&gt;</span><br><span class="line"><span class="built_in">rm</span> -rf &lt;子模块路径&gt;</span><br></pre></td></tr></tbody></table></figure><p>修改子模块url：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git submodule set-url &lt;子模块路径&gt; &lt;新的URL&gt;</span><br></pre></td></tr></tbody></table></figure><p>修改子模块路径：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">mv</span> &lt;旧路径&gt; &lt;新路径&gt;</span><br><span class="line">git submodule <span class="built_in">sync</span></span><br></pre></td></tr></tbody></table></figure><h1 id="安装">安装</h1><p>首先你需要一个<code>dotfiles</code>文件夹（名字可以自定义），里面是你所有的配置文件。</p><p>之后进入这个文件夹，添加 <code>dotbot</code>作为子模块：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize Repository</span></span><br><span class="line">git init</span><br><span class="line">git submodule add https://github.com/anishathalye/dotbot</span><br><span class="line"><span class="built_in">cp</span> dotbot/tools/git-submodule/install .</span><br><span class="line"><span class="built_in">touch</span> install.config.yaml</span><br></pre></td></tr></tbody></table></figure><blockquote><p>也推荐把需要安装的软件作为子模块使用</p></blockquote><h1 id="配置">配置</h1><p>通过修改 <code>install.config.yaml</code>可以调整安装命令<code>./install</code>的工作。</p><p>默认的配置为：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="attr">defaults:</span></span><br><span class="line">    <span class="attr">link:</span></span><br><span class="line">      <span class="attr">relink:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">clean:</span> [<span class="string">'~'</span>]</span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">link:</span></span><br><span class="line">    <span class="string">~/.bashrc:</span> <span class="string">bashrc</span></span><br><span class="line">    <span class="string">~/.zshrc:</span> <span class="string">zshrc</span></span><br><span class="line">    <span class="string">~/.vimrc:</span> <span class="string">vimrc</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">shell:</span></span><br><span class="line">    <span class="bullet">-</span> [<span class="string">git</span> <span class="string">submodule</span> <span class="string">update</span> <span class="string">--init</span> <span class="string">--recursive</span>, <span class="string">Installing</span> <span class="string">submodules</span>]</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><blockquote><p>几个类别的顺序会影响命令执行时的顺序</p></blockquote><p>主要类别有：</p><ul><li>defaults：会对所有操作进行的设置</li><li>clean：哪些links会被检查是否dead，如果是dead links会被删除</li><li>link：创建软链接的源目录和目标目录</li><li>shell：希望运行的指令</li></ul><p>之后就可以简单的使用git来管理dotfile，并且使用<code>./install</code>安装即可。</p><h1 id="参考资料">参考资料</h1><blockquote><ul><li><a href="https://github.com/anishathalye/dotbot">anishathalye/dotbot: Atool that bootstraps your dotfiles ⚡️ (github.com)</a></li><li><a href="https://anishathalye.com/managing-your-dotfiles/">管理您的点文件(anishathalye.com)</a></li><li><a href="https://dotfiles.github.io/">GitHub does dotfiles -dotfiles.github.io</a></li><li>杰哥</li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;介绍&quot;&gt;介绍&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;dotfiles&lt;/strong&gt;指的是
&lt;code&gt;.&lt;/code&gt;开头的隐藏文件夹，一般是用户的配置或者软件信息。使用服务器或者Linux的时候，安装一些软件配置自己的使用环境是十分常见的场景。一个优秀的配置和各种</summary>
      
    
    
    
    <category term="工具" scheme="https://bg51717.github.io/wiki/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="dotfiles" scheme="https://bg51717.github.io/wiki/tags/dotfiles/"/>
    
    <category term="dotbot" scheme="https://bg51717.github.io/wiki/tags/dotbot/"/>
    
    <category term="Linux" scheme="https://bg51717.github.io/wiki/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>zsh+powerlevel10K优化终端使用体验</title>
    <link href="https://bg51717.github.io/wiki/13107/"/>
    <id>https://bg51717.github.io/wiki/13107/</id>
    <published>2024-09-29T13:09:00.000Z</published>
    <updated>2024-11-02T02:55:49.874Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍">介绍</h1><p><strong>ZSH</strong>（Zshell）类似Bash，是被广泛用于类Unix系统的命令行解释器。在具备Bash的基本功能的同时，还扩展了很多功能，同时对插件的支持和高度定制化使其成为了很多Linux用户的最佳选择。经常使用的功能有：自动补全，历史命令，语法高亮等。</p><p><strong>powerlevel10K</strong>是ZSH的主题之一，但是扩展了一些额外的功能，比如更多信息的显示，运行时间和当前时间的查看等。</p><p>通过灵活使用这两个工具，可以在美化你的终端页面的同时提升你的效率。</p><blockquote><p>以下操作默认使用的<strong>Ubuntu</strong>系统。</p></blockquote><h1 id="zsh">ZSH</h1><h2 id="安装">安装</h2><p>Ubuntu：</p><ul><li><p>安装zsh</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install zsh</span><br></pre></td></tr></tbody></table></figure></li><li><p>设置为默认shell</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo chsh -s /bin/zsh</span><br><span class="line"><span class="comment"># 为其他用户设置：sudo chsh -s /bin/zsh &lt;username&gt;</span></span><br></pre></td></tr></tbody></table></figure></li><li><p>安装git</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install git</span><br></pre></td></tr></tbody></table></figure></li><li><p>安装oh-my-zsh 手动安装:</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --depth=1 https://github.com/ohmyzsh/ohmyzsh.git ~/.oh-my-zsh</span><br><span class="line"><span class="built_in">cp</span> ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc</span><br></pre></td></tr></tbody></table></figure><p>或 自动安装:</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -c <span class="string">"<span class="subst">$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)</span>"</span></span><br></pre></td></tr></tbody></table></figure></li></ul><h2 id="zsh设置">ZSH设置</h2><p>通过修改配置文件 <code>~/.zshrc</code>可以修改终端设置。经常使用的设置应该有：</p><ul><li>ZSH_THEME：ZSH主题。</li><li>plugins：ZSH插件。</li></ul><p>同时一些希望终端启动时运行的命令也可以放在该文件里，比如conda的启动。</p><blockquote><p>也就是终端加载的时候会运行的命令，也可以<code>source ~/.zshrc</code>更新配置文件对终端的设置</p></blockquote><h2 id="插件推荐">插件推荐</h2><h3 id="sh-completions"><a href="https://github.com/zsh-users/zsh-completions">sh-completions</a></h3><p>这个插件提供了基础的补全功能。目前似乎已经被集成到ZSH项目中。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --depth=1 https://github.com/zsh-users/zsh-completions <span class="variable">${ZSH_CUSTOM:-<span class="variable">${ZSH:-~/.oh-my-zsh}</span>/custom}</span>/plugins/zsh-completions</span><br></pre></td></tr></tbody></table></figure><h3 id="zsh-autosuggestions"><a href="https://github.com/zsh-users/zsh-autosuggestions">zsh-autosuggestions</a></h3><p>这个插件可以提供自动补全的功能，比如输入 <code>git</code>，自动补全为<code>git status</code>。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --depth=1 https://github.com/zsh-users/zsh-autosuggestions.git <span class="variable">${ZSH_CUSTOM:-<span class="variable">${ZSH:-~/.oh-my-zsh}</span>/custom}</span>/plugins/zsh-autosuggestions</span><br></pre></td></tr></tbody></table></figure><h3 id="incremental-completion-on-zsh"><a href="https://mimosa-pudica.net/zsh-incremental.html">Incrementalcompletion on zsh</a></h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> <span class="variable">$ZSH_CUSTOM</span>/plugins/incr</span><br><span class="line">curl -fsSL https://mimosa-pudica.net/src/incr-0.2.zsh -o <span class="variable">$ZSH_CUSTOM</span>/plugins/incr/incr.zsh</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'source $ZSH_CUSTOM/plugins/incr/incr.zsh'</span> &gt;&gt; ~/.zshrc</span><br><span class="line"><span class="built_in">source</span> ~/.zshrc</span><br></pre></td></tr></tbody></table></figure><h3 id="zsh-syntax-highlighting"><a href="https://github.com/zsh-users/zsh-syntax-highlighting">zsh-syntax-highlighting</a></h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --depth=1 https://github.com/zsh-users/zsh-syntax-highlighting.git <span class="variable">${ZSH_CUSTOM:-~/.oh-my-zsh/custom}</span>/plugins/zsh-syntax-highlighting</span><br></pre></td></tr></tbody></table></figure><h1 id="powerlevel10k">powerlevel10K</h1><p>目前配置的终端可以显示当前的虚拟环境、路径、git情况，以及上个命令的执行时间等。基本需要都被涵盖。这写额外的功能和美化得归功于ZSH主题——<strong>powerlevel10K</strong>。具体效果图如下所示：<img src="/wiki/wiki/13107/1727664718688.png"></p><h2 id="安装-1">安装</h2><h3 id="字体">字体</h3><p>由于会显示一些额外的符号，所以需要安装新的字体。</p><blockquote><p>字体需要在显示终端的设备上安装。比如如果系统在本地，那么在本地安装。如果连接的远程，那么需要在远程上安装ZSH和主题，在本地安装字体。原理不难理解。</p></blockquote><p>字体链接：<a href="https://github.com/ryanoasis/nerd-fonts#font-installation">NerFont</a>字体链接里有完整的安装教程，可以根据自己的平台进行安装。如果是克隆源码安装的时候注意设置depth，建议从该仓库的releases里选择字体进行安装。根据平台情况安装然后后设置字体显示即可，一般需要在终端或者编辑器里设置字体选项。考虑到种类较多且教程丰富简单，此处不罗列详细步骤。</p><blockquote><p>Tips：如果设置的是Vscode平台，需要设置整体的字体： Editor:FontFamily，单独设置终端字体好像无法正常工作。 <img src="/wiki/wiki/13107/1727665397412.png"></p></blockquote><h3 id="主题">主题</h3><p>使用命令安装到 <code>~/.oh-my-zsh</code>目录下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --depth=1 https://github.com/romkatv/powerlevel10k.git <span class="variable">${ZSH_CUSTOM:-<span class="variable">$HOME</span>/.oh-my-zsh/custom}</span>/themes/powerlevel10k</span><br></pre></td></tr></tbody></table></figure><p>在 <code>~/.zshrc</code>下启动主题：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ZSH_THEME="powerlevel10k/powerlevel10k"</span><br></pre></td></tr></tbody></table></figure><h2 id="配置">配置</h2><p>重启终端或者<code>source ~/.zshrc</code>切换主题，之后会进入配置界面。（配置完成后也可以使用<code>p10k configure</code>重新进入配置向导）配置向导就是一些问题来判断你的字体情况和个人偏好，然后写入配置文件<code>~/.p10k.sh</code>中，大约有十几个问题。配置向导的详细介绍可以参考：<a href="https://juejin.cn/post/7293342627814244367">我的终端环境：与众不同的zsh 主题 - powerlevel10k本文介绍 zsh 主题 powerlevel10k - 掘金</a>在配置向导完成后，如果需要一些额外的配置。可以修改配置文件<code>~/.p10k.sh</code>。 比如，通过修改<code>POWERLEVEL9K_LEFT_PROMPT_ELEMENTS</code>和<code>POWERLEVEL9K_RIGHT_PROMPT_ELEMENTS</code>可以修改两侧显示的元素以及顺序等。</p><h1 id="参考资料">参考资料</h1><blockquote><ul><li><a href="https://sysin.org/blog/linux-zsh/">Linux Zsh 使用 oh-my-zsh打造高效便捷的 shell 环境 - sysin | SYStem INside |软件与技术分享</a></li><li><a href="https://juejin.cn/post/7293342627814244367">我的终端环境：与众不同的zsh 主题 - powerlevel10k本文介绍 zsh 主题 powerlevel10k - 掘金</a></li><li>杰哥</li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;介绍&quot;&gt;介绍&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;ZSH&lt;/strong&gt;（Z
shell）类似Bash，是被广泛用于类Unix系统的命令行解释器。在具备Bash的基本功能的同时，还扩展了很多功能，同时对插件的支持和高度定制化使其成为了很多Linux用户的最佳选择。经</summary>
      
    
    
    
    <category term="工具" scheme="https://bg51717.github.io/wiki/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="终端" scheme="https://bg51717.github.io/wiki/tags/%E7%BB%88%E7%AB%AF/"/>
    
    <category term="oh-my-zsh" scheme="https://bg51717.github.io/wiki/tags/oh-my-zsh/"/>
    
    <category term="powerlevel10k" scheme="https://bg51717.github.io/wiki/tags/powerlevel10k/"/>
    
  </entry>
  
  <entry>
    <title>安卓手机配置Google</title>
    <link href="https://bg51717.github.io/wiki/61294/"/>
    <id>https://bg51717.github.io/wiki/61294/</id>
    <published>2024-08-27T10:14:49.000Z</published>
    <updated>2024-11-02T02:55:49.875Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p>这篇博客主要记录了如何在安卓手机上配置谷歌三件套的服务。</p><p>对于非华为荣耀手机，可能仅仅需要简单的使用一些第三方的安装软件即可完成，比如<code>go安装助手</code>等，资源较大且获取难度较低。</p><p>而本篇博客主要介绍华为荣耀手机如何获取谷歌三件套的服务和配置支付信息等。</p><p>介绍两个并行的方法，当其中一个方法失效的时候，可以用另一个方法的部分替代。</p><p>方法是 <code>华谷套件</code>和<a href="https://github.com/to-alan/HarmonyOSInstallGMS?tab=readme-ov-file">to-alan/HarmonyOSInstallGMS:华为安装GMS教程</a> 。</p><p>博主前面的流程都使用的是华谷套件，该软件可以在每一步运行完后自动检测是否设置成功。在卸载MicroG后，转为使用方法二进行后续的处理。</p><p>目前手机谷歌三件套运行稳定，基本可以提供原生的谷歌三件套服务。</p><blockquote><p>支付方式的添加todo。</p></blockquote><p>参考资料</p><blockquote><ul><li><a href="https://github.com/to-alan/HarmonyOSInstallGMS?tab=readme-ov-file">to-alan/HarmonyOSInstallGMS:华为安装GMS教程</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;这篇博客主要记录了如何在安卓手机上配置谷歌三件套的服务。&lt;/p&gt;
&lt;p&gt;对于非华为荣耀手机，可能仅仅需要简单的使用一些第三方的安装软件即可完成，比如
&lt;code&gt;go安装助手&lt;/code&gt;等，资源较大且获取难度较低。&lt;/p&gt;
&lt;p&gt;而</summary>
      
    
    
    
    <category term="工具" scheme="https://bg51717.github.io/wiki/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="安卓" scheme="https://bg51717.github.io/wiki/tags/%E5%AE%89%E5%8D%93/"/>
    
    <category term="Google" scheme="https://bg51717.github.io/wiki/tags/Google/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch代码转HF</title>
    <link href="https://bg51717.github.io/wiki/61054/"/>
    <id>https://bg51717.github.io/wiki/61054/</id>
    <published>2024-08-06T10:56:13.000Z</published>
    <updated>2024-11-02T02:55:49.879Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p>这篇博客主要介绍了怎么把一个已有的Pytorch代码转变成HF支持的格式，然后可以方便的放入HF代码流程中，并且使用一些HF的函数。代码转换主要涉及到以下几个方面：</p><ul><li>Config</li><li>Model</li><li>Trainer</li><li>Dataset</li></ul><p>因为ckpt里面的代码使用的会是相对导入，所以在转换的过程中，建议把<code>configuration_xxx.py</code>和<code>modeling_xxx.py</code>文件放在同一个目录下，并且添加<code>__init__.py</code>文件。</p><h2 id="config">Config</h2><p>参考：<a href="https://huggingface.co/docs/transformers/custom_models#building-custom-models">Buildingcustom models (huggingface.co)</a></p><p>示例：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> PretrainedConfig</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LtgBertConfig</span>(<span class="title class_ inherited__">PretrainedConfig</span>):</span><br><span class="line">    model_type = <span class="string">"LtgBert"</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"""Configuration class to store the configuration of a `LtgBertModel`.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 vocab_size_or_config_json_file=<span class="number">16384</span>,</span></span><br><span class="line"><span class="params">                 hidden_size=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">                 num_hidden_layers=<span class="number">12</span>,</span></span><br><span class="line"><span class="params">                 num_attention_heads=<span class="number">12</span>,</span></span><br><span class="line"><span class="params">                 intermediate_size=<span class="number">3072</span>,</span></span><br><span class="line"><span class="params">                 hidden_act=<span class="string">"gelu"</span>,</span></span><br><span class="line"><span class="params">                 hidden_dropout_prob=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                 attention_probs_dropout_prob=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                 max_position_embeddings=<span class="number">512</span>,</span></span><br><span class="line"><span class="params">                 type_vocab_size=<span class="number">2</span>,</span></span><br><span class="line"><span class="params">                 initializer_range=<span class="number">0.02</span>,</span></span><br><span class="line"><span class="params">                 output_all_encoded_layers=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 require_all_hidden_states=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                 batch_first=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                 **kwargs</span>):</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.num_hidden_layers = num_hidden_layers</span><br><span class="line">        self.num_attention_heads = num_attention_heads</span><br><span class="line">        self.hidden_act = hidden_act</span><br><span class="line">        self.intermediate_size = intermediate_size</span><br><span class="line">        self.hidden_dropout_prob = hidden_dropout_prob</span><br><span class="line">        self.attention_probs_dropout_prob = attention_probs_dropout_prob</span><br><span class="line">        self.max_position_embeddings = max_position_embeddings</span><br><span class="line">        self.type_vocab_size = type_vocab_size</span><br><span class="line">        self.initializer_range = initializer_range</span><br><span class="line">        self.output_all_encoded_layers = output_all_encoded_layers</span><br><span class="line">        self.require_all_hidden_states = require_all_hidden_states</span><br><span class="line">        self.batch_first=batch_first</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(vocab_size_or_config_json_file, <span class="built_in">str</span>) <span class="keyword">or</span> (sys.version_info[<span class="number">0</span>] == <span class="number">2</span></span><br><span class="line">                        <span class="keyword">and</span> <span class="built_in">isinstance</span>(vocab_size_or_config_json_file, unicode)):</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(vocab_size_or_config_json_file, <span class="string">"r"</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> reader:</span><br><span class="line">                json_config = json.loads(reader.read())</span><br><span class="line">            <span class="keyword">for</span> key, value <span class="keyword">in</span> json_config.items():</span><br><span class="line">                self.__dict__[key] = value</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(vocab_size_or_config_json_file, <span class="built_in">int</span>):</span><br><span class="line">            self.vocab_size = vocab_size_or_config_json_file</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"First argument must be either a vocabulary size (int)"</span></span><br><span class="line">                             <span class="string">"or the path to a pretrained model config file (str)"</span>)</span><br><span class="line">        <span class="built_in">super</span>(LtgBertConfig, self).__init__(**kwargs)</span><br></pre></td></tr></tbody></table></figure><p>必须满足：</p><ul><li>继承自 <code>PretrainedConfig</code></li><li><code>__init__</code>函数接受 <code>kwargs</code>，并且使用<code>super()).__init__</code>传递这些参数</li></ul><p><code>model_type</code>的作用是把模型注册到<code>AutoClass</code>中，建议设置。</p><h2 id="model">Model</h2><p>参考：<a href="https://huggingface.co/docs/transformers/custom_models#building-custom-models">Buildingcustom models (huggingface.co)</a></p><p>示例：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LtgBertForMaskedLM</span>(<span class="title class_ inherited__">PreTrainedModel</span>):</span><br><span class="line">    config_class=LtgBertConfig</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,config,activation_checkpointing=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(config)</span><br><span class="line"><span class="comment"># 这里可以把成员变成类的继承LtgBertForMaskedLM(Bert):</span></span><br><span class="line">        self.model=Bert(</span><br><span class="line">            config=config,</span><br><span class="line">            activation_checkpointing=activation_checkpointing</span><br><span class="line">        )</span><br><span class="line">        self.require_all_hidden_states=config.require_all_hidden_states</span><br><span class="line">        self.batch_first=config.batch_first</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask, masked_lm_labels=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> self.batch_first:</span><br><span class="line">            <span class="comment"># 模型把batch放在第二个维度</span></span><br><span class="line">            input_ids=input_ids.transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> masked_lm_labels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                masked_lm_labels=masked_lm_labels.transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">        subword_prediction=self.model(input_ids, attention_mask, masked_lm_labels=masked_lm_labels)</span><br><span class="line">        loss=<span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> masked_lm_labels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            target_ids = masked_lm_labels.flatten()</span><br><span class="line">            target_ids = target_ids[target_ids != -<span class="number">100</span>]</span><br><span class="line">            loss = F.cross_entropy(subword_prediction, target_ids)</span><br><span class="line">        all_hidden_states=<span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> self.require_all_hidden_states:</span><br><span class="line">            all_hidden_states=self.model.get_contextualized(input_ids=input_ids,attention_mask=attention_mask)</span><br><span class="line">        <span class="keyword">if</span> self.batch_first:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(subword_prediction.size())&gt;<span class="number">2</span>:</span><br><span class="line">                subword_prediction=subword_prediction.transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> all_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                all_hidden_states=[it.transpose(<span class="number">0</span>,<span class="number">1</span>) <span class="keyword">for</span> it <span class="keyword">in</span> all_hidden_states]</span><br><span class="line">        <span class="keyword">return</span> MaskedLMOutput(</span><br><span class="line">            loss=loss,</span><br><span class="line">            logits=subword_prediction,</span><br><span class="line">            hidden_states=all_hidden_states,</span><br><span class="line">            attentions=<span class="literal">None</span></span><br><span class="line">        )</span><br></pre></td></tr></tbody></table></figure><p>对于自定义模型，往往每个<code>AutoClass</code>上都会注册一个模型，因此往往要写多个自定义模型。</p><p><code>config_type</code>的作用是把模型注册到<code>AutoClass</code>中，建议设置。</p><p>由于简约性原则，官方要求<code>self.model</code>对应原来的模型，比如用Pytorch定义的模型。</p><p><code>forward</code>函数需要注意结果格式，<code>transformers.modeling_outputs</code>里定义了每种模型forward的结果格式。</p><p>其中对于每个特定的子任务都有个类似的模型，对于部分函数比如forward建议参考已有的代码进行操作，因为hf框架在使用特定子任务的模型的时候，可能会添加特殊的参数。比如，对于序列分类任务SequenceClassification，其中相关模型的forward为：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">def forward(</span><br><span class="line">        self,</span><br><span class="line">        input_ids: Optional[torch.Tensor] = None,</span><br><span class="line">        attention_mask: Optional[torch.Tensor] = None,</span><br><span class="line">        output_attentions: Optional[bool] = None,</span><br><span class="line">        output_hidden_states: Optional[bool] = None,</span><br><span class="line">        inputs_embeds: Optional[torch.Tensor] = None,</span><br><span class="line">        return_dict: Optional[bool] = None,</span><br><span class="line">        labels: Optional[torch.LongTensor] = None,</span><br><span class="line">    ) -&gt; Union[Tuple[torch.Tensor], SequenceClassifierOutput]:</span><br><span class="line">        if self.batch_first:</span><br><span class="line">            # 模型把batch放在第二个维度</span><br><span class="line">            input_ids=input_ids.transpose(0,1)</span><br><span class="line">        contextualized_embeddings=self.model.get_contextualized(input_ids, attention_mask)</span><br><span class="line">        if self.batch_first:</span><br><span class="line">            contextualized_embeddings=contextualized_embeddings.transpose(0,1)</span><br><span class="line">        logits = self.head(contextualized_embeddings[:, 0, :])</span><br><span class="line">        if labels is not None:</span><br><span class="line">            if self.config.problem_type is None:</span><br><span class="line">                if self.num_labels == 1:</span><br><span class="line">                    self.config.problem_type = "regression"</span><br><span class="line">                elif self.num_labels &gt; 1 and (labels.dtype == torch.long or labels.dtype == torch.int):</span><br><span class="line">                    self.config.problem_type = "single_label_classification"</span><br><span class="line">                else:</span><br><span class="line">                    self.config.problem_type = "multi_label_classification"</span><br><span class="line"></span><br><span class="line">    if self.config.problem_type == "regression":</span><br><span class="line">                loss_fct = nn.MSELoss()</span><br><span class="line">                if self.num_labels == 1:</span><br><span class="line">                    loss = loss_fct(logits.squeeze(), labels.squeeze())</span><br><span class="line">                else:</span><br><span class="line">                    loss = loss_fct(logits, labels)</span><br><span class="line">            elif self.config.problem_type == "single_label_classification":</span><br><span class="line">                loss_fct = nn.CrossEntropyLoss()</span><br><span class="line">                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))</span><br><span class="line">            elif self.config.problem_type == "multi_label_classification":</span><br><span class="line">                loss_fct = nn.BCEWithLogitsLoss()</span><br><span class="line">                loss = loss_fct(logits, labels)</span><br><span class="line"></span><br><span class="line">    assert output_attentions is None</span><br><span class="line">        assert output_hidden_states is None</span><br><span class="line">        return SequenceClassifierOutput(</span><br><span class="line">            loss=loss,</span><br><span class="line">            logits=logits,</span><br><span class="line">            hidden_states=contextualized_embeddings if output_hidden_states else None,</span><br><span class="line">            attentions=None</span><br><span class="line">        )</span><br></pre></td></tr></tbody></table></figure><p>这里hf框架会在配置中添加problem_type等内容。</p><h2 id="注册">注册</h2><p>如果在ckpt文件夹的 <code>config.json</code>里没有<code>auto_map</code>指明 <code>AutoClass</code>的注册：</p><figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">"auto_map"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">  <span class="attr">"AutoConfig"</span><span class="punctuation">:</span> <span class="string">"configuration_ltgbert.LtgBertConfig"</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">"AutoModelForMaskedLM"</span><span class="punctuation">:</span> <span class="string">"modeling_ltgbert.LtgBertForMaskedLM"</span></span><br><span class="line"><span class="punctuation">}</span></span><br></pre></td></tr></tbody></table></figure><p>那么需要手动添加，在读取ckpt的代码里添加：</p><figure class="highlight python-repl"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">AutoConfig.register("LtgBert", LtgBertConfig)</span><br><span class="line">AutoModelForMaskedLM.register(LtgBertConfig, LtgBertForMaskedLM)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>如果希望在保存模型的时候 <code>config.json</code>文件中自动包含<code>auto_map</code>，可以添加以下代码（如果模型是从ckpt里加载的就不需要添加）：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">LtgBertConfig.register_for_auto_class()</span><br><span class="line">LtgBertForMaskedLM.register_for_auto_class(<span class="string">"AutoModelForMaskedLM"</span>)</span><br></pre></td></tr></tbody></table></figure><p>后来发现只有注册可能会存在 <code>config.json</code>里<code>auto_map</code>不完整的情况（原因暂时没有调查），可以考虑直接在<code>config.__init__</code>里强制指定：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self,....):</span><br><span class="line">   ...</span><br><span class="line">   self.auto_map={</span><br><span class="line">    "AutoConfig": "configuration_ltgbert.LtgBertConfig",</span><br><span class="line">    "AutoModelForMaskedLM": "modeling_ltgbert.LtgBertForMaskedLM"</span><br><span class="line">   }</span><br></pre></td></tr></tbody></table></figure><h2 id="trainer">Trainer</h2><p>训练流程的转换主要设计HF的 <code>Trainer</code>类，可以参考<a href="https://huggingface.co/docs/transformers/main/zh/main_classes/trainer">Trainer(huggingface.co)</a>和<a href="https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments">Trainer(huggingface.co)</a>。</p><p><code>Trainer</code>把训练的流程分为几个过程，通过继承以及重写相关函数即可完成流程的定制，通过参数即可实现超参数的设置，细节阅读参考资料。</p><h2 id="dataset">Dataset</h2><p><code>dataset</code>可以继承自<code>torch.utils.data.dataset</code>，但是需要注意<code>__getitem__</code>，默认情况该函数返回的需要满足<code>dict</code>格式，从而实现参数的设置。</p><h2 id="参考资料">参考资料</h2><blockquote><ul><li><a href="https://huggingface.co/docs/transformers/custom_models#building-custom-models">Buildingcustom models (huggingface.co)</a></li><li><a href="https://huggingface.co/docs/transformers/main/zh/main_classes/trainer">Trainer(huggingface.co)</a></li><li><a href="https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments">Trainer(huggingface.co)</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;这篇博客主要介绍了怎么把一个已有的Pytorch代码转变成HF支持的格式，然后可以方便的放入HF代码流程中，并且使用一些HF的函数。代码转换主要涉及到以下几个方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Config&lt;/li&gt;
&lt;li&gt;Model</summary>
      
    
    
    
    <category term="模板" scheme="https://bg51717.github.io/wiki/categories/%E6%A8%A1%E6%9D%BF/"/>
    
    
    <category term="模板" scheme="https://bg51717.github.io/wiki/tags/%E6%A8%A1%E6%9D%BF/"/>
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="PyTorch" scheme="https://bg51717.github.io/wiki/tags/PyTorch/"/>
    
    <category term="HuggingFace" scheme="https://bg51717.github.io/wiki/tags/HuggingFace/"/>
    
    <category term="Trainer" scheme="https://bg51717.github.io/wiki/tags/Trainer/"/>
    
    <category term="config" scheme="https://bg51717.github.io/wiki/tags/config/"/>
    
    <category term="model" scheme="https://bg51717.github.io/wiki/tags/model/"/>
    
    <category term="dataset" scheme="https://bg51717.github.io/wiki/tags/dataset/"/>
    
  </entry>
  
  <entry>
    <title>随机数种子</title>
    <link href="https://bg51717.github.io/wiki/7369/"/>
    <id>https://bg51717.github.io/wiki/7369/</id>
    <published>2024-07-09T10:02:24.000Z</published>
    <updated>2024-11-02T02:55:49.881Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍">介绍</h1><p>在深度学习的实际项目中，为了减少随机性，增强项目的复现能力，设置固定随机数种子十分重要，因此这篇文章罗列了一些设置随机种子的方法和减少项目随机性的经验。</p><h1 id="通用函数">通用函数</h1><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">set_random_seed</span>(<span class="params">seed</span>):</span><br><span class="line">    <span class="string">"""Set random seeds."""</span></span><br><span class="line">    os.environ[<span class="string">'PYTHONHASHSEED'</span>] = <span class="built_in">str</span>(seed)</span><br><span class="line">    random.seed(seed)  <span class="comment"># 设置 Python 内置随机库的种子</span></span><br><span class="line">    np.random.seed(seed)  <span class="comment"># 设置 NumPy 随机库的种子</span></span><br><span class="line">    torch.manual_seed(seed)  <span class="comment"># 设置 PyTorch 随机库的种子</span></span><br><span class="line">    torch.cuda.manual_seed(seed)  <span class="comment"># 为当前 CUDA 设备设置种子</span></span><br><span class="line">    torch.cuda.manual_seed_all(seed)  <span class="comment"># 为所有 CUDA 设备设置种子</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h2 id="scikit-learn">Scikit-learn</h2><p>在 <code>Scikit-learn</code>中，部分算法需要设置<code>random_state</code>，比如聚类算法 <code>kmeans</code>。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KMeans(n_clusters=<span class="number">2</span>,random_state=<span class="number">42</span>)</span><br></pre></td></tr></tbody></table></figure><h1 id="工程经验">工程经验</h1><ol type="1"><li>由于部分原因，一些python数组或者python集合等，可能顺序也会影响结果的随机性。如果在无法确保顺序是固定的或者顺序是有要求的情况下，尝试对这些中间结果进行排序减少随机性。</li></ol><h1 id="参考资料">参考资料</h1><blockquote><ul><li><a href="https://blog.csdn.net/yangweipeng708/article/details/138793949">【Python】深度学习中随机数种子seed的种类和设置方式_seed设置-CSDN博客</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;介绍&quot;&gt;介绍&lt;/h1&gt;
&lt;p&gt;在深度学习的实际项目中，为了减少随机性，增强项目的复现能力，设置固定随机数种子十分重要，因此这篇文章罗列了一些设置随机种子的方法和减少项目随机性的经验。&lt;/p&gt;
&lt;h1 id=&quot;通用函数&quot;&gt;通用函数&lt;/h1&gt;
&lt;figure clas</summary>
      
    
    
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="工程细节" scheme="https://bg51717.github.io/wiki/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B7%A5%E7%A8%8B%E7%BB%86%E8%8A%82/"/>
    
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="随机数" scheme="https://bg51717.github.io/wiki/tags/%E9%9A%8F%E6%9C%BA%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>vscode调试python</title>
    <link href="https://bg51717.github.io/wiki/30403/"/>
    <id>https://bg51717.github.io/wiki/30403/</id>
    <published>2024-04-19T14:47:51.000Z</published>
    <updated>2024-11-02T02:55:49.879Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p>在学习项目的过程中，很多时候需要通过调试来高效率的了解代码的执行过程，因此这里介绍下怎么使用vscode对python程序进行调试。</p><h2 id="方法一简单图标点击">方法一：简单图标点击</h2><p>vscode对一些简单的程序提供了一些可视化的调试方式，对于一些不需要指定参数等简单的调试功能，可以直接点击vscode左上角的几个图标进行debug过程。由于过于简单，此处不做介绍。</p><p><img src="/wiki/wiki/30403/1713538455137.png"></p><h2 id="方法二编辑launch.json文件">方法二：编辑launch.json文件</h2><p>在工作目录下的<code>./vscode/launch.json</code>文件里面，指定了各种debug和程序运行的参数、环境、解释器、目录等基本所有的环境配置。</p><p>可以在左下角的添加配置里面快速添加常见的选项。</p><p>比如下面所示：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">    // 使用 IntelliSense 了解相关属性。 </span><br><span class="line">    // 悬停以查看现有属性的描述。</span><br><span class="line">    // 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387</span><br><span class="line">    "version": "0.2.0",</span><br><span class="line">    "configurations": [</span><br><span class="line">        {</span><br><span class="line">            "name": "(gdb) 启动",</span><br><span class="line">            "type": "cppdbg",</span><br><span class="line">            "request": "launch",</span><br><span class="line">            "program": "${fileDirname}/${fileBasenameNoExtension}",</span><br><span class="line">            "args": [],</span><br><span class="line">            "stopAtEntry": false,</span><br><span class="line">            "cwd": "${fileDirname}",</span><br><span class="line">            "environment": [],</span><br><span class="line">            "externalConsole": false,</span><br><span class="line">            "MIMode": "gdb",</span><br><span class="line">            "setupCommands": [</span><br><span class="line">                {</span><br><span class="line">                    "description": "为 gdb 启用整齐打印",</span><br><span class="line">                    "text": "-enable-pretty-printing",</span><br><span class="line">                    "ignoreFailures": true</span><br><span class="line">                },</span><br><span class="line">                {</span><br><span class="line">                    "description": "将反汇编风格设置为 Intel",</span><br><span class="line">                    "text": "-gdb-set disassembly-flavor intel",</span><br><span class="line">                    "ignoreFailures": true</span><br><span class="line">                }</span><br><span class="line">            ]</span><br><span class="line">        },</span><br><span class="line">        {</span><br><span class="line">            "name": "py-dbg QLLM",</span><br><span class="line">            "type": "debugpy",</span><br><span class="line">            "request": "launch",</span><br><span class="line">            "python": "/home/bg51717/.conda/envs/QLLM/bin/python",</span><br><span class="line">            // "program": "/home/bg51717/project/QLLM/qllm/__main__.py",</span><br><span class="line">            "module": "qllm",</span><br><span class="line">            "console": "integratedTerminal",</span><br><span class="line">            "args":[</span><br><span class="line">                "--model=/home/bg51717/project/models/facebook/opt-350m",</span><br><span class="line">                "--method=gptq",</span><br><span class="line">                "--nsamples=64",</span><br><span class="line">                "--wbits=4",</span><br><span class="line">                "--groupsize=128",</span><br><span class="line">                "--save",</span><br><span class="line">                "/home/bg51717/project/QLLM/facebook/opt-350m_gptq4b",</span><br><span class="line">                "--export_onnx",</span><br><span class="line">                "/home/bg51717/project/QLLM/onnx_model/facebook/opt-350m_gptq4b"</span><br><span class="line">            ],</span><br><span class="line">            "env": {"PYTHONPATH": "/home/bg51717/project/QLLM/qllm"},</span><br><span class="line">            "cwd": "/home/bg51717/project/QLLM"</span><br><span class="line">        }</span><br><span class="line">    ]</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>这里参数非常多（建议使用的时候查询<code>gpt</code>、<code>搜索引擎</code>、<code>文档</code>等。</p><p>这里介绍几个常用的选项。此外，在编辑的时候可以类似<code>Linux</code>那样使用 <code>${fileDirname}</code>来引用<code>vscode</code>程序的当前变量比如工作目录</p><table><thead><tr><th>参数</th><th>含义</th><th>类型</th></tr></thead><tbody><tr><td>name</td><td>过程名字</td><td>str</td></tr><tr><td>type</td><td>过程类型</td><td>str</td></tr><tr><td>python</td><td>解释器（使用虚拟环境的时候需要注意指定</td><td>str</td></tr><tr><td>program</td><td>程序文件，按照脚本方式运行过程</td><td>str</td></tr><tr><td>module</td><td>模块名，按照模块方式运行过程</td><td>str</td></tr><tr><td>args</td><td>运行过程的参数</td><td>list</td></tr><tr><td>env</td><td>环境变量</td><td>dict</td></tr><tr><td>cwd</td><td>工作目录</td><td>str</td></tr></tbody></table><blockquote><p>此外，在使用的过程中python里面绝对引入、相对引入等。建议参考<a href="https://zhuanlan.zhihu.com/p/416867942">python相对导入常见问题和解决方案- 知乎 (zhihu.com)</a> 。</p><p>此处发现那里也有些没有提及的东西。</p><ul><li>解决方案错误一：ImportError: attempted <strong>relativeimport</strong> with no known parent package 里可以不修改代码，使用<code>python -m</code>命令+调整工作目录成功运行。（笔者当时遇到一个坑，当时没有注意的调试的是工程目录里的qllm文件还是env里装的py包</li></ul></blockquote><h2 id="方法三使用debug.py文件引入要调试的文件">方法三：使用debug.py文件引入要调试的文件</h2><p>如题，建立一个<code>debug.py</code>文件引入要调试的文件，类似于使用代理进行调试过程。</p><iframe src="https://player.bilibili.com/player.html?isOutside=true&amp;aid=656228835&amp;bvid=BV1ta4y1u78v&amp;cid=1129808210&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="width:100%;aspect-ratio:16/9;"></iframe><p>参考<a href="https://www.bilibili.com/video/BV1ta4y1u78v/?spm_id_from=333.337.search-card.all.click&amp;vd_source=6ca4c818110b061ac41dc3fcd0178f77">【Python入门】新手必会 vscode Debug 调试技巧_哔哩哔哩_bilibili</a></p><h2 id="参考资料">参考资料</h2><blockquote><ul><li><a href="https://zhuanlan.zhihu.com/p/416867942">python相对导入常见问题和解决方案- 知乎 (zhihu.com)</a></li><li><a href="https://www.bilibili.com/video/BV1ta4y1u78v/?spm_id_from=333.337.search-card.all.click&amp;vd_source=6ca4c818110b061ac41dc3fcd0178f77">【Python入门】新手必会 vscode Debug 调试技巧_哔哩哔哩_bilibili</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;在学习项目的过程中，很多时候需要通过调试来高效率的了解代码的执行过程，因此这里介绍下怎么使用vscode对python程序进行调试。&lt;/p&gt;
&lt;h2 id=&quot;方法一简单图标点击&quot;&gt;方法一：简单图标点击&lt;/h2&gt;
&lt;p&gt;vscode对一</summary>
      
    
    
    
    <category term="杂项" scheme="https://bg51717.github.io/wiki/categories/%E6%9D%82%E9%A1%B9/"/>
    
    
    <category term="vscode" scheme="https://bg51717.github.io/wiki/tags/vscode/"/>
    
    <category term="python" scheme="https://bg51717.github.io/wiki/tags/python/"/>
    
    <category term="调试" scheme="https://bg51717.github.io/wiki/tags/%E8%B0%83%E8%AF%95/"/>
    
    <category term="debug" scheme="https://bg51717.github.io/wiki/tags/debug/"/>
    
  </entry>
  
  <entry>
    <title>nlp常用排行榜</title>
    <link href="https://bg51717.github.io/wiki/63314/"/>
    <id>https://bg51717.github.io/wiki/63314/</id>
    <published>2024-04-05T06:56:57.000Z</published>
    <updated>2024-11-02T02:55:49.890Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p>在工作和学习的时候发现，很多时候挑选合适的模型和数据集等也是一个重要且麻烦的过程。发现有很多相关的评测的排行榜，根据这些实时更新的排行榜，可以辅助我们进行选择模型等前期工作。</p><p><a href="https://huggingface.co/spaces">Spaces - Hugging Face</a></p><p>这里罗列了许多关于ai的最新新闻，也能搜索到各种排行榜leaderboard。</p><h3 id="nlp任务">nlp任务</h3><p><a href="https://huggingface.co/spaces/mteb/leaderboard">MTEBLeaderboard - a Hugging Face Space by mteb</a></p><p>Massive Text Embedding Benchmark (MTEB)，是关于文本嵌入的排行榜，同时关注排行榜的like人数（从某种意义上反应排行榜的效用）。</p><h3 id="大模型评测">大模型评测</h3><p><a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">OpenLLM Leaderboard - a Hugging Face Space by HuggingFaceH4</a></p><p>这里提供了各种关于大模型在多维度的数据集上的表现能力，并且支持根据大模型的类型、精度等过滤大模型排行榜。</p><p><a href="https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard">BigCode Models Leaderboard - a Hugging Face Space by bigcode</a></p><p>这里提供了关于大模型code能力的排行榜。</p><p><a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard">LMSysChatbot Arena Leaderboard - a Hugging Face Space by lmsys</a></p><p>这里提供了关于大模型对话能力的排行榜（但是由于不知名原因暂时无法打开）。</p><p><a href="https://chat.lmsys.org/">Chat with Open Large LanguageModels (lmsys.org)</a></p><p>这里是关于大模型对话能力的测评网站，也提供了参考用的排行榜。</p><p><a href="https://huggingface.co/spaces/optimum/llm-perf-leaderboard">LLM-PerfLeaderboard - a Hugging Face Space by optimum</a></p><p>这里提供了大模型在给定硬件条件的训练资源后微调的性能排行榜。</p><p><a href="https://huggingface.co/spaces/logikon/open_cot_leaderboard">OpenCoT Leaderboard - a Hugging Face Space by logikon</a></p><p>这里提供了关于大模型CoT（Chain of Thought）的排行榜。</p><h3 id="数据集">数据集</h3><h2 id="参考资料">参考资料</h2><blockquote><ul><li><a href=""></a></li><li><a href=""></a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;在工作和学习的时候发现，很多时候挑选合适的模型和数据集等也是一个重要且麻烦的过程。发现有很多相关的评测的排行榜，根据这些实时更新的排行榜，可以辅助我们进行选择模型等前期工作。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://huggi</summary>
      
    
    
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="自然语言处理" scheme="https://bg51717.github.io/wiki/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
  </entry>
  
  <entry>
    <title>pytroch_tutorials杂项</title>
    <link href="https://bg51717.github.io/wiki/20669/"/>
    <id>https://bg51717.github.io/wiki/20669/</id>
    <published>2024-03-23T08:23:43.000Z</published>
    <updated>2024-11-02T02:55:49.875Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p>当作快速过这个资料的笔记，一些关于别的库的介绍是不完全的，考虑在使用的时候从别的信息渠道就行信息的搜集。也可以作为后面待更博客列举？</p><h2 id="常用方式">常用方式</h2><p>可以参考<a href="https://zhuanlan.zhihu.com/p/576691638">huggingfacetransformers教程总结 - 知乎 (zhihu.com)</a>（这篇教程很多是基于tf的，使用时候可以考虑换成pt）和 <a href="https://huggingface.co/docs">Hugging Face - Documentation</a>。</p><h2 id="tutorials具体中文版本">tutorials具体（中文版本）</h2><p><a href="https://huggingface.co/docs/transformers/main/zh/autoclass_tutorial">使用AutoClass加载预训练实例(huggingface.co)</a></p><p>这里有个注意的点是使用AutoModelForSequenceClassification可能会在模型后面添加一层来适应下游任务（别的类似的类可能也会有类似的做法），如果想不添加任何层直接使用，考虑使用AutoModel类来加载模型。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/preprocessing">预处理(huggingface.co)</a></p><p>这里介绍了怎么预处理各种数据比如文本，多模态，图片等。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/training">微调预训练模型(huggingface.co)</a></p><p>这里介绍了怎么使用<code>transformers.Trainer</code>类和pytorch原生代码来微调模型（pytorchlighting也提供了一个Trainer）。</p><p>也介绍了怎么使用huggingface的github上分享的脚本来微调模型<a href="https://github.com/huggingface/transformers">huggingface/transformers:🤗 Transformers: State-of-the-art Machine Learning for Pytorch,TensorFlow, and JAX. (github.com)</a>，</p><p>accelerate提供了accelerate config命令通过命令行交互帮助配置文件。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/accelerate#%E8%AE%BE%E7%BD%AE">🤗加速分布式训练 (huggingface.co)</a></p><p>这里介绍了怎么使用accelerate来进行分布式训练，流程非常简单，基本就是pytorch框架单卡训练的代码改个四五行。</p><p><a href="https://huggingface.co/docs/transformers/v4.39.1/zh/peft">使用 🤗PEFT 加载adapters (huggingface.co)</a></p><p>这里大致介绍了怎么使用<code>perf</code>库进行高效的参数微调，具体表现为通过<code>adapters</code>实现少参数调整，还大致介绍了怎么量化加载、启用、禁止、训练等。</p><p><a href="https://huggingface.co/docs/transformers/v4.39.1/zh/transformers_agents">TransformersAgents (huggingface.co)</a></p><p>这里介绍了transformers新版本提供的一个api（测试版可能随时变化），通过自然语言来辅助进行功能实现，类似于通过自然语言编程。</p><p><a href="https://huggingface.co/docs/transformers/v4.39.1/zh/llm_tutorial">使用LLMs进行生成(huggingface.co)</a></p><p>这里大致介绍了怎么使用<code>model.generate()</code>进行更好且多样化生成。</p><p><a href="https://huggingface.co/docs/transformers/v4.39.1/zh/fast_tokenizers">使用🤗 Tokenizers 中的分词器 (huggingface.co)</a></p><p>大概介绍了怎么训练、保存和加载分词器tokenizer。</p><p><a href="https://huggingface.co/docs/transformers/v4.39.1/zh/multilingual">用于推理的多语言模型(huggingface.co)</a></p><p>介绍了一些常用的多语言翻译模型。</p><p><a href="https://huggingface.co/docs/transformers/v4.39.1/zh/create_a_model">创建自定义架构(huggingface.co)</a></p><p>如题，使用一些配置文件自定义模型架构和各种处理器比如自然语言处理的分词器、语音识别的特征提取器等。</p><p><a href="https://huggingface.co/docs/transformers/v4.39.1/zh/custom_models#%E7%BC%96%E5%86%99%E8%87%AA%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B">共享自定义模型(huggingface.co)</a></p><p>这里介绍了怎么自定义模型，推送到hub上面以及怎么在AutoClass（比如AutoModel等）上面注册。</p><p><a href="https://huggingface.co/docs/transformers/v4.39.1/zh/chat_templating#%E6%88%91%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E8%81%8A%E5%A4%A9%E6%A8%A1%E6%9D%BF">聊天模型的模板(huggingface.co)</a></p><p>这里介绍了聊天的模板，怎么把多轮对话转变成一个字符串输入模型。并且支持用户自定义模板和使用内置模板等功能。自定义模板的办法使用的是Jinja。</p><p><a href="https://huggingface.co/docs/transformers/v4.39.1/zh/serialization">导出为ONNX (huggingface.co)</a></p><p>这里介绍了怎么通过Optimum库把模型导出为ONNX格式，ONNX模式下为公开具有标准化运算符和数据类型的图，可以进行各种优化和框架间的相互转移。</p><p>（这里还介绍了transformers.onnx，但这个库已经停止了维护并且回头会从教程里删去</p><p><a href="https://huggingface.co/docs/transformers/v4.39.1/zh/tflite">导出为TFLite (huggingface.co)</a></p><p><a href="https://www.tensorflow.org/lite/guide">TensorFlow Lite</a>是一个轻量级框架，用于资源受限的设备上，如手机、嵌入式系统和物联网（IoT）设备，部署机器学习模型，其文件扩展名为<code>.tflite</code>，同样可以通过Optimum库导出。</p><p><a href="https://huggingface.co/docs/transformers/v4.39.1/zh/torchscript">导出为TorchScript (huggingface.co)</a></p><p>这里关于torchscirpt的介绍可以参考pytorch的，这里还介绍了使用 NeuronSDK 将 Hugging Face TorchScript 模型部署到 AWS。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/performance">性能与可扩展性(huggingface.co)</a></p><p>这是关于训练和推理的综述，里面有许多指向英语文档的链接。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/fsdp">完全分片数据并行(huggingface.co)</a></p><p>介绍了怎么使用accelerate把模型参数切片在多个GPU运行以及部分优化。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/perf_hardware">训练用的定制硬件(huggingface.co)</a></p><p>这里引入了怎么更好的使用和了解硬件，但只是引入，还是需要后期继续深入学习。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/hpo_train">使用TrainerAPI进行超参数搜索 (huggingface.co)</a></p><p>如题，支持多种超参数搜索框架。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/big_models">实例化大型模型(huggingface.co)</a></p><p>这里介绍了怎么加载大模型，降低内存，具体表现为分片，把权重文件分成多个。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/debugging">调试(huggingface.co)</a></p><p>这里介绍了怎么使用脚本进行多GPU网络问题调试和上溢下溢检测（通过hook函数统计每个模块的输入和输出的绝对值的极值）。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/perf_torch_compile">使用torch.compile() 优化推理 (huggingface.co)</a></p><p>torch.compile() 优化推理过程，并且列出了许多实验结果。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/add_new_pipeline">如何创建自定义流水线？(huggingface.co)</a></p><p>这里介绍了怎么自定义流水线pipeline并且注册和推送到云端。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/tokenizer_summary#%E5%AD%90%E8%AF%8D%E5%88%86%E8%AF%8D">分词器的摘要(huggingface.co)</a></p><p>这里介绍了许多分词算法，如何构造tokenizer。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/main_classes/agent#transformers.Tool.setup">Agents和工具(huggingface.co)</a></p><p>这里介绍了各种各样的agent和tools，agents相较于模型可以使用tools。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/main_classes/callback">Callbacks(huggingface.co)</a></p><p>Callbacks可以用来自定义PyTorch[Trainer]中训练循环行为的对象（此功能尚未在TensorFlow中实现），该对象可以检查训练循环状态（用于进度报告、在TensorBoard或其他ML平台上记录日志等），并做出决策（例如提前停止）。</p><p>这里介绍了一些常用的callbacks，怎么自定义callbacks，TrainerState代表当前训练状态，TrainerControl控制训练循环。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/main_classes/configuration">Configuration(huggingface.co)</a></p><p>介绍了Pretrainedconfig类。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/main_classes/data_collator">DataCollator (huggingface.co)</a></p><p>这里介绍了各种Data collators来处理数据。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/main_classes/logging">Logging(huggingface.co)</a></p><p>这里介绍了transformers的日志系统。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/main_classes/model#transformers.PreTrainedModel">模型(huggingface.co)</a></p><p>这里介绍了怎么使用PretrainedModel，由于一定原因，官方似乎推荐PretrainedModel是用于对model的一层包装，forward等函数在model里面实现。</p><p>还介绍了ModuleUtilsMixin，这个是py通过多继承扩展模型功能的一个类，可以参考<a href="https://blog.csdn.net/u012814856/article/details/81355935">一个例子走近Python 的 Mixin 类：利用 Python多继承的魔力_mixin类-CSDN博客</a>学习Mixin类和调用顺序。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/main_classes/text_generation">Generation(huggingface.co)</a></p><p>这里介绍了GenerationConfig和GenerationMixin来使得模型进行多样化的生成过程，扩展了模型的功能。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/main_classes/onnx#%E5%AF%BC%E5%87%BA--transformers-%E6%A8%A1%E5%9E%8B%E5%88%B0-onnx">导出🤗 Transformers 模型到 ONNX (huggingface.co)</a></p><p>这里介绍了一些配置类来帮助transformers模型导出到ONNX。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/main_classes/optimizer_schedules">Optimization(huggingface.co)</a></p><p>这里只是列举了几个模型参数跟新过程会用到的优化器等，但不是很全，也不是很深入，连示例过程都没有。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/main_classes/output">模型输出(huggingface.co)</a></p><p>Transformers库所有模型的输出都是 <a href="https://huggingface.co/docs/transformers/main/zh/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a>的子类的实例，这里介绍了许多输出类和基本的类。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/main_classes/pipelines">Pipelines(huggingface.co)</a></p><p>这里介绍了许多Pipeline和自定义Pipeline。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/main_classes/processors">Processors(huggingface.co)</a></p><p>这个库大概介绍了Processor类，用于编码或者解码多模型输入并且组合。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/main_classes/quantization#autogptq-%E9%9B%86%E6%88%90">量化🤗 Transformers 模型 (huggingface.co)</a></p><p>这个大致介绍了常见的量化方法并给了简答的示例。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/main_classes/tokenizer#transformers.BatchEncoding">Tokenizer(huggingface.co)</a></p><p>大致介绍了Tokenizer和常用的办法。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/main_classes/trainer#pytorch%E5%AE%8C%E5%85%A8%E5%88%86%E7%89%87%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8Cfsdp">Trainer(huggingface.co)</a></p><p>这里介绍了Transformers的Trainer，同时说明了这个Trainer用于别的库的模型时要注意的问题，同时还介绍了一些常用的问题和通过accelerate使用Trainer。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/main_classes/deepspeed">DeepSpeed集成(huggingface.co)</a></p><p>这里介绍了DeepSpeed的很多常用的配置选项（非常复杂，挖坑todo</p><p><a href="https://huggingface.co/docs/transformers/main/zh/main_classes/feature_extractor#transformers.ImageFeatureExtractionMixin">FeatureExtractor (huggingface.co)</a></p><p>这里介绍了Feature负责为音频或视觉模型准备输入特征。这包括从序列中提取特征。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/main_classes/image_processor">ImageProcessor (huggingface.co)</a></p><p>这里介绍了对于图像处理的Processor。</p><h3 id="这里往后就是介绍一些功能类可以用于扩展调试控制等功能">这里往后就是介绍一些功能类，可以用于扩展、调试控制等功能</h3><p><a href="https://huggingface.co/docs/transformers/main/zh/internal/modeling_utils#transformers.modeling_tf_utils.TFConv1D">自定义层和工具(huggingface.co)</a></p><p>这里介绍了一些Transformers库的为模型提供的自定义层和帮助函数（底层还是调用的pytorch或者TensorFlow，但是组合了一些常用的层。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/internal/pipelines_utils">pipelines的工具(huggingface.co)</a></p><p>这里介绍了一些为Pipeline使用的功能类。</p><p><a href="https://huggingface.co/docs/transformers/main/zh/internal/tokenization_utils">Tokenizers的工具(huggingface.co)</a></p><p><a href="https://huggingface.co/docs/transformers/main/zh/internal/trainer_utils">Trainer的工具(huggingface.co)</a></p><p><a href="https://huggingface.co/docs/transformers/main/zh/internal/trainer_utils">用于生成的工具(huggingface.co)</a></p><p><a href="https://huggingface.co/docs/transformers/main/zh/internal/image_processing_utils">ImageProcessors的工具 (huggingface.co)</a></p><p><a href="https://huggingface.co/docs/transformers/main/zh/internal/audio_utils">FeatureExtractors的工具 (huggingface.co)</a></p><p><a href="https://huggingface.co/docs/transformers/main/zh/internal/file_utils">通用工具(huggingface.co)</a></p><p><a href="https://huggingface.co/docs/transformers/main/zh/internal/time_series_utils">时间序列工具(huggingface.co)</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;当作快速过这个资料的笔记，一些关于别的库的介绍是不完全的，考虑在使用的时候从别的信息渠道就行信息的搜集。也可以作为后面待更博客列举？&lt;/p&gt;
&lt;h2 id=&quot;常用方式&quot;&gt;常用方式&lt;/h2&gt;
&lt;p&gt;可以参考&lt;a href=&quot;https:</summary>
      
    
    
    
    <category term="杂项" scheme="https://bg51717.github.io/wiki/categories/%E6%9D%82%E9%A1%B9/"/>
    
    <category term="huggingface" scheme="https://bg51717.github.io/wiki/categories/huggingface/"/>
    
    
    <category term="transformers_tutorials" scheme="https://bg51717.github.io/wiki/tags/transformers-tutorials/"/>
    
  </entry>
  
  <entry>
    <title>pytorch_model</title>
    <link href="https://bg51717.github.io/wiki/42347/"/>
    <id>https://bg51717.github.io/wiki/42347/</id>
    <published>2024-02-24T09:54:19.000Z</published>
    <updated>2024-11-02T02:55:49.879Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p>作为深度学习的基本模板使用，方便使用的时候作为骨架</p><p>许多文件可以考虑添加<strong>argparse</strong>和<strong><em>sh</em></strong>来引入外部配置来抽象过程，增强代码重用性</p><h2 id="dataset.py">dataset.py</h2><p>这个文件提供了各种数据集的定义，自定义数据集需要实习三个主要函数</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class MyDataset(torch.utils.data.Dataset):</span><br><span class="line">    def __init__(self):</span><br><span class="line">super().__init__()</span><br><span class="line">#todo</span><br><span class="line"></span><br><span class="line">    def __getitem__(self,idx):</span><br><span class="line">#todo</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">#todo</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h2 id="models.py">models.py</h2><p>这个文件负责提供各种模型的定义，可以是完全自定义的模型或者预训练模型</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class MyModel(torch.nn.Module):</span><br><span class="line">    def __init__(self)</span><br><span class="line">super().__init__()</span><br><span class="line">#todo</span><br><span class="line">    def forward(self,input):</span><br><span class="line">#todo</span><br><span class="line"></span><br><span class="line">def get_model(config):</span><br><span class="line">    #todo</span><br><span class="line">    return model</span><br></pre></td></tr></tbody></table></figure><h2 id="criterion.py">criterion.py</h2><p>这个文件负责提供各种损失函数的定义，可以是完全自定义的损失函数或者框架提供的损失函数</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class CustomLoss(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(CustomLoss, self).__init__()</span><br><span class="line">        # 在这里初始化你的参数，如果有的话</span><br><span class="line"></span><br><span class="line">    def forward(self, input, target):</span><br><span class="line">        # 计算损失的逻辑</span><br><span class="line">        # 例如，这里我们使用简单的均方误差作为示例</span><br><span class="line">        loss = torch.mean((input - target) ** 2)</span><br><span class="line">        return loss</span><br><span class="line"></span><br><span class="line">def get_criterion()</span><br><span class="line">    #todo</span><br></pre></td></tr></tbody></table></figure><h2 id="optimizer.py">optimizer.py</h2><p>这个文件负责提供各种优化器的定义，可以是完全自定义的优化器或者框架提供的优化器</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">class CustomOptimizer(torch.optim.Optimizer):</span><br><span class="line">    def __init__(self, params, lr=0.01, momentum=0.5, weight_decay=0, learning_rate_decay=0.9):</span><br><span class="line">        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay, learning_rate_decay=learning_rate_decay)</span><br><span class="line">        super(CustomOptimizer, self).__init__(params, defaults)</span><br><span class="line"></span><br><span class="line">    def step(self, closure=None):</span><br><span class="line">        """Performs a single optimization step."""</span><br><span class="line">        loss = None</span><br><span class="line">        if closure is not None:</span><br><span class="line">            loss = closure()</span><br><span class="line"></span><br><span class="line">        for group in self.param_groups:</span><br><span class="line">            for p in group['params']:</span><br><span class="line">                if p.grad is None:</span><br><span class="line">                    continue</span><br><span class="line">                d_p = p.grad.data</span><br><span class="line">                if group['momentum'] &gt; 0:</span><br><span class="line">                    param_state = self.state[p]</span><br><span class="line">                    if 'momentum_buffer' not in param_state:</span><br><span class="line">                        buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()</span><br><span class="line">                        buf.mul_(group['momentum'])</span><br><span class="line">                    else:</span><br><span class="line">                        buf = param_state['momentum_buffer']</span><br><span class="line">                        buf.mul_(group['momentum']).add_(d_p, alpha=1 - group['momentum'])</span><br><span class="line">                    d_p = buf</span><br><span class="line"></span><br><span class="line">                if group['weight_decay'] != 0:</span><br><span class="line">                    d_p.add_(p.data, alpha=group['weight_decay'])</span><br><span class="line"></span><br><span class="line">                p.data.add_(d_p, alpha=-lr)</span><br><span class="line"></span><br><span class="line">        return loss</span><br><span class="line"></span><br><span class="line">def get_Optimizer():</span><br><span class="line">    #todo</span><br></pre></td></tr></tbody></table></figure><h2 id="lr_scheduler.py">lr_scheduler.py</h2><p>这个文件负责提供各种学习率调度器的定义，可以是完全自定义的学习率调度器或者框架提供的学习率调度器</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">class MyLRScheduler(torch.optim.lr_scheduler._LRScheduler):</span><br><span class="line">    def __init__(self, optimizer, step_size=10, gamma=0.1):</span><br><span class="line">        self.step_size = step_size</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        super(CustomLRScheduler, self).__init__(optimizer)</span><br><span class="line"></span><br><span class="line">    def get_lr(self):</span><br><span class="line">        """Calculate the learning rate at a given step."""</span><br><span class="line">        return [base_lr * self.gamma ** (self.last_step // self.step_size)</span><br><span class="line">                for base_lr in self.base_lrs]</span><br><span class="line"></span><br><span class="line">    def step(self, epoch=None):</span><br><span class="line">        """Update the learning rate at the end of the given epoch."""</span><br><span class="line">        if epoch is None:</span><br><span class="line">            self.last_step += 1</span><br><span class="line">        else:</span><br><span class="line">            self.last_step = epoch + 1</span><br><span class="line">        self._last_lr = self.get_lr()</span><br><span class="line">        for param_group, lr in zip(self.optimizer.param_groups, self._last_lr):</span><br><span class="line">            param_group['lr'] = lr</span><br><span class="line"></span><br><span class="line">def get_lr_scheduler()</span><br><span class="line">    #todo</span><br></pre></td></tr></tbody></table></figure><h2 id="train.py">train.py</h2><p>这个文件负责提供各种训练方法和过程</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">def train_model(model,criterion,optimizer,scheduler,num_epochs):</span><br><span class="line">    since=time.time()</span><br><span class="line">    best_model_wts=copy.deepcopy(model.state_dict())</span><br><span class="line">    best_acc=0.0</span><br><span class="line">    #每个epoch</span><br><span class="line">    for epoch in range(num_epochs):</span><br><span class="line">        print('Epoch {}/{}'.format(epoch,num_epochs-1))</span><br><span class="line">        print('-'*10)</span><br><span class="line"># 分为训练或者测试阶段</span><br><span class="line">        for phase in ['train','val']:</span><br><span class="line">            if phase=='train':</span><br><span class="line">                model.train()</span><br><span class="line">            else:</span><br><span class="line">                model.eval()</span><br><span class="line">            running_loss=0.0</span><br><span class="line">            running_corrects=0</span><br><span class="line">    # 每个批次进行计算损失和反向梯度</span><br><span class="line">            for inputs,labels in dataloaders[phase]:</span><br><span class="line">                inputs=inputs.to(device)</span><br><span class="line">                labels=labels.to(device)</span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">                with torch.set_grad_enabled(phase=='train'):</span><br><span class="line">                    outputs=model(inputs)</span><br><span class="line">                    _,preds=torch.max(outputs,1)</span><br><span class="line">                    loss=criterion(outputs,labels)</span><br><span class="line">                    if phase=='train':</span><br><span class="line">                        loss.backward()</span><br><span class="line">                        optimizer.step()</span><br><span class="line">                running_loss+=loss.item()*inputs.size(0)</span><br><span class="line">                running_corrects+=torch.sum(preds==labels.data)</span><br><span class="line">            epoch_loss=running_loss/dataset_sizes[phase]</span><br><span class="line">            epoch_acc=running_corrects/dataset_sizes[phase]</span><br><span class="line">            print('{} Loss :{:.4f} Acc:{:.4f}'.format(phase,epoch_loss,epoch_acc))</span><br><span class="line">            if phase=='val' and epoch_acc&gt;best_acc:</span><br><span class="line">                best_acc=epoch_acc</span><br><span class="line">                best_model_wts=copy.deepcopy(model.state_dict())</span><br><span class="line">      scheduler.step()</span><br><span class="line">print()</span><br><span class="line">  </span><br><span class="line">    time_elapsed=time.time()-since</span><br><span class="line">    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed//60,time_elapsed%60))</span><br><span class="line">    model.load_state_dict(best_model_wts)</span><br><span class="line">    return model   </span><br></pre></td></tr></tbody></table></figure><h2 id="main.py">main.py</h2><p>主要负责对于各个文件部分的引用，整合代码，基本逻辑为</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#数据集</span><br><span class="line">dataset</span><br><span class="line">#数据迭代器</span><br><span class="line">dataloader</span><br><span class="line">#模型</span><br><span class="line">model</span><br><span class="line">#损失函数</span><br><span class="line">criterion</span><br><span class="line">#优化器</span><br><span class="line">optimizer</span><br><span class="line">#学习率优化器</span><br><span class="line">lr_scheduler</span><br><span class="line">#训练</span><br><span class="line">train</span><br><span class="line">#保存</span><br><span class="line">save</span><br><span class="line">#预测</span><br><span class="line">predict</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h2 id="可选优化">可选优化</h2><ul><li>梯度裁剪 torch.nn.utils.clip_grad_norm_</li><li>加载最优参数</li><li>...</li></ul><h2 id="其他">其他</h2><p>可以去<a href="https://pytorch.org/tutorials/">pytorch官网</a>找找一些关于模型的优化。</p><h2 id="参考资料">参考资料</h2><blockquote><ul><li><a href=""></a></li><li><a href=""></a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;作为深度学习的基本模板使用，方便使用的时候作为骨架&lt;/p&gt;
&lt;p&gt;许多文件可以考虑添加&lt;strong&gt;argparse&lt;/strong&gt;和&lt;strong&gt;&lt;em&gt;sh&lt;/em&gt;&lt;/strong&gt;来引入外部配置来抽象过程，增强代码重用性</summary>
      
    
    
    
    <category term="模板" scheme="https://bg51717.github.io/wiki/categories/%E6%A8%A1%E6%9D%BF/"/>
    
    
    <category term="模板" scheme="https://bg51717.github.io/wiki/tags/%E6%A8%A1%E6%9D%BF/"/>
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>信息熵</title>
    <link href="https://bg51717.github.io/wiki/5656/"/>
    <id>https://bg51717.github.io/wiki/5656/</id>
    <published>2024-01-31T11:19:50.000Z</published>
    <updated>2024-11-02T02:55:49.879Z</updated>
    
    <content type="html"><![CDATA[<h2 id="信息熵的公式"><strong>信息熵的公式</strong></h2><p>计算信息熵的公式为:</p><p><span class="math display">\[H(x)=-\sum p(x_i)logp(x_i)\]</span></p><p>其中<span class="math inline">\(p(x_i)\)</span>表示事件结果为<span class="math inline">\(x_i\)</span>的概率</p><h2 id="理解">理解</h2><p>信息熵表示对事件不确定性的一个度量，计算思路为“编码一个事件的最短平均编码长度”（任意进制编码都行，彼此差一个常数，但常用的是二进制以及自然对数）</p><p>所以信息熵的计算也可以写作：</p><p><span class="math display">\[H(x)=\sum p(x_i)f(x_i)\]</span></p><p>其中<span class="math inline">\(p(x_i)\)</span>表示事件结果为<span class="math inline">\(x_i\)</span>的概率，<span class="math inline">\(f(x_i)\)</span>为编码<span class="math inline">\(x_i\)</span>需要的位数（这也是为什么在比较概率分布的时候，会选择用拟合的概率来计算<span class="math inline">\(f(x_i)\)</span>）</p><h1 id="huffman编码树">Huffman编码树</h1><p>类比哈夫曼树，根据贪心思想，</p><ul><li>出现概率大的结果应该占据相对短的编码</li><li>编码结果的种类和编码位数是指数级关系</li></ul><p>所以我们得到</p><p><span class="math display">\[f(x_i)=-logp(x_i)\]</span></p><p>代入就得到了最终形式。</p><h2 id="应用">应用</h2><ul><li>KL散度</li><li>交叉熵损失</li></ul><h2 id="参考资料">参考资料</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;信息熵的公式&quot;&gt;&lt;strong&gt;信息熵的公式&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;计算信息熵的公式为:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
H(x)=-&#92;sum p(x_i)logp(x_i)
&#92;]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;</summary>
      
    
    
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="前置数学" scheme="https://bg51717.github.io/wiki/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%89%8D%E7%BD%AE%E6%95%B0%E5%AD%A6/"/>
    
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="数学" scheme="https://bg51717.github.io/wiki/tags/%E6%95%B0%E5%AD%A6/"/>
    
    <category term="信息学" scheme="https://bg51717.github.io/wiki/tags/%E4%BF%A1%E6%81%AF%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>pytroch_tutorials杂项</title>
    <link href="https://bg51717.github.io/wiki/20668/"/>
    <id>https://bg51717.github.io/wiki/20668/</id>
    <published>2023-12-29T08:23:43.000Z</published>
    <updated>2024-11-02T02:55:49.878Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p>当作快速过这个资料的笔记，一些关于别的库的介绍是不完全的，考虑在使用的时候从别的信息渠道就行信息的搜集。也可以作为后面待更博客列举？</p><h2 id="具体">具体</h2><h3 id="torchscript">torchscript</h3><p><a href="https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html">Introductionto TorchScript — PyTorch Tutorials 2.2.1+cu121 documentation</a></p><p><a href="https://pytorch.org/tutorials/advanced/cpp_export.html">Loading aTorchScript Model in C++ — PyTorch Tutorials 2.2.1+cu121documentation</a></p><p>这里介绍了把pytorch模型转换成torchscript的两种方法torch.jit.trace和torch.jit.script，前者会失去控制流信息，后者会保留控制流信息（具体查阅文档）。</p><p>转化为torchscript有以下好处：</p><ul><li>不需要Python解释器也可以运行，可以被pytorch自带的特殊解释器直接使用</li><li>转为torchscript可以进行各种优化，提高运行效率</li><li>方便被其他语言调用</li></ul><h3 id="onnx">ONNX</h3><p><a href="https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html">(optional)Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime— PyTorch Tutorials 2.2.1+cu121 documentation</a></p><p>这里介绍了在高效且跨平台ONNX运行环境部署的基本教程，通过torch.onnx把模型转化为onnx格式并保存，然后读取本地保存onnx格式模型并运行。</p><h3 id="剖析pytorch-model">剖析pytorch model</h3><p><a href="https://pytorch.org/tutorials/beginner/profiler.html">Profilingyour PyTorch Module — PyTorch Tutorials 2.2.1+cu121documentation</a></p><p>这里介绍了torch.autograd.profiler,在代码里添加withprofiler.record_function("MASKINDICES"):可以记录代码运行的指标，比如时间，cpu和内存使用率等，名字可以自定义，支持可视化结果。</p><p><a href="https://pytorch.org/tutorials/beginner/hta_intro_tutorial.html">Introductionto Holistic Trace Analysis — PyTorch Tutorials 2.2.1+cu121documentation</a></p><p><a href="https://pytorch.org/tutorials/beginner/hta_trace_diff_tutorial.html">TraceDiff using Holistic Trace Analysis — PyTorch Tutorials 2.2.1+cu121documentation</a></p><p>这里介绍了HolisticTraceAnalysis（HTA），一个用来剖析GPU运行情况的库。</p><p>把GPU运行时间分为了等待时间、计算时间和非计算时间，方便开发者评测模型运行的情况。还支持查看GPU内部的运行情况，也支持和之前的记录进行对比，也能可视化结果。</p><h3 id="troch.fx-操作计算图">troch.fx 操作计算图</h3><p><a href="https://pytorch.org/tutorials/intermediate/fx_conv_bn_fuser.html">（测试版）在FX 中构建卷积/批量范数热熔器 — PyTorch 教程 2.2.1+cu121 文档</a></p><p><code>torch.fx</code> 是 PyTorch提供的一个模块，它允许用户通过操作计算图来转换和优化 PyTorch模型。这个模块提供了一种方式来表示和操作 PyTorch模型的抽象，使得开发者可以更容易地对模型进行修改，例如重构模型结构、插入调试代码、优化性能等。</p><p>教程里介绍了一种融合相邻层的操作（只能用于eval模式）。比如融合卷积和归一化，融合之前，模型会把卷积结果写回显存，然后在调用归一化并且从显存读取之前结果；融合后变成一种操作，卷积结果不会写回显存，而是直接进行归一化后写会显存。</p><figure class="highlight python-repl"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">def _parent_name(target : str) -&gt; Tuple[str, str]:</span><br><span class="line">    """</span><br><span class="line">    Splits a ``qualname`` into parent path and last atom.</span><br><span class="line">    For example, `foo.bar.baz` -&gt; (`foo.bar`, `baz`)</span><br><span class="line">    """</span><br><span class="line">    *parent, name = target.rsplit('.', 1)</span><br><span class="line">    return parent[0] if parent else '', name</span><br><span class="line"></span><br><span class="line">def replace_node_module(node: fx.Node, modules: Dict[str, Any], new_module: torch.nn.Module):</span><br><span class="line">    assert(isinstance(node.target, str))</span><br><span class="line">    parent_name, name = _parent_name(node.target)</span><br><span class="line">    setattr(modules[parent_name], name, new_module)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def fuse(model: torch.nn.Module) -&gt; torch.nn.Module:</span><br><span class="line">    model = copy.deepcopy(model)</span><br><span class="line">    # The first step of most FX passes is to symbolically trace our model to</span><br><span class="line">    # obtain a `GraphModule`. This is a representation of our original model</span><br><span class="line">    # that is functionally identical to our original model, except that we now</span><br><span class="line">    # also have a graph representation of our forward pass.</span><br><span class="line">    #获取计算图</span><br><span class="line">    fx_model: fx.GraphModule = fx.symbolic_trace(model)</span><br><span class="line">    modules = dict(fx_model.named_modules())</span><br><span class="line"></span><br><span class="line">    # The primary representation for working with FX are the `Graph` and the</span><br><span class="line">    # `Node`. Each `GraphModule` has a `Graph` associated with it - this</span><br><span class="line">    # `Graph` is also what generates `GraphModule.code`.</span><br><span class="line">    # The `Graph` itself is represented as a list of `Node` objects. Thus, to</span><br><span class="line">    # iterate through all of the operations in our graph, we iterate over each</span><br><span class="line">    # `Node` in our `Graph`.</span><br><span class="line">    #枚举所有计算图节点</span><br><span class="line">    for node in fx_model.graph.nodes:</span><br><span class="line">        # The FX IR contains several types of nodes, which generally represent</span><br><span class="line">        # call sites to modules, functions, or methods. The type of node is</span><br><span class="line">        # determined by `Node.op`.</span><br><span class="line">#计算图节点还有别的属性,具体可以查阅相关资料</span><br><span class="line">        if node.op != 'call_module': # If our current node isn't calling a Module then we can ignore it.</span><br><span class="line">            continue</span><br><span class="line">        # For call sites, `Node.target` represents the module/function/method</span><br><span class="line">        # that's being called. Here, we check `Node.target` to see if it's a</span><br><span class="line">        # batch norm module, and then check `Node.args[0].target` to see if the</span><br><span class="line">        # input `Node` is a convolution.</span><br><span class="line">#modules指的是模块,node.target指的是节点名字,node.args应该指的是输入这个节点的前置节点</span><br><span class="line">        if type(modules[node.target]) is nn.BatchNorm2d and type(modules[node.args[0].target]) is nn.Conv2d:</span><br><span class="line">            if len(node.args[0].users) &gt; 1:  # Output of conv is used by other nodes</span><br><span class="line">                continue</span><br><span class="line">            conv = modules[node.args[0].target]</span><br><span class="line">            bn = modules[node.target]</span><br><span class="line">            fused_conv = fuse_conv_bn_eval(conv, bn)</span><br><span class="line">            replace_node_module(node.args[0], modules, fused_conv)</span><br><span class="line">            # As we've folded the batch nor into the conv, we need to replace all uses</span><br><span class="line">            # of the batch norm with the conv.</span><br><span class="line">    #把使用到node节点输出的地方变成node.args[0]节点输出</span><br><span class="line">            node.replace_all_uses_with(node.args[0])</span><br><span class="line">            # Now that all uses of the batch norm have been replaced, we can</span><br><span class="line">            # safely remove the batch norm.</span><br><span class="line">            fx_model.graph.erase_node(node)</span><br><span class="line">    #检查计算图（Graph）的完整性和一致性,确定计算图的正确性</span><br><span class="line">    fx_model.graph.lint()</span><br><span class="line">    # After we've modified our graph, we need to recompile our graph in order</span><br><span class="line">    # to keep the generated code in sync.</span><br><span class="line">    #recompile()方法的作用是将这些修改后的计算图转换回 Python 代码，这样你就可以创建一个新的 PyTorch 模型实例，它包含了你所做的所有修改</span><br><span class="line">    fx_model.recompile()</span><br><span class="line">    return fx_model</span><br></pre></td></tr></tbody></table></figure><p><a href="https://pytorch.org/tutorials/intermediate/fx_profiling_tutorial.html">（测试版）使用FX 构建简单的 CPU 性能分析器 — PyTorch 教程 2.2.1+cu121 文档</a></p><p>通过<code>print(traced_rn18.graph)</code>我们可以查看计算图的信息，这里只选择一行查看，例子:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%layer1_1_conv2 : [num_users=1] = call_module[target=layer1.1.conv2](args = (%layer1_1_relu,), kwargs = {})</span><br><span class="line">%layer1_1_conv2:节点名字,</span><br><span class="line">[num_users=1]:被几个后续节点使用,</span><br><span class="line">call_module:表面这是一个调用模块,</span><br><span class="line">target=layer1.1.conv2:调用模块的名字,</span><br><span class="line">args = (%layer1_1_relu,):传递给模块的参数(输入模块的之前节点)</span><br><span class="line">kwargs = {}:额外关键词参数</span><br></pre></td></tr></tbody></table></figure><p>可以参照教程写一个继承torch.fx.Interpreter的类,重写run和run_node实现对于计算图的捕获，在运行过程中添加自定义行为。</p><h3 id="存储组织">存储组织</h3><p><a href="https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html">(beta)Channels Last Memory Format in PyTorch — PyTorch Tutorials 2.2.1+cu121documentation</a></p><p>这里介绍了内存组织方式torch.channels_last，torch.contiguous_format等，同时介绍了一下torch.stride、torch.contiguous和torch.is_contiguous等函数。</p><p>有的操作符会保留内存组织格式。</p><p>可以通过model.to(memory_format=)把模型转化为适合的组织形式，同时输入也需要相应的转换（框架应该支持自动转换？）。</p><p>但由于不是所有的操作符都支持某种内存组织格式，可能需要进行检查，这里给了检查和设置的示例代码。</p><h3 id="前向模式自动微分">前向模式自动微分</h3><p><a href="https://pytorch.org/tutorials/intermediate/forward_ad_usage.html">正向模式自动微分（Beta） — PyTorch 教程 2.2.1+cu121 文档</a></p><p>这里看的不是很明白。挖个坑（todo...</p><p>正向传播一次只能算出相对一个参数的梯度，如果有n个参数，需要计算n次，所以这里的tangent只有一个，而不是每个参数对应一个。</p><p>可以参考<a href="https://zhuanlan.zhihu.com/p/518296942">【自动微分原理】AD的正反向模式- 知乎 (zhihu.com)</a>。</p><p>（这么一看，正向自动微分被方向自动微分爆杀？目前应用少且pytorch也只是测试版本</p><h3 id="微分矩阵计算">微分矩阵计算</h3><p><a href="https://pytorch.org/tutorials/intermediate/jacobians_hessians.html">雅可比派、黑森派、hvp、vhp等：编写函数转换 — PyTorch 教程 2.2.1+cu121 文档</a></p><p>这里提供了各种使用自动求导计算微分矩阵的方法。</p><h3 id="模型组装">模型组装</h3><p><a href="https://pytorch.org/tutorials/intermediate/ensembling.html">模型组装— PyTorch 教程 2.2.1+cu121 文档</a></p><p>这里介绍了torch.func.stack_model_state、torch.vmap等函数用来组装一系列结构相同参数不同的模型的输出，类似于使用cat连接模型输出，但是增加了底层优化。</p><h3 id="单样本梯度">单样本梯度</h3><p><a href="https://pytorch.org/tutorials/intermediate/per_sample_grads.html">Per-sample-gradients— PyTorch 教程 2.2.1+cu121 文档</a></p><p>在一些要求对每个样本单独计算梯度的特殊情况下，可以使用本篇介绍的方法优化速度。</p><p>这里介绍了torch.func.grad、torch.func.functional_call、torch.func.vmap来优化加速（注意不同库有不同的同名函数）</p><p>这里grad和vmap都是创建的可调用对象，与别的同名函数不同。</p><h3 id="c接口">c++接口</h3><p><a href="https://pytorch.org/tutorials/advanced/cpp_frontend.html">Usingthe PyTorch C++ Frontend — PyTorch Tutorials 2.2.1+cu121documentation</a></p><p>介绍了怎么使用c++调用</p><h3 id="torchscript-中的动态并行性">TorchScript 中的动态并行性</h3><p><a href="https://pytorch.org/tutorials/advanced/torch-script-parallelism.html">TorchScript中的动态并行性 — PyTorch 教程 2.2.1+cu121 文档</a></p><p>这里介绍了torch.jit.fork和torch.jit.wait两个函数用来并行执行pytorch相关代码。同时也引入了相关的类torch.jit.Future。</p><h3 id="c接口中的自动求导">c++接口中的自动求导</h3><p><a href="https://pytorch.org/tutorials/advanced/cpp_autograd.html">Autogradin C++ Frontend — PyTorch Tutorials 2.2.1+cu121 documentation</a></p><h3 id="这里都是一些用c扩展pytorch的内容后面需要细致学习挖坑todo">这里都是一些用c++扩展pytorch的内容，后面需要细致学习（挖坑todo</h3><h3 id="自定义函数求二阶导">自定义函数求二阶导</h3><p><a href="https://pytorch.org/tutorials/intermediate/custom_function_double_backward_tutorial.html">DoubleBackward with Custom Functions — PyTorch Tutorials 2.2.1+cu121documentation</a></p><p>这篇文章介绍了怎么使用继承自torch.autograd.Function的自定义函数（重新forward和backward），计算导数，并且用torch.autograd.gradcheck使用数值解验证求导以及使用torchviz可视化图形。</p><p>同时，注意在运行过程中，保存求导所需要参数时候建议按照教程分保存输入、保存输出和保存中间结果考虑代码结构，避免bug。</p><p>forward函数返回结果似乎和backward参数对应，backward输入参数是每个forward函数返回结果的grad。</p><p>保存中间结果的时候需要把中间结果返回，这样可以让pytorch的自动微分系统追踪这些中间结果（个人理解为返回中间结果相当于注册了这些中间结果，在backward的时候设置grad模型就会追踪这些中间结果，理解可能有误，但是使用的时候参考教程应该问题不大）。</p><h3 id="自定义函数实现卷积">自定义函数实现卷积</h3><p><a href="https://pytorch.org/tutorials/intermediate/custom_function_conv_bn_tutorial.html">FusingConvolution and Batch Norm using Custom Function — PyTorch Tutorials2.2.1+cu121 documentation</a></p><p>先介绍了自定义卷积函数的构造，这里的once_differentiable指的是正向传播的结果只会反向传播一次，作用为求二阶导的时候会报错，从而起到限制的作用。</p><p>然后介绍了自定义批量归一化层的构造，然后介绍了自定义函数混合卷积和批量归一化，从而实现了节省内存的作用。</p><h3 id="自定义-c-和-cuda-扩展">自定义 C++ 和 CUDA 扩展</h3><p><a href="https://pytorch.org/tutorials/advanced/cpp_extension.html">CustomC++ and CUDA Extensions — PyTorch Tutorials 2.2.1+cu121documentation</a></p><p>这里介绍了两个扩展c++组件的方法，“ahead of time ”和“just intime”。</p><p>ahead oftime：构建setup.py文件用于构建c++扩展，然后在对应的cpp文件里面构建c++扩展代码。</p><p>just intime:通过torch.utils.cpp_extension.load直接加载cpp文件，会把编译的中间文件存在一个临时目录里。</p><p>这里还介绍了编写混合 C++/CUDA扩展，由于还没有学过cuda，挖个坑todu...</p><h3 id="使用自定义-c-运算符扩展-torchscript">使用自定义 C++ 运算符扩展TorchScript</h3><p><a href="https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html">ExtendingTorchScript with Custom C++ Operators — PyTorch Tutorials 2.2.1+cu121documentation</a></p><p>如题，后面需要细致学习todo,,,</p><h3 id="使用自定义-c-类扩展-torchscript">使用自定义 C++ 类扩展TorchScript</h3><p><a href="https://pytorch.org/tutorials/advanced/torch_script_custom_classes.html">ExtendingTorchScript with Custom C++ Classes — PyTorch Tutorials 2.2.1+cu121documentation</a></p><p>如题，后面需要细致学习todo,,,</p><h3 id="在c中注册一个分派操作符">在C++中注册一个分派操作符</h3><p><a href="https://pytorch.org/tutorials/advanced/dispatcher.html">Registeringa Dispatched Operator in C++ — PyTorch Tutorials 2.2.1+cu121documentation</a></p><p>由于一个操作在不同的设备上对应不同的底层代码，所以为了抽象化其操作，方便调用，需要在内核注册对应设备对应函数，然后在调用。</p><p>比如，第一个把 <code>myadd_cpu</code>注册到cpu上的<code>myadd</code>函数，当在cpu上运行 <code>myadd</code>函数时候，会调用<code>myadd_cpu</code></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">TORCH_LIBRARY_IMPL(myops, CPU, m) {</span><br><span class="line">  m.impl("myadd", myadd_cpu);</span><br><span class="line">}</span><br><span class="line">TORCH_LIBRARY_IMPL(myops, CUDA, m) {</span><br><span class="line">  m.impl("myadd", myadd_cuda);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>然后在使用的时候，通过torch的调度器自动根据设备在内核寻找合适函数调用。</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Tensor myadd(const Tensor&amp; self, const Tensor&amp; other) {</span><br><span class="line">  static auto op = torch::Dispatcher::singleton()</span><br><span class="line">    .findSchemaOrThrow("myops::myadd", "")</span><br><span class="line">    .typed&lt;decltype(myadd)&gt;();</span><br><span class="line">  return op.call(self, other);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>这里还介绍了自动投影机制autocast，用于在运算之前把精度投影到合适的精度。</p><h3 id="在-c-中扩展新后端的调度程序">在 C++ 中扩展新后端的调度程序</h3><p><a href="https://pytorch.org/tutorials/advanced/extend_dispatcher.html">Extendingdispatcher for a new backend in C++ — PyTorch Tutorials 2.2.1+cu121documentation</a></p><p>后端（backend）通常指的是支持PyTorch运行的底层系统或框架，这里介绍了如何添加自定义的框架。(大致是需要实现一些基本的运算，别的运算可以表示成这样运算的组合)。</p><p><a href="https://pytorch.org/tutorials/advanced/privateuseone.html">FacilitatingNew Backend Integration by PrivateUse1 — PyTorch Tutorials 2.2.1+cu121documentation</a></p><p>这里介绍了使用privateuse1注册新后端的流程。</p><h3 id="结合tensorboard的profiler">结合tensorboard的profiler</h3><p><a href="https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html">PyTorchProfiler With TensorBoard — PyTorch Tutorials 2.2.1+cu121documentation</a></p><p>这篇文章介绍了profiler结合tensorboard的使用。</p><h3 id="使用ray-tune进行超参数调优">使用Ray Tune进行超参数调优</h3><p><a href="https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html">Hyperparametertuning with Ray Tune — PyTorch Tutorials 2.2.1+cu121documentation</a></p><p>这篇文章介绍了怎么使用Ray Tune库进行超参数的搜索。</p><h3 id="优化vision-transformer模型">优化Vision Transformer模型</h3><p><a href="https://pytorch.org/tutorials/beginner/vt_tutorial.html">OptimizingVision Transformer Model for Deployment — PyTorch Tutorials 2.2.1+cu121documentation</a></p><p>这篇文章介绍了怎么去优化VisionTransformer模型，列举了一些常见的优化方式。</p><h3 id="参数化教程">参数化教程</h3><p><a href="https://pytorch.org/tutorials/intermediate/parametrizations.html">ParametrizationsTutorial — PyTorch Tutorials 2.2.1+cu121 documentation</a></p><p>这里的参数化指的是类似于对模型部分权重使用的一种限制，应该是在参数修改后调用（比如初始化，参数更新等）的一些nn.module，从而使得模型更加灵活，实现对不同参数的不同处理过程，具体使用过程也是看教程，进行类的注册，使用函数parametrize。</p><p>with parametrize.cached():可以开启参数的缓存模型。</p><p>可以对于一个参数注册多个参数化模块。</p><p>同时参数化模块内部有的函数可以只在特殊时期调用，比如初始化。</p><p>可以移除参数化模块。</p><h3 id="剪枝教程">剪枝教程</h3><p><a href="https://pytorch.org/tutorials/intermediate/pruning_tutorial.html">PruningTutorial — PyTorch Tutorials 2.2.1+cu121 documentation</a></p><p>这里介绍了剪枝和机制，以及钩子函数的引入。</p><p>介绍了组合剪枝，通过移除重参数化来永久化剪枝，不同模块针对剪枝，全局剪枝，扩展自己的剪枝函数等。</p><h3 id="动态量化">动态量化</h3><p><a href="https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html">(beta)Dynamic Quantization on an LSTM Word Language Model — PyTorch Tutorials2.2.1+cu121 documentation</a></p><p><a href="https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html">(beta)Dynamic Quantization on BERT — PyTorch Tutorials 2.2.1+cu121documentation</a></p><p>这里介绍了使用torch.quantization.quantize_dynamic动态量化一次常见模型的例子。</p><h3 id="计算机视觉的量化迁移学习教程">计算机视觉的量化迁移学习教程</h3><p><a href="https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html">(beta)Quantized Transfer Learning for Computer Vision Tutorial — PyTorchTutorials 2.2.1+cu121 documentation</a></p><p>这篇文章介绍了在微调时候插入量化模拟层和反量化层，从而让模型适应量化的过程（QAT）。</p><h3 id="pytorch-中具有-eager-模式的静态量化">PyTorch 中具有 Eager模式的静态量化</h3><p><a href="https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html">(beta)Static Quantization with Eager Mode in PyTorch — PyTorch Tutorials2.2.1+cu121 documentation</a></p><p>这个教程演示如何进行训练后静态量化，并说明两种更先进的技术，通道量化和量化感知训练。</p><h3 id="从第一性原理挖掘-pytorch-intel-cpu-性能">从第一性原理挖掘PyTorch Intel CPU 性能</h3><p><a href="https://pytorch.org/tutorials/intermediate/torchserve_with_ipex.html">GrokkingPyTorch Intel CPU performance from first principles — PyTorch Tutorials2.2.1+cu121 documentation</a></p><p><a href="https://pytorch.org/tutorials/intermediate/torchserve_with_ipex_2.html">GrokkingPyTorch Intel CPU performance from first principles (Part 2) — PyTorchTutorials 2.2.1+cu121 documentation</a></p><p>这里介绍了优化CPU性能的方法和原理，目前来说太高深（挖矿todo</p><h3 id="带-ax-的多目标-nas">带 Ax 的多目标 NAS</h3><p><a href="https://pytorch.org/tutorials/intermediate/ax_multiobjective_nas_tutorial.html">Multi-ObjectiveNAS with Ax — PyTorch Tutorials 2.2.1+cu121 documentation</a></p><p>这篇文章介绍了怎么使用Ax平台进行神经网络架构的搜索。</p><h3 id="torch.compile介绍">torch.compile介绍</h3><p><a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">Introductionto torch.compile — PyTorch Tutorials 2.2.1+cu121 documentation</a></p><p>可以使用torch.compile当做装饰器或者函数使用。</p><p>同时也列举了几个模式以及和别的几个优化方法的对比。</p><h3 id="inductor-cpu-backend-debugging-and-profiling">Inductor CPUbackend debugging and profiling</h3><p><a href="https://pytorch.org/tutorials/intermediate/inductor_debug_cpu.html">InductorCPU backend debugging and profiling — PyTorch Tutorials 2.2.1+cu121documentation</a></p><p>这篇文章介绍了怎么对C++后端进行debug和profile的过程。有点复杂，，，（挖坑todo</p><h3 id="实现高效缩放点积注意力transformer">实现高效缩放点积注意力Transformer</h3><p><a href="https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html">(Beta)Implementing High-Performance Transformers with Scaled Dot ProductAttention (SDPA) — PyTorch Tutorials 2.2.1+cu121 documentation</a></p><p>这篇文章介绍了怎么优化使用常用的缩放点积注意力。</p><p>torch.nested.nested_tensor支持融合不同长度的张量。</p><h3 id="知识蒸馏">知识蒸馏</h3><p><a href="https://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html">KnowledgeDistillation Tutorial — PyTorch Tutorials 2.2.1+cu121documentation</a></p><p>这篇教程介绍了知识蒸馏的几个方式，主要都是在损失函数里面添加教师模型和学生模型的各种量化差异来训练学生模型。</p><h3 id="分布式挖坑todo">分布式（挖坑todo</h3><h3 id="参考资料">参考资料</h3><blockquote><ul><li><a href="https://pytorch.org/tutorials/">Welcome to PyTorchTutorials — PyTorch Tutorials 2.2.1+cu121 documentation</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;当作快速过这个资料的笔记，一些关于别的库的介绍是不完全的，考虑在使用的时候从别的信息渠道就行信息的搜集。也可以作为后面待更博客列举？&lt;/p&gt;
&lt;h2 id=&quot;具体&quot;&gt;具体&lt;/h2&gt;
&lt;h3 id=&quot;torchscript&quot;&gt;torch</summary>
      
    
    
    
    <category term="杂项" scheme="https://bg51717.github.io/wiki/categories/%E6%9D%82%E9%A1%B9/"/>
    
    
    <category term="pytroch_tutorials" scheme="https://bg51717.github.io/wiki/tags/pytroch-tutorials/"/>
    
  </entry>
  
  <entry>
    <title>Adam Optimizer</title>
    <link href="https://bg51717.github.io/wiki/12551/"/>
    <id>https://bg51717.github.io/wiki/12551/</id>
    <published>2023-12-29T08:23:43.000Z</published>
    <updated>2024-11-02T02:55:49.881Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景">背景</h2><p>传统的随机梯度下降算法SGD(Stochastic Gradient Descent)的式子为:</p><p><span class="math display">\[\theta_{t+1} \leftarrow \theta_{t} -\alpha\nabla_{\theta_{t}}J_{minibatcg}(\theta_{t})\]</span></p><p>其中<span class="math inline">\(J\)</span>为损失函数,<span class="math inline">\(\theta_{t}\)</span>为t时刻的参数,<span class="math inline">\(\alpha\)</span>为学习率,<span class="math inline">\(\nabla_{\theta_{t}}J_{minibatcg}(\theta_{t})\)</span>为t时刻梯度.</p><h2 id="adam算法初步优化">Adam算法初步优化</h2><p>考虑让梯度更加平滑,有以下式子:</p><p>$$ m_{t+1}<em>{1}m</em>{t}+(1-<em>{1})</em>{<em>{t}}J</em>{minibatcg}(_{t}) \</p><p><em>{t+1} </em>{t} -m_{t+1} $$</p><p>优点:</p><ul><li>梯度更加平滑,减少了振荡</li><li>可以在初始设置更大的学习率</li></ul><h2 id="adam算法最终优化">Adam算法最终优化</h2><p>通过对应"动量"这一想法的进一步扩展,我们得到了新的学习算法式子:</p><p>$$ m_{t+1} _1 m_t + (1 - <em>1) </em>{<em>t} J</em>{}(<em>t) \v</em>{t+1} _2 v_t + (1 - <em>2)(</em>{<em>t} J</em>{}(<em>t)</em>{<em>t} J</em>{}(_t)) \</p><p>_{t+1} _t - $$</p><p>其中<span class="math inline">\(\beta_{1},\beta_{2}\)</span>是大小在0和1之间的超参数,<span class="math inline">\(\odot\)</span>是平方的符号,<span class="math inline">\(\alpha\)</span>是学习率.</p><p>优点:</p><ul><li>梯度更加平滑,减少了振荡</li><li>可以在初始设置更大的学习率</li><li>接收较小或很少更新的模型参数将得到较大的更新</li></ul><h2 id="参考资料">参考资料</h2><blockquote><p><a href="https://zh.d2l.ai/chapter_optimization/adam.html">11.10.Adam算法 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></p><p><a href="https://web.stanford.edu/class/cs224n/">Stanford CS 224N |Natural Language Processing with Deep Learning</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;
&lt;p&gt;传统的随机梯度下降算法SGD(Stochastic Gradient Descent)的式子为:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;&#92;[
&#92;theta_{t+1} &#92;leftarrow &#92;theta_</summary>
      
    
    
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="经典模块" scheme="https://bg51717.github.io/wiki/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9D%97/"/>
    
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="优化算法" scheme="https://bg51717.github.io/wiki/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
