<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Blogs</title>
  
  
  <link href="https://bg51717.github.io/wiki/atom.xml" rel="self"/>
  
  <link href="https://bg51717.github.io/wiki/"/>
  <updated>2024-11-21T00:59:30.295Z</updated>
  <id>https://bg51717.github.io/wiki/</id>
  
  <author>
    <name>bg51717</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>数学计算库:SymPy</title>
    <link href="https://bg51717.github.io/wiki/16846/"/>
    <id>https://bg51717.github.io/wiki/16846/</id>
    <published>2024-11-19T11:19:17.000Z</published>
    <updated>2024-11-21T00:59:30.295Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p><strong>SymPy</strong>是一个功能强大、开源的符号计算库（数值计算也同样支持），涵盖了代数、微积分、线性代数、数论、组合数学、物理学等领域的广泛功能。</p><p>这个库可以提供你想象到的所有数学操作，包括但不限于：</p><ul><li>符号计算：可以定义符号，然后求解方程或者优化问题的解析解。比如合并同类项，简化，展开，因式分解，求导，微分，解方程，极限，泰勒展开，数论，离散数学，统计，几何学等。</li><li>数值计算：作为数学库的必备功能，计算数值答案也是稳定支持的。</li><li>公式转 <code>latex</code>：对于部分符号，支持转化为<code>latex</code>代码。</li><li>可视化：提供了基本的可视化功能，包括绘制函数图像，几何图像，向量场，符号计算结果等。</li></ul><p>和别的科学计算库相比，<code>SymPy</code>的优势有：</p><ul><li>基于 <code>BSD</code>协议，开源免费</li><li>基于Python，方便使用，可以在交互窗口使用也可以在代码中作为库引入</li><li>轻量化，相比别的计算系统，<code>SymPy</code>需要的依赖少且体积小很多</li></ul><p>总之，个人感觉这个开源库提供了你所能想象到的所有数学相关的操作，也是许多常用的框架的前置库之一。当你遇到一些数学问题的时候，不妨先查询<code>SymPy</code>库能否解决。</p><h2 id="教程">教程</h2><p>由于这个库包罗万象，支持的功能过多，因此通过短短一篇博客了解或者快速查询到需要的功能是不太现实的。</p><p>因此建议使用的时候可以直接查看参考资料里的相关文档和伟大的<code>GPT</code>。</p><h2 id="参考资料">参考资料</h2><blockquote><ul><li><a href="https://docs.sympy.org/latest/tutorials/index.html">Tutorials -SymPy 1.13.3 documentation</a></li><li><a href="https://chatgpt.com/">Chatgpt</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;SymPy&lt;/strong&gt;
是一个功能强大、开源的符号计算库（数值计算也同样支持），涵盖了代数、微积分、线性代数、数论、组合数学、物理学等领域的广泛功能。&lt;/p&gt;
&lt;p&gt;这个库可以提供你想象到的所有数学操作，包括但不</summary>
      
    
    
    
    <category term="数学" scheme="https://bg51717.github.io/wiki/categories/%E6%95%B0%E5%AD%A6/"/>
    
    <category term="代码" scheme="https://bg51717.github.io/wiki/categories/%E6%95%B0%E5%AD%A6/%E4%BB%A3%E7%A0%81/"/>
    
    
    <category term="数学" scheme="https://bg51717.github.io/wiki/tags/%E6%95%B0%E5%AD%A6/"/>
    
    <category term="SymPy" scheme="https://bg51717.github.io/wiki/tags/SymPy/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读习惯</title>
    <link href="https://bg51717.github.io/wiki/22407/"/>
    <id>https://bg51717.github.io/wiki/22407/</id>
    <published>2024-11-11T13:20:18.000Z</published>
    <updated>2024-11-21T00:58:37.745Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p>发现<strong>论文阅读习惯</strong>是一个十分重要的事情。</p><p>在和师兄的交流当中发现，读完一篇文献总是会遗漏一部分内容，这是因为没有良好的论文阅读习惯导致无法从一个文献中获得足够的信息从而变成自己的东西。</p><p>因此考虑使用这篇博客记录、分享和培养自己的阅读习惯，进而形成一种规范。</p><p>当然，如果你作为有缘人看到这篇博客，希望我的阅读习惯可以为你带来一些帮助，我也十分欢迎有缘人一起在评论区分享自己的阅读习惯。</p><h2 id="关注重点">关注重点</h2><p>一篇论文，是为解决一个问题或者公布一个发现而存在的，因此关注的重点应该有。</p><ul><li>background：这个论文的背景包括，这个论文是关于什么问题或者现象而存在的，关于目前问题的解决办法或者现象的发现方法。</li><li>motivation：这个方法的发现动机是什么？是怎么发现或者提出这个方法的？</li><li>method：这个方法的过程具体是什么样的？具体是怎么解决问题和发现现象的？</li><li>contribution（advantages）：本文的主要贡献是什么？这个方法的优势，和别的方法的对比是什么？</li><li>experiment<ul><li>settings：验证方法的实验的设置具体是什么？通过设置可以复现实验。</li><li>comparisonresults：对比结果代表了这个方法的改进以及与过去方法的不同体现在哪里，包括不使用这个方法的效果会怎么样。结果可能会以图标或者文字的方式呈现。</li><li>ablationresults：消融实验代表了方法的内部组件或者不同设置的对比，可以体现不同组件的作用。呈现方式和对比实验一样。</li></ul></li><li>limitation：局限代表了作者认为方法的局限体现在哪里，可以作为日后改进的参考。</li></ul><h2 id="流程">流程</h2><blockquote><p>我目前是使用 <code>Zotero7</code>来进行论文的阅读。</p></blockquote><h3 id="大致了解">大致了解</h3><p>通过 <code>ChatGPT</code>或<code>博客文章分享</code>来首先对文章有个大致的了解。</p><h3 id="细致阅读笔记">细致阅读+笔记</h3><p>有了大致的了解后，对文章进行细致的阅读。</p><p>使用 <code>Zotero</code>阅读论文的时候，可以在右侧记笔记。</p><p><img src="/wiki/wiki/22407/1731333627028.png"></p><p>文章不同需要关注的点也可以画出来。</p><figure><img src="/wiki/wiki/22407/1731333678362.png" alt="img"><figcaption aria-hidden="true">img</figcaption></figure><table><colgroup><col style="width: 64%"><col style="width: 35%"></colgroup><thead><tr><th style="text-align: center;">颜色</th><th style="text-align: center;">论文重点</th></tr></thead><tbody><tr><td style="text-align: center;"><img src="/wiki/wiki/22407/1731336154271.png"></td><td style="text-align: center;">method</td></tr><tr><td style="text-align: center;"><img src="/wiki/wiki/22407/1731336257655.png"></td><td style="text-align: center;">motivation</td></tr><tr><td style="text-align: center;"><img src="/wiki/wiki/22407/1731336286381.png"></td><td style="text-align: center;">background</td></tr><tr><td style="text-align: center;"><img src="/wiki/wiki/22407/1731336410193.png"></td><td style="text-align: center;">experiment settings</td></tr><tr><td style="text-align: center;"><img src="/wiki/wiki/22407/1731336620113.png"></td><td style="text-align: center;">experiment comparison results</td></tr><tr><td style="text-align: center;"><img src="/wiki/wiki/22407/1731336562418.png"></td><td style="text-align: center;">experiment ablation results</td></tr><tr><td style="text-align: center;"><img src="/wiki/wiki/22407/1732150648533.png"></td><td style="text-align: center;">contribution(advantages)</td></tr><tr><td style="text-align: center;"><img src="/wiki/wiki/22407/1731336812112.png"></td><td style="text-align: center;">limitation</td></tr></tbody></table><p>其中背景色代表重要的，下划线代表重要性次一点的。</p><h3 id="代码">代码</h3><p>如果可以的话，尽量去尝试阅读代码，观察具体是如何实现的，同时也可以作为学习的过程，解答论文里不理解的点。</p><h3 id="博客">博客</h3><p>如果有足够时间，可以尝试去试着输出，通过输出可能会更好的把握内容。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;发现&lt;strong&gt;论文阅读习惯&lt;/strong&gt;是一个十分重要的事情。&lt;/p&gt;
&lt;p&gt;在和师兄的交流当中发现，读完一篇文献总是会遗漏一部分内容，这是因为没有良好的论文阅读习惯导致无法从一个文献中获得足够的信息从而变成自己的东西。&lt;/</summary>
      
    
    
    
    <category term="科研" scheme="https://bg51717.github.io/wiki/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文阅读" scheme="https://bg51717.github.io/wiki/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models</title>
    <link href="https://bg51717.github.io/wiki/58958/"/>
    <id>https://bg51717.github.io/wiki/58958/</id>
    <published>2024-11-09T09:25:51.000Z</published>
    <updated>2024-11-11T13:16:01.202Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p>很多针对LLM的PTQ量化算法在设计参数的时候都添加了太多的先验知识，导致性能不佳，尤其在低比特量化中。为了解决这个问题，本文提出了全向校准量化（OmniQuant）技术。</p><p>OmniQuant包含两个组件，可学习权重裁剪 (LWC) 和可学习等效变换 (LET)。LWC通过优化限幅阈值来调节权重的极值。同时，LET通过将量化的挑战从激活转移到权重来解决激活异常值。OmniQuant在使用逐块误差最小化的可微框架内运行。在校准的时候，OmniQuant冻结了模型原始的全精度权重，仅包含LWC和LET可学习的量化参数。</p><p>本文也利用了 <code>LLM.int8()</code>和<code>AWQ</code>中的几个结论：</p><ul><li>相比模型的权重，大模型的激活的异常值（绝对值远超别的参数）的绝对值更大。并且无论是哪个<code>token</code>或者哪个<code>Layer</code>，异常值都集中在固定的少数维度。</li><li>由于与激活相对应的权重的重要性，权重的量化误差在最终性能中也起着关键作用。</li></ul><h2 id="方法">方法</h2><h3 id="分块量化">分块量化</h3><p>和别的针对LLm的PTQ量化方法一样，为了降低LLM量化的复杂度，OmniQuant也是逐个block进行量化的。针对每个block，优化的目标函数为：</p><p><span class="math display">\[\arg \min_{\Theta_1, \Theta_2} \left\| \mathcal{F}(\mathbf{W},\mathbf{X}) - \mathcal{F} \left( Q_w(\mathbf{W}; \Theta_1, \Theta_2),Q_a(\mathbf{X}, \Theta_2) \right) \right\|\]</span></p><p>其中，<span class="math inline">\(\mathcal{F}\)</span> 表示 LLM中一个 Transformer 块的映射函数，<span class="math inline">\(\mathbf{W}\)</span> 和 <span class="math inline">\(\mathbf{X}\)</span>分别是全精度的权重和激活，<span class="math inline">\(Q_w(\cdot)\)</span> 和 <span class="math inline">\(Q_a(\cdot)\)</span>分别表示权重和激活的量化器，<span class="math inline">\(\Theta_1\)</span> 和 <span class="math inline">\(\Theta_2\)</span>分别是可学习权重裁剪（LWC）和可学习等效变换（LET）的量化参数。公式中的逐块量化在移动到下一个块之前，依次量化一个Transformer 块的参数。</p><p>以block作为量化优化的最小单位有两个好处：</p><ul><li>可以对LWC和LET的参数同时进行训练</li><li>可以显著减少资源需求</li></ul><h3 id="lwc-learnable-weight-clipping">LWC (Learnable WeightClipping)</h3><p>LWC考虑在量化函数里面加入可学习的参数，具体来说，LWC优化了一个Clip过程，其公式如下：</p><p><span class="math display">\[\mathbf{W}_q = \text{clamp}\left(\left\lfloor \frac{\mathbf{W}}{h}\right\rceil + z, 0, 2^N - 1\right), \quad \text{其中 } h = \frac{\gamma\max(\mathbf{W}) - \beta \min(\mathbf{W})}{2^N - 1}, \quad z =-\left\lfloor \frac{\beta \min(\mathbf{W})}{h} \right\rceil\]</span></p><p>公式里面的<span class="math inline">\(\left\lfloor \cdot\right\rceil\)</span>代表取整操作，<span class="math inline">\(N\)</span>是量化的比特数，<span class="math inline">\(\mathbf{W}_q\)</span> 和 <span class="math inline">\(\mathbf{W}\)</span>分别表示量化后的权重和全精度权重。<span class="math inline">\(h\)</span>是权重的归一化因子，<span class="math inline">\(z\)</span>是零点值。clamp 操作将值限制在 <span class="math inline">\(N\)</span>位整数范围内，具体为 <span class="math inline">\([0, 2^N -1]\)</span>。在公式中，<span class="math inline">\(\gamma \in [0,1]\)</span> 和 <span class="math inline">\(\beta \in [0, 1]\)</span>是分别针对权重上界和下界的可学习剪辑强度。我们通过 sigmoid 函数对 <span class="math inline">\(\gamma\)</span> 和 <span class="math inline">\(\beta\)</span> 进行初始化。因此，在公式 (1)中，<span class="math inline">\(\Theta_1 = \{\gamma,\beta\}\)</span>。</p><p>其中<span class="math inline">\(h,z\)</span>是个整型，如果直接学习可能难度较大。</p><h3 id="let-learnable-equivalent-transformation">LET (LearnableEquivalent Transformation)</h3><p>考虑到激活中的异常值是系统性的并且对于特定通道是固定的，follow之前的工作，使用通道级缩放和通道级移位来操纵激活分布。参数方面作者认为之前的工作是手动设计的参数，导致结果可能不是很理想。</p><p>OmniQuant 没有引入额外的计算或参数，因为和AWQ量化算法一样，LWC中的限幅阈值和 LET 中的等效因子可以融合为量化权重。</p><h4 id="linear">Linear</h4><p>线性层接受一个输入标记序列 <span class="math inline">\(\mathbf{X} \in\mathbb{R}^{T \times C_\text{in}}\)</span>，其中 <span class="math inline">\(T\)</span> 是标记长度，其操作是权重矩阵 <span class="math inline">\(\mathbf{W} \in \mathbb{R}^{C_\text{in} \timesC_\text{out}}\)</span> 与偏置向量 <span class="math inline">\(\mathbf{B}\in \mathbb{R}^{1 \times C_\text{out}}\)</span>的乘积。一个数学等价的线性层表示为：</p><p><span class="math display">\[\mathbf{Y} = \mathbf{X} \mathbf{W} + \mathbf{B} =\underbrace{\left(\mathbf{X} - \delta\right) \odots}_{\tilde{\mathbf{X}}} \cdot \underbrace{\left(s \odot\mathbf{W}\right)}_{\tilde{\mathbf{W}}} + \underbrace{\left(\mathbf{B} +\delta \mathbf{W}\right)}_{\tilde{\mathbf{B}}}\]</span></p><p>其中，<span class="math inline">\(\mathbf{Y}\)</span> 表示输出，<span class="math inline">\(s \in \mathbb{R}^{1 \times C_\text{in}}\)</span>和 <span class="math inline">\(\delta \in \mathbb{R}^{1 \timesC_\text{in}}\)</span> 分别是通道级的缩放和偏移参数。<span class="math inline">\(\tilde{\mathbf{X}}\)</span>，<span class="math inline">\(\tilde{\mathbf{W}}\)</span> 和 <span class="math inline">\(\tilde{\mathbf{B}}\)</span>分别是等效的激活、权重和偏置，符号“<span class="math inline">\(\odot\)</span>”表示元素级的除法和乘法。</p><p>通过公式，激活被转化为量化友好的形式，但代价是增加了权重量化的难度。在此意义上，LWC可以改善通过LET实现的权重-激活量化的性能，因为它使权重更加量化友好。最终，我们对转换后的激活和权重进行量化，公式如下：</p><p><span class="math display">\[\mathbf{Y} = Q_a(\tilde{\mathbf{X}}) Q_w(\tilde{\mathbf{W}}) +\tilde{\mathbf{B}}\]</span></p><p>其中，<span class="math inline">\(Q_a\)</span> 是普通的 MinMax量化器，<span class="math inline">\(Q_w\)</span>是带有可学习权重剪辑（即 LWC）的 MinMax 量化器。</p><p>请注意，<span class="math inline">\(\tilde{\mathbf{X}}\)</span>中的缩放和偏移参数可以吸收进前面的归一化或线性层，而 <span class="math inline">\(\tilde{\mathbf{W}}\)</span>中的缩放因子可以融合进原始线性权重 <span class="math inline">\(\mathbf{W}\)</span>中。因此，公式中的等效变换可以在不引入额外参数或成本的情况下有效减少量化误差。</p><p>OmniQuant在 LLM 的线性层（除了 FFN的第二个线性层）中使用此等效变换。这可能是因为在应用可学习的等效变换时，非线性层后的特征高度稀疏，导致梯度不稳定。</p><h4 id="attention-operation">Attention operation</h4><p>除了线性层外，注意力操作也占据了计算的一个重要部分。此外，LLM的自回归模式需要存储每个标记的键值（KV）缓存，这对于长序列带来了大量的内存需求。因此，考虑把<span class="math inline">\(Q,K,V\)</span>矩阵在权重-激活量化设置中量化为低比特值。具体而言，自注意力相似度矩阵的可学习等效变换可写为：</p><p><span class="math display">\[\mathbf{P} = \text{Softmax}(\mathbf{Q} \mathbf{K}^T) = \text{Softmax}\left( \underbrace{\mathbf{Q} \odot s_a}_{\tilde{\mathbf{Q}}} \,\underbrace{(s_a \odot \mathbf{K}^T)}_{\tilde{\mathbf{K}}^T} \right).\]</span></p><p>其中，<span class="math inline">\(s_a \in \mathbb{R}^{1 \timesC_\text{out}}\)</span>是相似度矩阵中的缩放因子。量化的相似度矩阵计算表示为 <span class="math inline">\(\mathbf{P} =\text{Softmax}(Q_a(\tilde{\mathbf{Q}})Q_a(\tilde{\mathbf{K}}^T))\)</span>。这里我们同样使用 MinMax 量化方案<span class="math inline">\(Q_a\)</span> 对 <span class="math inline">\(\tilde{\mathbf{Q}}/\tilde{\mathbf{K}}\)</span>矩阵进行量化。我们可以得出 <span class="math inline">\(\Theta_2 = \{\delta, s, s_a \}\)</span> 。</p><p><span class="math inline">\(\tilde{\mathbf{Q}}\)</span> 和 <span class="math inline">\(\tilde{\mathbf{K}}\)</span>中的通道级缩放因子，可以分别吸收到查询和键投影的线性权重中。值得一提的是，由于逆变换操作的存在，输出投影线性层中的显式变换在其分布上已按通道维度进行了更改，因此省略了<span class="math inline">\(\mathbf{V}\)</span> 的变换。</p><h2 id="实验">实验</h2><p>首先测试了只量化模型权重的结果，和GPTQ和AWQ进行了对比，取得了在WikiText2上最低的困惑度。</p><p><img src="/wiki/wiki/58958/1731158459757.png"></p><p>也测试了权重和激活都进行量化的表现，对比的baseline是SmoothQuant和LLM-QAT，可以看到甚至比LLM-QAT这种QAT量化方法还要好。</p><p><img src="/wiki/wiki/58958/1731158796713.png"></p><p>然后还参考AWQ的论文，测试了模型回答问题的准确率。把两个模型的回答连接在一起，让GPT-4判断哪个回答更好，为了去除顺序的影响，会交换顺序让GPT-4再回答一次，结果如下图所示：</p><p><img src="/wiki/wiki/58958/1731158884137.png"></p><p>最后测试了模型的效率情况，统计了算法在不同规模的Llama上的存储需求和推理速度。</p><p><img src="/wiki/wiki/58958/1731159099167.png"></p><p>这篇论文的附录罗列了很多细节，需要的可以去看原文。</p><h2 id="代码">代码</h2><p>论文源码：<a href="https://github.com/OpenGVLab/OmniQuant/tree/main">OpenGVLab/OmniQuant:[ICLR2024 spotlight] OmniQuant is a simple and powerful quantizationtechnique for LLMs.</a></p><h2 id="参考资料">参考资料</h2><blockquote><ul><li><a href="https://arxiv.org/abs/2308.13137">[2308.13137] OmniQuant:Omnidirectionally Calibrated Quantization for Large LanguageModels</a></li><li><a href="https://github.com/OpenGVLab/OmniQuant/tree/main">OpenGVLab/OmniQuant:[ICLR2024 spotlight] OmniQuant is a simple and powerful quantizationtechnique for LLMs.</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;很多针对LLM的PTQ量化算法在设计参数的时候都添加了太多的先验知识，导致性能不佳，尤其在低比特量化中。为了解决这个问题，本文提出了全向校准量化（OmniQuant）技术。&lt;/p&gt;
&lt;p&gt;OmniQuant包含两个组件，可学习权重裁剪</summary>
      
    
    
    
    <category term="科研" scheme="https://bg51717.github.io/wiki/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文阅读" scheme="https://bg51717.github.io/wiki/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>OneBit: Towards Extremely Low-bit Large Language Models</title>
    <link href="https://bg51717.github.io/wiki/341/"/>
    <id>https://bg51717.github.io/wiki/341/</id>
    <published>2024-11-08T12:33:02.000Z</published>
    <updated>2024-11-11T03:20:58.488Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p>OneBit属于量化方法中的量化感知训练QAT。基于BitNet的OneBit将LLM的权重矩阵量化为1位，用一种新颖的1 位参数表示方法以更好地量化LLM，以及一种基于矩阵分解的有效参数初始化方法以提高量化框架的收敛速度。</p><h2 id="方法">方法</h2><h3 id="bitlinear">BitLinear</h3><p>OneBitfollow的工作是BitNet，BitNet提出BitLinear，BitLinear的forward函数为：</p><p><span class="math display">\[\begin{align*}&amp; \mathbf{W}_{\pm 1} = \operatorname{Sign}\left( \mathbf{W} -\operatorname{Mean}(\mathbf{W}) \right), \\&amp; \eta = \operatorname{Mean} \left( \operatorname{Abs} \left(\mathbf{W} - \operatorname{Mean}(\mathbf{W}) \right) \right), \\&amp; \mathbf{Y} = \eta \cdot \operatorname{LayerNorm}(\mathbf{X})\mathbf{W}_{\pm 1}^{\top},\end{align*}\]</span></p><p>其中，<span class="math inline">\(W\)</span>表示量化后的权重矩阵，形状为 <span class="math inline">\(m \times n\)</span>，<span class="math inline">\(W_{±1}\)</span>表示 1 位量化矩阵。<span class="math inline">\(X\)</span>是线性层的输入，<span class="math inline">\(Y\)</span> 是输出。函数 <span class="math inline">\(Sign()\)</span>、<span class="math inline">\(Mean()\)</span> 和 <span class="math inline">\(Abs()\)</span>分别返回符号矩阵、平均值和绝对值矩阵。</p><p>作者认为，缺失的浮点精度仍然破坏了模型性能，因此额外引入了两个始终保持fp16精度的向量：</p><p><span class="math display">\[\begin{align*}&amp; \mathbf{W}_{\pm 1} = \operatorname{Sign}(\mathbf{W}), \\&amp; \mathbf{Y} = \left[ (\mathbf{X} \odot \mathbf{g}) \mathbf{W}_{\pm1}^{\top} \right] \odot \mathbf{h}, \\&amp; \mathbf{Z} = \operatorname{LayerNorm}(\mathbf{Y}),\end{align*}\]</span></p><p>其中的<span class="math inline">\(g\)</span>和<span class="math inline">\(h\)</span>是fp16精度的向量，<span class="math inline">\(Z\)</span>是最终的输出。同时括号严格要求了计算顺序从而减小计算成本。其余的和BitNet保持一致。</p><p>训练的时候，不需要保存一个高精度的参数矩阵，只需要保存两个高精度的向量即可。</p><h3 id="svid">SVID</h3><p>为了可以使用训练好的模型的ckpt来初始化量化模型的权重，这篇论文还引入了Sign-Value-IndependentDecomposition (SVID)，所以近似初始化方法为，</p><p><span class="math display">\[\mathbf{W} \approx \mathbf{W}_{\text{sign}} \odot (\mathbf{a}\mathbf{b}^{\top})\]</span></p><p>就是用训练好模型的权重矩阵<span class="math inline">\(W\)</span>，分解为量化模型的三部分<span class="math inline">\(\mathbf{W}_{\text{sign}} , \mathbf{a} ,\mathbf{b}\)</span>。</p><p>这里提出两个命题来说明合理性：</p><p><strong>命题一</strong>：使用上面的近似，我们可以得到，</p><p><span class="math display">\[\mathbf{X} \mathbf{W}^{\top} \approx \left[ (\mathbf{X} \odot\mathbf{b}^{\top}) \mathbf{W}_{\text{sign}}^{\top} \right] \odot\mathbf{a}^{\top}\]</span></p><p>根据这个命题，可以从近似初始化得到BitLinear的forward公式。</p><blockquote><p>证明过程：</p><p>令<span class="math inline">\(s_{ij}\)</span>为1Bit矩阵<span class="math inline">\(W_{sign}\)</span>的一个元素，我们很容易得到：<span class="math inline">\(w_{i,j} \approx s_{i,j} \cdot a_i b_j\)</span>。</p><p>因此，我们有，</p><p><span class="math display">\[\begin{align*}\left( \mathbf{X} \mathbf{W}^{\top} \right)_{ij} &amp; = \sum_k x_{ik}w_{kj} \\&amp; = \sum_k x_{ik} w_{jk} \\&amp; \approx \sum_k x_{ik} s_{jk} a_j b_k \\&amp; = \sum_k x_{ik} b_k s_{jk} a_j \\&amp; = \sum_k \left( \mathbf{X} \odot \mathbf{b}^{\top} \right)_{ik}s_{kj}^{\top} a_j \\&amp; = \left[ \left( \mathbf{X} \odot \mathbf{b}^{\top} \right)\mathbf{W}_{\text{sign}}^{\top} \right]_{ij} a_j \\&amp; = \left\{ \left[ \left( \mathbf{X} \odot \mathbf{b}^{\top} \right)\mathbf{W}_{\text{sign}}^{\top} \right] \odot \mathbf{a}^{\top}\right\}_{ij}\end{align*}\]</span></p></blockquote><p><strong>命题二</strong>：给定矩阵 $W $和 <span class="math inline">\(|W|\)</span>，其中 <span class="math inline">\(W =W_{\text{sign}} \odot |W|\)</span>。以如下方式对这些矩阵进行分解：<span class="math inline">\(W = \mathbf{a} \mathbf{b}^{\top} +\mathbf{E}_1\)</span>和<span class="math inline">\(|W| =\tilde{\mathbf{a}} \tilde{\mathbf{b}}^{\top} +\mathbf{E}_2\)</span>，其中<span class="math inline">\(E_i\)</span>表示误差矩阵。就 Frobenius范数而言，SVD 分解比原始矩阵 $W $更接近：</p><p><span class="math display">\[\left\| W - W_{\text{sign}} \odot \tilde{\mathbf{a}}\tilde{\mathbf{b}}^{\top} \right\|_F^2 \leq \left\| W - \mathbf{a}\mathbf{b}^{\top} \right\|_F^2.\]</span></p><p>根据这个命题，可以说明提出符号位可以更好地减小误差。</p><blockquote><p><strong>引理 1</strong> 令 <span class="math inline">\(\sigma_i(\mathbf{W})\)</span> 表示矩阵 <span class="math inline">\(\mathbf{W}\)</span> 的第 <span class="math inline">\(i\)</span> 大奇异值。则以下不等式成立：</p><p><span class="math display">\[\sigma_1(|\mathbf{W}|) \geq \sigma_1(\mathbf{W}).\]</span></p><p><strong>引理证明</strong> 根据诱导范数的定义，有</p><p><span class="math display">\[\sigma_1(\mathbf{W}) = \|\mathbf{W}\|_2 = \max_{\mathbf{x},\|\mathbf{x}\|_2=1} \|\mathbf{W} \mathbf{x}\|_2,\]</span></p><p><span class="math display">\[\sigma_1(|\mathbf{W}|) = \||\mathbf{W}|\|_2 = \max_{\mathbf{y},\|\mathbf{y}\|_2=1} \||\mathbf{W}| \mathbf{y}\|_2.\]</span></p><p>注意对于任意 <span class="math inline">\(\mathbf{x}\)</span>，<span class="math inline">\(\|\mathbf{x}\|_2 = 1\)</span>，我们有</p><p><span class="math display">\[\||\mathbf{W}| \mathbf{x}\|_2^2 = \sum_i \left( \sum_j |w_{ij}||x_j|\right)^2 \geq \sum_i \left( \sum_j w_{ij} x_j \right)^2 = \|\mathbf{W}\mathbf{x}\|_2^2.\]</span></p><p>因此</p><p><span class="math display">\[\max_{\mathbf{y}, \|\mathbf{y}\|_2=1} \||\mathbf{W}| \mathbf{y}\|_2 \geq\max_{\mathbf{x}, \|\mathbf{x}\|_2=1} \|\mathbf{W} \mathbf{x}\|_2.\]</span></p><p>该引理得证。</p><hr><p><strong>命题证明</strong> 考虑通过 SVD 来证明。对于 SVD分解，因为使用两个向量近似，等同于只保留最大的奇异值和对应的特征向量，rank-1近似中的误差矩阵 <span class="math inline">\(\mathbf{E}\)</span>的范数等于所有奇异值平方和中除去最大奇异值后的总和。我们有</p><p><span class="math display">\[\|\mathbf{E}_1\|_F^2 = \sum_{i=2}^n \sigma_i^2(\mathbf{W}),\]</span></p><p><span class="math display">\[\|\mathbf{E}_2\|_F^2 = \sum_{i=2}^n \sigma_i^2(|\mathbf{W}|).\]</span></p><p>根据 <span class="math inline">\(\|\mathbf{W}\|_F^2 =\||\mathbf{W}|\|_F^2\)</span>，我们有</p><p><span class="math display">\[\sum_{i=1}^n \sigma_i^2(\mathbf{W}) = \sum_{i=1}^n\sigma_i^2(|\mathbf{W}|).\]</span></p><p>根据引理 1，我们可以得出</p><p><span class="math display">\[\|\mathbf{E}_1\|_F^2 \geq \|\mathbf{E}_2\|_F^2.\]</span></p><p>根据该命题中的方程，我们可以表示</p><p><span class="math display">\[\mathbf{W}_{\text{sign}} \odot |\mathbf{W}| = \mathbf{W}_{\text{sign}}\odot (\tilde{\mathbf{a}} \tilde{\mathbf{b}}^{\top} + \mathbf{E}_2).\]</span></p><p>因此我们有</p><p><span class="math display">\[\mathbf{W} - \mathbf{W}_{\text{sign}} \odot \tilde{\mathbf{a}}\tilde{\mathbf{b}}^{\top} = \mathbf{W}_{\text{sign}} \odot \mathbf{E}_2.\]</span></p><p>因此</p><p><span class="math display">\[\|\mathbf{W}_{\text{sign}} \odot \mathbf{E}_2\|_F^2 = \sum_{i,j}s_{ij}^2 e_{ij}^2 = \sum_{i,j} e_{ij}^2 = \|\mathbf{E}_2\|_F^2 \leq\|\mathbf{E}_1\|_F^2,\]</span></p><p>其中 <span class="math inline">\(s_{ij} = \pm 1\)</span> 是 <span class="math inline">\(\mathbf{W}_{\text{sign}}\)</span>的元素。由此，该命题中的不等式得证。</p></blockquote><h3 id="知识蒸馏">知识蒸馏</h3><p>考虑在量化模型训练的时候加入知识蒸馏，大的教师模型代表量化前的模型，小的学生模型代表量化后的模型。损失由两部分组成，首先是交叉熵损失，</p><p><span class="math display">\[\mathcal{L}_{\text{CE}} = -\frac{1}{n_s} \sum_{i=1}^{n_s} \sum_{c}P_c^{T}(o_i) \log P_c^{S}(o_i)\]</span></p><p>第二个是所以hidden states归一化后的差值的L2范数，</p><p><span class="math display">\[\mathcal{L}_{\text{MSE}} = \sum_{i=1}^{n_s} \sum_{j=1}^{n_l} \left\|\frac{q_{i,j}^{T}}{\|q_{i,j}^{T}\|_2} -\frac{q_{i,j}^{S}}{\|q_{i,j}^{S}\|_2} \right\|_2^2\]</span></p><p>总的损失为：</p><p><span class="math display">\[\mathcal{L}_{\text{KD}} = \mathcal{L}_{\text{CE}} + \alpha\mathcal{L}_{\text{MSE}}\]</span></p><blockquote><p>由于符号十分常用，因此这里省去对于其中的符号的解释说明。</p></blockquote><p>训练的时候，使用是模拟量化进行训练，即原来的权重也会进行训练，部署的时候再压缩：</p><p><img src="/wiki/wiki/341/1731079859743.png"></p><h2 id="实验">实验</h2><p>首先测试了该量化方法和一些PTQ方法和QAT方法的困惑度比较，其中的baseline为W2A16量化，本文方法为W1A16，可以看到取得了较大领先，</p><p><img src="/wiki/wiki/341/1731079794197.png"></p><p>为了评估实际解决问题的能力，还测试了和一些小模型能力的差异和资源的需求，其中的OneBit是本文方法训练的模型，LowRankLLaMA是通过低秩分解压缩的Llama模型：</p><p><img src="/wiki/wiki/341/1731080140597.png"></p><h2 id="代码">代码</h2><p>论文源码：<a href="https://github.com/xuyuzhuang11/OneBit">xuyuzhuang11/OneBit: Thehomepage of OneBit model quantization framework.</a></p><p>个人认为其中比较重要的部分：</p><ul><li><a href="https://github.com/xuyuzhuang11/OneBit/blob/main/transformers/src/transformers/models/bitnet.py">OneBit/transformers/src/transformers/models/bitnet.pyat main · xuyuzhuang11/OneBit</a></li><li><a href="https://github.com/xuyuzhuang11/OneBit/blob/main/scripts/build_start_ckpt.py">OneBit/scripts/build_start_ckpt.pyat main · xuyuzhuang11/OneBit</a></li></ul><h2 id="参考资料">参考资料</h2><blockquote><ul><li><a href="https://arxiv.org/abs/2402.11295">[2402.11295] OneBit:Towards Extremely Low-bit Large Language Models</a></li><li><a href="https://github.com/xuyuzhuang11/OneBit">xuyuzhuang11/OneBit: Thehomepage of OneBit model quantization framework.</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;OneBit属于量化方法中的量化感知训练QAT。基于BitNet的OneBit将LLM的权重矩阵量化为1位，用一种新颖的
1 位参数表示方法以更好地量化
LLM，以及一种基于矩阵分解的有效参数初始化方法以提高量化框架的收敛速度。&lt;/p</summary>
      
    
    
    
    <category term="科研" scheme="https://bg51717.github.io/wiki/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文阅读" scheme="https://bg51717.github.io/wiki/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="量化" scheme="https://bg51717.github.io/wiki/tags/%E9%87%8F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>矩阵奇异值分解SVD</title>
    <link href="https://bg51717.github.io/wiki/33996/"/>
    <id>https://bg51717.github.io/wiki/33996/</id>
    <published>2024-11-08T05:26:13.000Z</published>
    <updated>2024-11-08T17:00:43.626Z</updated>
    
    <content type="html"><![CDATA[<h2 id="定理陈述">定理陈述</h2><p>对于任意 <span class="math inline">\(m \times n\)</span> 的矩阵 <span class="math inline">\(A\)</span>，存在一个分解：</p><p><span class="math display">\[A = U \Sigma V^T\]</span></p><p>其中：</p><ul><li>r=rank(A)是矩阵A的秩</li><li><span class="math inline">\(U\)</span> 是一个 <span class="math inline">\(m \times m\)</span> 的正交矩阵（即 <span class="math inline">\(U^T U = I\)</span>），前r列是矩阵<span class="math inline">\(A A^\top\)</span>的非零特征值的特征向量。</li><li><span class="math inline">\(V\)</span> 是一个 <span class="math inline">\(n \times n\)</span> 的正交矩阵（即 <span class="math inline">\(V^T V = I\)</span>），前r列是矩阵$ A^A$的非零特征值的特征向量。</li><li><span class="math inline">\(\Sigma\)</span> 是一个 <span class="math inline">\(m \times n\)</span>的对角矩阵，其对角线元素是非负实数，前r个非负实数称为A的奇异值。</li></ul><p>分解结果不是唯一的，不同分解的区别主要在于特征向量和奇异值的顺序。</p><h2 id="证明">证明</h2><blockquote><p>参考自：<a href="https://blog.csdn.net/THUChina/article/details/102576718">奇异值分解（SVD）推导证明与应用_简述奇异值分解定理,并进行推导与证明(公式以及矩阵可以写在纸上照下来)-CSDN博客</a></p></blockquote><p>因为<span class="math inline">\(AA^\top\)</span>是半正定矩阵，所以存在正交矩阵<span class="math inline">\(V_{n\times n}\)</span>，</p><p><span class="math display">\[V^T (A^T A) V = \operatorname{diag}(\lambda_1, \lambda_2, \dots,\lambda_n)\]</span></p><p>其中<span class="math inline">\(\lambda_1 &gt; \lambda_2 &gt; \cdots&gt; \lambda_r &gt; 0 = \lambda_{r+1} = \cdots =\lambda_n\)</span>为<span class="math inline">\(AA^\top\)</span>的n个非负特征根。</p><p>令<span class="math inline">\(\Sigma_1 =\operatorname{diag}(\sigma_1, \sigma_2, \dots, \sigma_r) =\operatorname{diag}(\sqrt{\lambda_1}, \sqrt{\lambda_2}, \dots,\sqrt{\lambda_r}), \quad V_1 = [\mathbf{v}_1, \mathbf{v}_2, \dots,\mathbf{v}_r], \quad V_2 = [\mathbf{v}_{r+1}, \mathbf{v}_{r+2}, \dots,\mathbf{v}_n],V=[V_1,V_2]\)</span>，所以根据<span class="math inline">\(V\)</span>定义，两个式子左边乘以<span class="math inline">\(V\)</span>，得到</p><p><span class="math display">\[A^\top A V=V\Sigma_1\]</span></p><p>取其中的r个列向量，有</p><p><span class="math display">\[A^\top A V_1=V_1\Sigma_1\]</span></p><p>进一步，我们可以得到</p><p><span class="math display">\[\Sigma_1^{-1} V_1^\top A^\top A V_1 \Sigma_1^{-1} = I\]</span></p><p>令 <span class="math inline">\(U_1 = A V_1\Sigma_1^{-1}\)</span>，则有 <span class="math inline">\(U_1^\top U_1 =I\)</span>。此时，我们可以选择 <span class="math inline">\(m -r\)</span> 组标准正交向量与 <span class="math inline">\(U_1\)</span>的列向量组成一组标准正交基，也即 <span class="math inline">\([U_1,U_2]\)</span> 是一个 <span class="math inline">\(m \times m\)</span>阶正交矩阵，且 <span class="math inline">\(U_1^\top U_2 =0\)</span>。</p><p>另一方面，容易得到：</p><p><span class="math display">\[A^\top A V_2 = V_2 0 \Rightarrow V_2^\top A^\top A V_2 = 0\]</span></p><p>根据矩阵性质<span class="math inline">\(A = 0 \Leftrightarrow A^\topA = 0\)</span>，我们可以得到<span class="math inline">\(AV_2=0\)</span>。</p><p><span class="math display">\[U^\top A V = \begin{bmatrix} U_1^\top A V_1 &amp; U_1^\top A V_2 \\U_2^\top A V_1 &amp; U_2^\top A V_2 \end{bmatrix}= \begin{bmatrix} \Sigma_1 &amp; 0 \\ U_2^T U_1 \Sigma_1 &amp; 0\end{bmatrix}= \begin{bmatrix} \Sigma_1 &amp; 0 \\ 0 &amp; 0 \end{bmatrix}= \Sigma\]</span></p><p>即，</p><p><span class="math display">\[U A V^\top=\Sigma\]</span></p><h2 id="几何理解">几何理解</h2><p>如果几何变换的方法为<span class="math inline">\(XA\)</span>，那么SVD将矩阵的线性变换拆解为三步：旋转（或方向调整）<span class="math inline">\(U\)</span>- 缩放 <span class="math inline">\(\Sigma\)</span> - 再旋转 <span class="math inline">\(V\)</span>，其中的缩放操作可以理解为，首先奇异值会对向量进行缩放：</p><ul><li>如果m&lt;n，会额外增加一些为0的维度</li><li>如果m&gt;n，会删去一些维度</li></ul><h2 id="应用">应用</h2><p>在机器学习的PCA中使用较多，作为特征工程降低数据维度的一个重要方法。</p><p>在一些信号领域也可以用于去噪或者图像领域的数据压缩。</p><h2 id="代码">代码</h2><p>Pytorch:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例矩阵</span></span><br><span class="line">A = torch.tensor([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>, <span class="number">9.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 SVD</span></span><br><span class="line">U, S, V = torch.svd(A)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"U:"</span>, U)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"S:"</span>, S)  <span class="comment"># 奇异值向量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"V:"</span>, V)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>numpy：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例矩阵</span></span><br><span class="line">A = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 SVD</span></span><br><span class="line">U, sigma, VT = np.linalg.svd(A)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"U:"</span>, U)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Sigma:"</span>, sigma)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"V^T:"</span>, VT)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>scikit-learn:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> TruncatedSVD</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例矩阵</span></span><br><span class="line">A = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化 TruncatedSVD，指定保留的奇异值数量</span></span><br><span class="line">svd = TruncatedSVD(n_components=<span class="number">2</span>)</span><br><span class="line">A_reduced = svd.fit_transform(A)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Reduced Matrix (A_reduced):"</span>)</span><br><span class="line"><span class="built_in">print</span>(A_reduced)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Explained Variance Ratio:"</span>, svd.explained_variance_ratio_)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h2 id="参考资料">参考资料</h2><blockquote><ul><li><a href="https://blog.csdn.net/THUChina/article/details/102576718">奇异值分解（SVD）推导证明与应用_简述奇异值分解定理,并进行推导与证明(公式以及矩阵可以写在纸上照下来)-CSDN博客</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;定理陈述&quot;&gt;定理陈述&lt;/h2&gt;
&lt;p&gt;对于任意 &lt;span class=&quot;math inline&quot;&gt;&#92;(m &#92;times n&#92;)&lt;/span&gt; 的矩阵 &lt;span class=&quot;math inline&quot;&gt;&#92;(A&#92;)&lt;/span&gt;，存在一个分解：&lt;/p&gt;
&lt;p&gt;&lt;s</summary>
      
    
    
    
    <category term="数学" scheme="https://bg51717.github.io/wiki/categories/%E6%95%B0%E5%AD%A6/"/>
    
    <category term="线性代数" scheme="https://bg51717.github.io/wiki/categories/%E6%95%B0%E5%AD%A6/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
    
    <category term="矩阵" scheme="https://bg51717.github.io/wiki/tags/%E7%9F%A9%E9%98%B5/"/>
    
    <category term="SVD" scheme="https://bg51717.github.io/wiki/tags/SVD/"/>
    
  </entry>
  
  <entry>
    <title>梯度估计STE</title>
    <link href="https://bg51717.github.io/wiki/13277/"/>
    <id>https://bg51717.github.io/wiki/13277/</id>
    <published>2024-11-04T12:15:47.000Z</published>
    <updated>2024-11-08T12:36:20.417Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景">背景</h2><p>反向传播是现在训练模型的重要方法，但是在部分场景下，会遇到不可微分的函数，从而导致梯度传播失败。比如量化里的取整函数。因此，需要对梯度进行估计然后反向传播。</p><p><strong>STE(Straight-Through Estimator)</strong>是2013年YoshuaBengio等人针对梯度估计进行了研究，那篇论文提出了几种梯度估计的方法，并推导出了一些理论性质，然后通过实验证明，STE是效果最好的方法。</p><blockquote><p>由于那篇论文很多篇幅在介绍较为复杂的估计方法，且理论推导也极为复杂，效果没有简洁的STE好，因此不对其进行详细介绍。</p></blockquote><h2 id="应用">应用</h2><h3 id="恒等函数">恒等函数</h3><p>STE在反向传播理论推导的时候，把不可微的原子函数（比如量化函数里有放缩和取整两部分，其中取整是不可微的原子函数）替换为恒等函数。</p><p>这种应用可以大大减少理论推导的难度，但是在代码里应用反向传播的时候不太方便，以pytorch框架为例，可能需要自己手写函数类<code>torch.autograd.Function</code>，具体文档可以查看：<a href="https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html">PyTorch:Defining New autograd Functions — PyTorch Tutorials 2.5.0+cu124documentation</a> 。</p><h3 id="sg函数">SG函数</h3><p>苏神在他的博客（<a href="https://spaces.ac.cn/archives/6760">VQ-VAE的简明介绍：量子化自编码器- 科学空间|Scientific Spaces</a>）中提出了一个函数<code>sg(stop gradient)</code>，代表梯度反向传播终止的恒等函数。</p><p>所以原来的不可微的原子函数可以写为<span class="math inline">\(f(x)=x+sg(x'-x)\)</span>，在前向传播的时候和x'相同，在反向传播的时候梯度等于直接对<code>x</code>反向传播梯度。</p><p>这个式子也能用于理论推导，但是不如视为恒等函数麻烦，但是在代码方面容易完成，可以使用pytorch的默认反向传播函数，不需要自定义<code>torch.autograd.Function</code>（当然自定义也可以完成任务）。</p><p><a href="https://github.com/kyegomez/BitNet/blob/main/bitnet/bitlinear.py">BitNet/bitnet/bitlinear.pyat main · kyegomez/BitNet</a> 代码示例：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># w_quant 是 w 量化（不可微）后的值</span></span><br><span class="line"><span class="comment"># STE using detach</span></span><br><span class="line">w_quant = w + (weight_quant(w) - w).detach()</span><br></pre></td></tr></tbody></table></figure><p>这里也给出 <code>torch.autograd.Function</code>的一个示例(<a href="https://github.com/xuyuzhuang11/OneBit/blob/main/transformers/src/transformers/models/bitnet.py">OneBit/transformers/src/transformers/models/bitnet.pyat main · xuyuzhuang11/OneBit</a>)：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SignSTEFunc</span>(torch.autograd.Function):</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, <span class="built_in">input</span></span>):</span><br><span class="line">        ctx.save_for_backward(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.sign(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_output</span>):</span><br><span class="line">        <span class="built_in">input</span>, = ctx.saved_tensors</span><br><span class="line">        <span class="keyword">return</span> grad_output * (<span class="number">1.001</span> - torch.tanh(<span class="built_in">input</span>) ** <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># return grad_output * (1.01 - torch.tanh(input) ** 2)</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h2 id="参考资料">参考资料</h2><blockquote><ul><li><a href="https://arxiv.org/abs/1308.3432">[1308.3432] Estimating orPropagating Gradients Through Stochastic Neurons for ConditionalComputation</a></li><li><a href="https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html">PyTorch:Defining New autograd Functions — PyTorch Tutorials 2.5.0+cu124documentation</a></li><li><a href="https://spaces.ac.cn/archives/6760">VQ-VAE的简明介绍：量子化自编码器- 科学空间|Scientific Spaces</a></li><li><a href="https://github.com/kyegomez/BitNet">kyegomez/BitNet:Implementation of "BitNet: Scaling 1-bit Transformers for Large LanguageModels" in pytorch</a></li><li><a href="https://github.com/xuyuzhuang11/OneBit/blob/main/transformers/src/transformers/models/bitnet.py">OneBit/transformers/src/transformers/models/bitnet.pyat main · xuyuzhuang11/OneBit</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;
&lt;p&gt;反向传播是现在训练模型的重要方法，但是在部分场景下，会遇到不可微分的函数，从而导致梯度传播失败。比如量化里的取整函数。因此，需要对梯度进行估计然后反向传播。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;STE(Straight-Through Es</summary>
      
    
    
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="经典模块" scheme="https://bg51717.github.io/wiki/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9D%97/"/>
    
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="梯度估计" scheme="https://bg51717.github.io/wiki/tags/%E6%A2%AF%E5%BA%A6%E4%BC%B0%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>Welcome to bg51717&#39;s Wiki and Blog</title>
    <link href="https://bg51717.github.io/wiki/14261/"/>
    <id>https://bg51717.github.io/wiki/14261/</id>
    <published>2024-11-02T02:55:49.874Z</published>
    <updated>2024-11-02T02:55:49.874Z</updated>
    
    <content type="html"><![CDATA[<p>这是bg51717的个人Wiki和Blog站点，主要是把知识系统的罗列出来以及存放一些特殊bug的处理，当然也会更一些游戏或者二次元相关东西，也希望在成长的过程中可以认识许多志同道合的人。</p><p>本人一直认为互联网的开源是社会发展的重要原因之一，因为开源使得技术知识和解决问题的经验可以被记录和传承下去，很多时候在需要的时候可以被人们所发掘。</p><p>也希望可以通过博客让自己的<strong>思维有条理</strong>。很多时候我喜欢观察别人的思路，发现其实人与人的很多思路差距可能没有那么多。除开经验上的差别，很多人能成功的做成一件事很多原因是思维非常有条理，时时刻刻明白自己的应该做什么，下一步思路是什么。不会让自己的思维局限在某个步骤或者门槛上。从而即使在逆境中，也能实现把烂牌打出最好的效果。</p><p>在偶尔反思自己的不足的时候，深刻的发现<strong>拖延症是致命的</strong>，很多事情只要按照条理有计划的进行，结果其实都可以完成。但是拖延容易导致事情出现计划外的变故，进而导致完成的质量。这对于个人的成长来说是极为不利的。很早以前就看到了<strong>知行合一</strong>这个词，但是一直没有理解其重要性。后来发现，缺少行动力也会导致很多计划的失败。很多事物是需要我们用心去做，而不是用敷衍的态度去进行。在实践中不断地提升自己，革新自己。</p><p>不知道此刻在阅读这个博客的你是在何时何地打开的，也不知道你是为了探究什么而点开了这个链接。但是祝你身体健康，万事如意。大家一起学习，互相交流，共同成长！</p><p>最后，附上我喜欢的一句话：</p><blockquote><p>世界靠现实主义者维系，靠理想主义者发展。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这是bg51717的个人Wiki和Blog站点，主要是把知识系统的罗列出来以及存放一些特殊bug的处理，当然也会更一些游戏或者二次元相关东西，也希望在成长的过程中可以认识许多志同道合的人。&lt;/p&gt;
&lt;p&gt;本人一直认为互联网的开源是社会发展的重要原因之一，因为开源使得技术知识</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</title>
    <link href="https://bg51717.github.io/wiki/14592/"/>
    <id>https://bg51717.github.io/wiki/14592/</id>
    <published>2024-10-27T05:35:56.000Z</published>
    <updated>2024-11-11T14:55:02.024Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p>本篇博客介绍论文<a href="https://arxiv.org/abs/2306.00978">[2306.00978] AWQ:Activation-aware Weight Quantization for LLM Compression andAcceleration</a>提出的一种针对权重的训练后量化方法。该方法基于一个发现：<strong>模型的权重不是同等重要的，只保护1%的突出权重可以大大减少量化误差</strong>。但是混合精度会导致算法对硬件不友好，具体保护的方法是对模型的权重进行扩大，输入也进行对应的缩小。具体的比例根据激活通过网格搜索获得。由于没有进行反向传播，因此量化模型在与校准数据集不同分布的数据上也能表现良好。并最终实现了推理加速效果。</p><p>下图是PTQ大致流程图，和GPTQ算法一样，AWQ算法针对的主要是量化步骤中的参数调整部分：</p><p><img src="/wiki/wiki/14592/1730525025940.png"></p><h2 id="发现">发现</h2><p>首先介绍作者根据经验提出的假设：<strong>模型的权重不是同等重要的，只保护1%的突出权重可以大大减少量化误差</strong>。下图是示例：</p><p><img src="/wiki/wiki/14592/1730009795566.png"></p><p>从图里可以看出，在某一层layer的输入中，不同token的异常维度应该是集中在某几个维度中的。在LLM.int8中也有类似的结论，并且因此设计了llm.int8算法：</p><p><img src="/wiki/wiki/14592/1730258455859.png"></p><p>为了验证这个假设，作者进行了实验，尝试对少部分显著的权重保持原来精度，别的权重量化为int3，然后进行了多组测试。RTN是直接量化，不做调整，作为对比实验。作者分别尝试了使用权重的大小，激活的大小以及随机选择来确定哪些权重的显著的，结果发现根据激活选择的效果最好。</p><p><img src="/wiki/wiki/14592/1730010346827.png"></p><h2 id="方法">方法</h2><p>根据发现，只需要根据激活选择少部分权重保持为高精度即可实现优秀的量化算法，但是混合精度是硬件不友好的操作。因此，作者选择别的方法来保护这些权重：<strong>通过激活感知缩放来保护显着权重</strong>。</p><p>考虑权重 <span class="math inline">\(\mathbf{w}\)</span>，线性层操作可以记为<span class="math inline">\(y = \mathbf{w} \mathbf{x}\)</span>,对应的量化操作（包含反量化）为<span class="math inline">\(y =Q(\mathbf{w}) \mathbf{x}\)</span>，<span class="math inline">\(N\)</span>是量化的比特数，<span class="math inline">\(\Delta\)</span>是缩放因子，量化函数为:</p><p><span class="math display">\[Q(\mathbf{w}) = \Delta \cdot\text{Round}\left(\frac{\mathbf{w}}{\Delta}\right), \quad \Delta =\frac{\max(|\mathbf{w}|)}{2^{N-1}}\]</span></p><p>考虑针对权重中的一个向量 $w $ 乘以 $s &gt; 1 $ ，然后按同等比例缩小$x $, 我们有 $Q(w s)(x / s) $， 具体为:</p><p><span class="math display">\[Q(w \cdot s) \cdot \frac{x}{s} = \Delta' \cdot\text{Round}\left(\frac{w s}{\Delta}\right) \cdot x \cdot \frac{1}{s}\]</span></p><p><span class="math inline">\(\Delta'\)</span>是新的缩放因子。</p><p>这里解释一下为什么放缩针对是一行（这里假设乘号左边是X，右边是W）：</p><p><img src="/wiki/wiki/14592/1731230615706.png"></p><p>然后量化的时候，以列或者行作为量化函数的基本单位：</p><p><img src="/wiki/wiki/14592/1730272140732.png"></p><p>作者根据经验判断：</p><ul><li>量化为整数（Round函数）的时候，误差在0~0.5之间均匀分布，因此，平均误差为0.25。因此在对权重放大后，相对误差会减小，比如之前值是1，相对误差为0.25，1放大2倍后，相对误差只有0.125。</li><li>对部分<span class="math inline">\(w\)</span>进行放缩对量化时分组里的极值改变较少，因此<span class="math inline">\(\Delta' \approx \Delta\)</span></li><li>第二点的误差可以表示为<span class="math inline">\(\text{Err}' =\Delta' \cdot \text{RoundErr} \cdot\frac{1}{s}\)</span>，与原始误差的比例为<span class="math inline">\(\frac{\Delta'}{\Delta} \cdot\frac{1}{s}\)</span>。如果<span class="math inline">\(\Delta'\approx \Delta,s&gt;1\)</span>，那么就会降低误差</li></ul><p>然后作者选择了一部分参数进行了验证：</p><p><img src="/wiki/wiki/14592/1730011375988.png"></p><p>根据结果发现，在s=2的时候PPL达到了最小。在s不超过2的时候，基本符合根据经验得到的结论。</p><p>对于AWQ算法中的<span class="math inline">\(s\)</span>，作者认为这是一个最优化问题：</p><p><span class="math display">\[\mathbf{s}^* = \arg \min_{\mathbf{s}} \, \mathcal{L}(\mathbf{s}), \quad\mathcal{L}(\mathbf{s}) = \left\| Q(\mathbf{W} \cdot \mathbf{s})\left(\mathbf{s}^{-1} \cdot \mathbf{X}\right) - \mathbf{W} \mathbf{X}\right\|\]</span></p><p>由于量化函数不可微分，别的近似方法存在收敛不稳定的情况，作者最终根据激活使用网格搜索获得：</p><p><span class="math display">\[\mathbf{s} = \mathbf{s}_{\mathbf{X}}^{\alpha}, \quad \alpha^* = \arg\min_{\alpha} \mathcal{L}(\mathbf{s}_{\mathbf{X}}^{\alpha})\]</span></p><p><span class="math inline">\(\mathbf{s}_{\mathbf{X}}\)</span>和输入<span class="math inline">\(\mathbf{X}\)</span>有关，<span class="math inline">\(\alpha\)</span>搜索的范围在[0,1]之间。</p><p>作者通过实验验证了搜索的有效性：</p><p>此外，作者还进行了以下优化：</p><ul><li>对扩大化的权重进行裁剪</li><li>把对输入的缩小和前一个算子融合（比如，以前一个算子是矩阵乘法作为例子：前一个矩阵权重权重缩小<span class="math inline">\(s\)</span>，对前一个算子来说，扩大比例和缩小的比例大小不一样，并且可能方向也不一样，比如一个是列方向，另一个是行方向，因此不会完全抵消</li></ul><h2 id="实验">实验</h2><p>作者首先在LlaMa家族模型中进行了实验，比较在WikiText-2上的困惑度，发现AWQ的效果比GPTQ要好一点。</p><p><img src="/wiki/wiki/14592/1730012063950.png"></p><p>然后作者选取了80个样本问题，把量化前后模型的回答连接起来，让GPT-4打分判断哪个更好（交换次序后再重复一次）。结果也比之前的方法好。</p><p><img src="/wiki/wiki/14592/1730012673547.png"></p><p>作者还测试了在多模态大模型上表现：</p><p><img src="/wiki/wiki/14592/1730012744431.png"></p><p>还测试了在2比特量化下的表现，作者还提出AWQ和GPTQ是可以一起使用的：</p><p><img src="/wiki/wiki/14592/1730013136175.png"></p><p>然后作者罗列了一下推理加速的效果，可以看到每秒生成的token数目增长了很多：</p><p><img src="/wiki/wiki/14592/1730013299656.png"></p><p>作者认为，由于AWQ在使用校准数据进行量化的时候没有进行反向传播，因此可以过拟合校准集合：</p><p><img src="/wiki/wiki/14592/1730013388948.png"></p><h2 id="代码">代码</h2><p>论文源码：<a href="https://github.com/mit-han-lab/llm-awq">mit-han-lab/llm-awq:[MLSys 2024 Best Paper Award] AWQ: Activation-aware Weight Quantizationfor LLM Compression and Acceleration</a></p><p>算法工具包：<a href="https://github.com/AutoGPTQ/AutoGPTQ">AutoGPTQ/AutoGPTQ: Aneasy-to-use LLMs quantization package with user-friendly apis, based onGPTQ algorithm.</a></p><h2 id="参考资料">参考资料</h2><blockquote><ul><li><a href="https://arxiv.org/abs/2306.00978">[2306.00978] AWQ:Activation-aware Weight Quantization for LLM Compression andAcceleration</a></li><li><a href="https://github.com/mit-han-lab/llm-awq">mit-han-lab/llm-awq:[MLSys 2024 Best Paper Award] AWQ: Activation-aware Weight Quantizationfor LLM Compression and Acceleration</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;本篇博客介绍论文&lt;a href=&quot;https://arxiv.org/abs/2306.00978&quot;&gt;[2306.00978] AWQ:
Activation-aware Weight Quantization for LLM Com</summary>
      
    
    
    
    <category term="科研" scheme="https://bg51717.github.io/wiki/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文阅读" scheme="https://bg51717.github.io/wiki/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="量化" scheme="https://bg51717.github.io/wiki/tags/%E9%87%8F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>拉格朗日乘数法解条件极值</title>
    <link href="https://bg51717.github.io/wiki/49444/"/>
    <id>https://bg51717.github.io/wiki/49444/</id>
    <published>2024-10-24T09:10:04.000Z</published>
    <updated>2024-11-02T02:55:49.880Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p>求解最优化是一类十分常见且难以求解的问题，因此，考虑开一个博客系统性的介绍一下重要解法：拉格朗日乘数法（LagrangeMultiplier Method）。之后再扩展到广义的拉格朗日乘数法。</p><p>拉格朗日乘数法的重点是<strong>在一些列约束条件下，构造包含隐藏条件的拉格朗日函数等同于优化的目标函数。</strong></p><blockquote><p>隐藏条件指的是在限制条件和拉格朗日函数共同作用下实际暗含的一些条件或方程。</p></blockquote><p>最优化问题通常是指对于给定的某一函数，<strong>求其在指定作用域上的全局最小值(因为最小值与最大值可以很容易转化，即最大值问题可以转化成最小值问题)</strong>。</p><h2 id="无约束条件">无约束条件</h2><p>对所有变量求偏导计算极值点后，把极值点代回原函数验证即可。</p><h2 id="等式约束条件">等式约束条件</h2><p>求一个函数在等式约束下的最值，即</p><p><span class="math display">\[\begin{aligned}    \min &amp; \quad f(\mathbf{x}) \\    \text{s.t.} &amp; \quad h_j(\mathbf{x}) = 0, \quad j = 1, 2, \dots,l\end{aligned}\]</span></p><p>定义拉格朗日函数<span class="math inline">\(F(x)\)</span>，其中<span class="math inline">\(\lambda_k\)</span>为引入的变量：</p><p><span class="math display">\[F(\mathbf{x}, \lambda) = f(\mathbf{x}) + \sum_{j=1}^{l} \lambda_jh_j(\mathbf{x})\]</span></p><p>求解极值点只需求解方程组：</p><p><span class="math display">\[\frac{\partial F}{\partial x_i} = 0 \\\frac{\partial F}{\partial \lambda_j} = 0\]</span></p><p>下面关于不等式约束下的最优值求解给了通用的证明过程，不过，维基百科列出了一个形象的解释：</p><p>考虑有个优化问题：</p><p><span class="math display">\[\begin{aligned}    \min &amp; \quad f(x, y) \\    \text{s.t.} &amp; \quad g(x, y) = c\end{aligned}\]</span></p><p><img src="/wiki/wiki/49444/1729772448154.png"></p><p>图上画的是两个函数的等高线，箭头表示梯度方向（暂时不考虑梯度正负）。绿色的线是约束条件，代表需要在绿色的线上寻找最优点。最优点显然在两个函数相切的地方，即两个函数梯度共线，<span class="math inline">\(\nabla f(x, y) = \lambda \left( \nabla g(x, y) - C\right)\)</span>（<span class="math inline">\(\nabla\)</span>代表梯度算子，<span class="math inline">\(\lambda\)</span>是非0实数），这就是等式约束下拉格朗日乘数法的<strong>隐藏条件</strong>。</p><p>从反证法考虑，假如两个梯度不共线，那么沿着约束函数的切向，即与约束函数梯度垂直的方向，一定可以找到一个向量，与约束函数梯度垂直且在目标函数梯度上有分量。那么沿着该向量方向修改变量，可以在约束函数值不变的条件下继续优化目标函数。</p><h2 id="不等式条件约束">不等式条件约束</h2><p>添加上不等式约束条件，就得到了广义拉格朗日问题，此时的最优化问题为，</p><p><span class="math display">\[\begin{aligned}    \min &amp; \quad f(\mathbf{\mathbf{x}}) \\    \text{s.t.} &amp; \quad h_j(\mathbf{\mathbf{x}}) = 0, \quad j = 1,2, \dots, p \\                 &amp; \quad g_k(\mathbf{\mathbf{x}}) \leq 0, \quad k =1, 2, \dots, q\end{aligned}\]</span></p><p>对应的拉格朗日函数L为，</p><p><span class="math display">\[L(\mathbf{x}, \lambda, \mu) = f(\mathbf{x}) + \sum_{j=1}^{p} \lambda_jh_j(\mathbf{x}) + \sum_{k=1}^{q} \mu_k g_k(\mathbf{x})\]</span></p><p>常用的方法是KKT条件，即最优值必须满足：</p><p><span class="math display">\[\begin{aligned}    \frac{\partial F}{\partial x_i} = 0 \quad &amp;(1)\\    \frac{\partial F}{\partial \lambda_j} = 0 \quad &amp;(2)\\    \lambda_j \neq 0 \quad &amp;(3)\\    \mu_k g_k(\mathbf{x}) = 0 \quad &amp;(4)\\    g_k(\mathbf{x}) \leq 0 \quad &amp;(5)\\    \mu_k \geq 0 \quad  &amp;(6)\end{aligned}\]</span></p><p>其中前三个式子是等式条件约束里的，后三个式子是不等式条件引入的。</p><h2 id="kkt推导">KKT推导</h2><blockquote><p><span class="math inline">\(\max_\mu L(x,\mu)\)</span>代表调整<span class="math inline">\(\mu\)</span>最大化目标函数<span class="math inline">\(L(x,\mu)\)</span>。</p></blockquote><p>首先，令（这里的几个变量都是多元变量，即向量，为了书写方便，没有引入<code>\mathbf</code>）</p><p><span class="math display">\[\ L(x, \lambda, \mu) = f(x) + \sum_{k=1}^{q} \mu_k g_k(x)\]</span></p><p>引入约束条件，</p><p><span class="math display">\[\because\begin{cases}    \mu_k \geq 0 \\    g_k(x) \leq 0\end{cases} \\\\\therefore \mu_k g_k(x) \leq 0 \\\therefore 根据非负得\ \max_\mu L(x,\mu)=f(x) \\\therefore \min_ x f(x) = \min_x \max_ \mu L(x,\mu)\]</span></p><p>所以我们发现调整<span class="math inline">\(\mu\)</span>最大化<span class="math inline">\(L\)</span>就等于<span class="math inline">\(f(x)\)</span>，所以</p><p><span class="math display">\[\min_ x f(x) = \min_x \max_ \mu L(x,\mu)\]</span></p><p>另一方面，</p><p><span class="math display">\[\max_\mu \min_x L(x, \mu) = \max_\mu \left[ \min_x f(x) + \min_x \mug(x) \right]= \max_\mu \min_x f(x) + \max_\mu \min_x \mu g(x)= \min_x f(x) + \max_\mu \min_x \mu g(x) \\又\because \begin{aligned}    \mu_k \geq 0, \quad g_k(x) \leq 0 \quad &amp;\Rightarrow \quad\min_x \mu g(x) = \begin{cases}        0, &amp; \text{if } \mu = 0 \text{ or } g(x) = 0 \\        -\infty, &amp; \text{if } \mu &gt; 0 \text{ and } g(x) &lt; 0    \end{cases}\end{aligned}\]</span></p><p>所以，引入约束条件<span class="math inline">\(\mug(X)=0\)</span>，我们得到，</p><p><span class="math display">\[\max_\mu \min_x \mu g(x) = 0 \\\therefore \max_\mu \min_x L(x, \mu) = \min_x f(x) + \max_\mu \min_x \mug(x) = \min_x f(x)\]</span></p><p>综上所述，</p><p><span class="math display">\[\begin{aligned}\begin{rcases}L(x, \lambda, \mu) &amp;= f(x) + \sum_{k=1}^{q} \mu_k g_k(x) \\\mu_k &amp;\geq 0 \\g_k(x) &amp;\leq 0 \\\mu g(X)&amp;=0\end{rcases} \end{aligned}\Rightarrow\max_\mu \min_x L(x, \mu)= \min_x \max_ \mu L(x,\mu)=\min_ x f(x)\]</span></p><p>引入等式约束的条件，我们得到了，</p><p><span class="math display">\[\begin{aligned}\begin{rcases}    \frac{\partial F}{\partial x_i} = 0\\    \frac{\partial F}{\partial \lambda_j} = 0\\    \lambda_j \neq 0\\    \mu_k g_k(\mathbf{x}) = 0\\    g_k(\mathbf{x}) \leq 0\\    \mu_k \geq 0\end{rcases} \end{aligned}\Rightarrow\max_\mu \min_x L(x, \mu)= \min_x \max_ \mu L(x,\mu)=\min_ x f(x)\]</span></p><p>所以我们完成了我们需要证明的：<strong>在一些列约束条件下，拉格朗日函数等同于优化的目标函数。</strong></p><p>补充KTT下的隐藏条件：</p><p><span class="math display">\[\frac{\partial L(x, \lambda, \mu)}{\partial x} \Bigg|_{x = x^*} = 0\quad \text{表明} \quad f(x) \text{在极值点} x^* \text{处的梯度是含有}h_j(x^*) \text{和} g_k(x^*) \text{梯度的线性组合。}\]</span></p><h2 id="参考资料">参考资料</h2><blockquote><ul><li><a href="https://zh.wikipedia.org/zh-cn/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0">拉格朗日乘数- 维基百科，自由的百科全书</a></li><li><a href="https://www.cnblogs.com/mo-wang/p/4775548.html">【整理】深入理解拉格朗日乘子法（LagrangeMultiplier) 和KKT条件 - mo_wang - 博客园</a></li><li><a href="https://blog.csdn.net/johnnyconstantine/article/details/46335763">KKT条件介绍-CSDN博客</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;求解最优化是一类十分常见且难以求解的问题，因此，考虑开一个博客系统性的介绍一下重要解法：拉格朗日乘数法（Lagrange
Multiplier Method）。之后再扩展到广义的拉格朗日乘数法。&lt;/p&gt;
&lt;p&gt;拉格朗日乘数法的重点是&lt;</summary>
      
    
    
    
    <category term="数学" scheme="https://bg51717.github.io/wiki/categories/%E6%95%B0%E5%AD%A6/"/>
    
    <category term="微积分" scheme="https://bg51717.github.io/wiki/categories/%E6%95%B0%E5%AD%A6/%E5%BE%AE%E7%A7%AF%E5%88%86/"/>
    
    
    <category term="数学" scheme="https://bg51717.github.io/wiki/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>大模型量化~GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</title>
    <link href="https://bg51717.github.io/wiki/31769/"/>
    <id>https://bg51717.github.io/wiki/31769/</id>
    <published>2024-10-24T08:12:19.000Z</published>
    <updated>2024-11-02T02:57:00.290Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p>GPTQ算法的原理从数学公式出发，推导出权重的量化顺序和其余参数的调整值，然后根据这些值对block里的所有参数以列为单位进行量化，每次量化可以量化多个列，同时调整其余未量化的列的参数减小量化误差。</p><p>GPTQ算法是只针对权重的量化方式。在计算的时候，对应的内核会把需要计算的权重还原回原来的数据类型（比如int4-&gt;fp16）从而保证计算的稳定。</p><p>下图是PTQ大致流程图，GPTQ算法针对的主要是量化步骤中的参数调整部分：、</p><p><img src="/wiki/wiki/31769/1730268555276.png"></p><p>GPTQ量化算法最早来自1990年YannLeCun的OBD算法，然后按照OBS，OBC（OBQ）的顺序进行演化，最终到GPTQ算法。当然，这里的演化算法不都是量化算法，由于量化和剪枝是很像的两个技术，量化调整参数为距离最近的整数，剪枝调整参数为0，所以量化算法和剪枝算法有很多共同点。</p><h2 id="obdoptimal-brain-damage">OBD：Optimal Brain Damage</h2><p>OBD是一种剪枝方法，考虑目标函数为<span class="math inline">\(E\)</span>，模型参数为<span class="math inline">\(w_i\)</span>，对目标函数进行泰勒展开，有</p><p><span class="math display">\[\Delta E = \sum_i g_i \Delta w_i + \frac{1}{2} \sum_i h_{ii} \Deltaw_i^2 + \frac{1}{2} \sum_{i \neq j} h_{ij} \Delta w_i \Delta w_j +O(\Delta w^3)\]</span></p><p>其中<span class="math inline">\(g_i = \frac{\partial E}{\partialw_i}\)</span>是一阶偏导，<span class="math inline">\(h_{ij} =\frac{\partial^2 E}{\partial w_i \partialw_j}\)</span>是海森矩阵元素。</p><p>OBD假设：</p><ul><li>目标函数为二阶，不考虑高阶项<span class="math inline">\(O(\Deltaw^3)\)</span></li><li>模型训练充分收敛，一阶偏导为0，<span class="math inline">\(g_i=0,\forall i\)</span></li><li>每个参数对目标函数影响是独立的，海森矩阵中的交叉项为0，即<span class="math inline">\(h_{ij} = 0, \forall i, j, i \neq j\)</span></li></ul><p>于是，上式可以优化为，</p><p><span class="math display">\[\Delta E = \frac{1}{2} \sum_i h_{ii} \Delta w_i^2\]</span></p><p>因此，OBD算法只需要根据每个参数对目标的影响从小到大进行排序，然后进行剪枝即可。</p><h2 id="obsoptimal-brain-surgeon">OBS：Optimal Brain Surgeon</h2><p><img src="/wiki/wiki/31769/1729833758876.png"></p><p>OBS算法认为参数之间的独立性不成立，因此交叉项需要考虑，因此上式变为，</p><p><span class="math display">\[\Delta E = \frac{1}{2} \sum_i h_{ii} \Delta w_i^2 + \frac{1}{2} \sum_{i\neq j} h_{ij} \Delta w_i \Delta w_j\]</span></p><p>向量，矩阵形式为，</p><p><span class="math display">\[\Delta E = \frac{1}{2} \Delta \mathbf{w}^\mathrm{T} \mathbf{H} \Delta\mathbf{w}\]</span></p><p>剪枝删除一个权重，那么<span class="math inline">\(\Delta\mathbf{w}\)</span>的第 q 维固定为 <span class="math inline">\(-w_q\)</span>，但其他维度的值不固定，可以用于减少删除该权重带来的目标偏离。即约束条件为，</p><p><span class="math display">\[\mathbf{e}_q^\mathrm{T} \cdot \Delta \mathbf{w} + w_q = 0\]</span></p><p>其中，<span class="math inline">\(\mathbf{e}_q\)</span>是one-hot向量，第q个位置是1，其余位置为0。</p><p>所以剪枝转化为最优化问题，</p><p><span class="math display">\[\min_{\Delta \mathbf{w}, q} \frac{1}{2} \Delta \mathbf{w}^\mathrm{T}\mathbf{H} \Delta \mathbf{w} \quad \text{s.t.} \quad\mathbf{e}_q^\mathrm{T} \cdot \Delta \mathbf{w} + w_q = 0\]</span></p><p>构造拉格朗日函数，</p><p><span class="math display">\[L = \frac{1}{2} \Delta \mathbf{w}^\mathrm{T} \mathbf{H} \Delta\mathbf{w} + \lambda (\mathbf{e}_q^\mathrm{T} \cdot \Delta \mathbf{w} +w_q)\]</span></p><p>求解，得</p><p><span class="math display">\[\Delta \mathbf{w} = -\frac{w_q}{[\mathbf{H}^{-1}]_{qq}} \mathbf{H}^{-1}\cdot \mathbf{e}_q \quad \text{and} \quad L = \frac{1}{2}\frac{w_q^2}{[\mathbf{H}^{-1}]_{qq}}\]</span></p><p>因此，我们可以根据海森矩阵求得下一个剪枝的参数和其他参数调整的值。</p><blockquote><p>每次更新参数都需要重新求解海森矩阵，原论文中提到了数学上的优化方法，但是较为复杂且在后续算法没有使用，此处不予列出。</p></blockquote><h2 id="obc">OBC</h2><p>由于需要求解全参数的海森矩阵甚至逆矩阵，复杂度极高，在参数规模上去后求解是不现实的。</p><p>因此假设：</p><ul><li>在一个矩阵中只有同一行的参数是相关的</li></ul><p>同时，为了更好的求解，对于一个维度为<span class="math inline">\((d_{row},d_{col})\)</span>参数矩阵设置一个优化的目标函数，<span class="math inline">\(\mathbf{W}_{i,\cdot},\hat{\mathbf{W_{i,\cdot}}}\)</span>分别是参数调整前后的权重向量</p><p><span class="math display">\[E = \sum_{i=1}^{d_{\text{row}}} \| \mathbf{W}_{i,\cdot} \mathbf{X} -\hat{\mathbf{W}}_{i,\cdot} \mathbf{X} \|_2^2\]</span></p><p>求每行的海森矩阵，</p><p><span class="math display">\[\frac{\partial E}{\partial \hat{\mathbf{W}}_{i,\cdot}^2} = \mathbf{H}_i= 2 \mathbf{X} \mathbf{X}^\mathrm{T}, \quad \forall i = 1, 2, \dots,d_{\text{row}}\]</span></p><p>之后就可以求得矩阵每一行的剪枝顺序。</p><p>算法伪代码：</p><p><img src="/wiki/wiki/31769/1729836598356.png"></p><p>首先遍历每一行，然后重复k次，每次找到对目标函数影响最小的参数p，对p进行剪枝同时更新其他参数，删除海森矩阵的p行p列（不包括<span class="math inline">\(H_{pp}\)</span>），再求逆。（这里罗列的好像是降低复杂度后的数学等效。）</p><blockquote><p>OBC论文也罗列了一些别的性能上的优化，在后续算法没有使用，此处不予列出。</p><p>此处数学等效的方法是高斯消元，海森矩阵移除k行k列但包括<span class="math inline">\(H_{kk}\)</span>，因为左乘矩阵代表对行向量线性组合对，右乘矩阵代表对列向量线性组合，因此通过高斯消元即可实现对0的操作。证明可以从GPTQ的Cholesky证明中基本无条件的迁移。</p></blockquote><h2 id="obq">OBQ</h2><p>OBQ（和OBC是同一篇文章）指出，剪枝是一种特殊的量化（即剪枝的参数等价于量化到0 点），因此只需要修改一下 OBC 的约束条件即可：</p><p><span class="math display">\[\mathbf{e}_p^\mathrm{T} \cdot \Delta \mathbf{w} + w_p =\text{quant}(w_p)\]</span></p><p>相应的，权重的调整更新为，</p><p><span class="math display">\[\Delta \mathbf{w} = -\frac{w_p -\text{quant}(w_p)}{[\mathbf{H}^{-1}]_{pp}} \mathbf{H}^{-1} \cdot\mathbf{e}_p\quad \text{and} \quadL = \frac{1}{2} \frac{(w_p -\text{quant}(w_p))^2}{[\mathbf{H}^{-1}]_{pp}}\]</span></p><p>OBQ的算法伪代码，同OBC的很相似：</p><p><img src="/wiki/wiki/31769/1729837217525.png"></p><h2 id="gptq">GPTQ</h2><p>OBQ的算法复杂度还是太高了，GPTQ继续进行优化。</p><p>GPTQ是逐层量化的，同时做出以下假设：</p><ul><li>参数调整前量化网络（量化函数）是确定的，比如量化的缩放因子和零点等</li><li>权重参数可以在量化网格内进行调整</li></ul><h3 id="按索引顺序量化">按索引顺序量化</h3><p>将每一行的量化权重选择方式从贪心策略改成按索引顺序选择。</p><p>修改索引顺序的好处有两个，首先多个行的量化顺序是一样的，因此可以多行并行处理；第二个是<span class="math inline">\(\mathbf{H} = 2 \mathbf{X}\mathbf{X}^\mathrm{T}\)</span>说明，海森矩阵只与输入相关，因此不同行可以共用一个海森矩阵。<img src="/wiki/wiki/31769/1729841730501.png"></p><p>行里第p个权重调整公式可以为改为，</p><p><span class="math display">\[\Delta \mathbf{w} = -\frac{w_p -\text{quant}(w_p)}{[\mathbf{H}_{p:,p:}^{-1}]_{0,0}} \left([\mathbf{H}_{p:,p:}^{-1}]_{:,0} \right)^\top\]</span></p><p>加入多行并行后变为，</p><p><span class="math display">\[\Delta W_{:,p:} = -\frac{W_{:,p:} -\text{quant}(W_{:,p:})}{[\mathbf{H}_{p:,p:}^{-1}]_{0,0}} \left([\mathbf{H}_{p:,p:}^{-1}]_{:,0} \right)^\top=-\frac{\left( [\mathbf{H}_{p:,p:}^{-1}]_{:,0}\right)^\top}{[\mathbf{H}_{p:,p:}^{-1}]_{0,0}} ( W_{:,p:} -\text{quant}(W_{:,p:}))\]</span></p><p>逆矩阵的更新公式变为，</p><p><span class="math display">\[[H_{p:,p:}]^{-1} = \left( [H_{p-1:,p-1:}]^{-1}  -\frac{1}{[H_{p-1:,p-1:}]^{-1}_{0,0}} [H_{p-1:,p-1:}]^{-1}_{:,0}[H_{p-1:,p-1:}]^{-1}_{0,:}\right)_{1:,1:}\]</span></p><h3 id="cholesky-分解">Cholesky 分解</h3><blockquote><p>关于Cholesky分解的数学介绍可以看：<a href="https://blog.csdn.net/xbinworld/article/details/104663481">三十分钟理解：矩阵Cholesky分解，及其在求解线性方程组、矩阵逆的应用_cholesky分解法求解线性方程组-CSDN博客</a></p></blockquote><p>实验过程中发现：在大规模参数矩阵上重复使用海森矩阵逆矩阵的更新公式会产生非正定的海森矩阵逆矩阵，原因可能是数值误差的积累。</p><p>作者对初始的<span class="math inline">\(H^{-1}\)</span>进行Cholesky分解，得到一个上三角矩阵<span class="math inline">\(T\)</span>，它的每一行刚好就等于使用更新公式得到的逆矩阵序列的第一行乘以一个常数，即，</p><p><span class="math display">\[C_p T_{p,p:} = \left[ H_{p:,p:} \right]^{-1}_{0,:}\]</span></p><p>图示，</p><p><img src="/wiki/wiki/31769/1729847766438.png"></p><p>所以，权重的调整更新为，</p><p><span class="math display">\[\Delta W_{:,p:} = - \frac{W_{:,p} - \text{quant}(W_{:,p})}{C_p T_{pp}}C_p T_{p,p:} = - \frac{W_{:,p} - \text{quant}(W_{:,p})}{T_{pp}} T_{p,p:}\]</span></p><p>发现很多资料都没有系统的罗列大致的推导过程，甚至论文基本也是一笔带过，因此考虑在此给出一些浅薄的证明（笔者不是数学系的。</p><p>首先介绍一下教程中的Cholesky分解：给定一个<span class="math inline">\(n\times n\)</span>的实数对称正定矩阵<span class="math inline">\(A\)</span>，存在一个对角元全为正数的下三角矩阵，是的<span class="math inline">\(A=LL^\top\)</span>成立。</p><p><span class="math display">\[\begin{align*}\mathbf{A} &amp;= \begin{bmatrix} a_{11} &amp; \mathbf{A}_{21}^T \\\mathbf{A}_{21} &amp; \mathbf{A}_{22} \end{bmatrix}, \quad\mathbf{L} = \begin{bmatrix} l_{11} &amp; 0 \\ L_{21} &amp; L_{22}\end{bmatrix}, \quad\mathbf{L}^T = \begin{bmatrix} l_{11} &amp; L_{21}^T \\ 0 &amp; L_{22}^T\end{bmatrix}\end{align*}\]</span></p><p>其中 $a_{11} $ 和 $ l_{11} $是一个标量， <span class="math inline">\(\mathbf{A}_{21}\)</span> 和 <span class="math inline">\(L_{21}\)</span> 是一个列向量，$<em>{22} $是一个 (n-1 ) 阶的方阵，而 $ L</em>{22} $ 是一个 ( n-1 )阶的下三角矩阵。我们有，</p><p><span class="math display">\[\begin{align*}\begin{bmatrix} a_{11} &amp; \mathbf{A}_{21}^T \\ \mathbf{A}_{21} &amp;\mathbf{A}_{22} \end{bmatrix}&amp;= \begin{bmatrix} l_{11} &amp; 0 \\ L_{21} &amp; L_{22}\end{bmatrix}\begin{bmatrix} l_{11} &amp; L_{21}^T \\ 0 &amp; L_{22}^T \end{bmatrix}&amp;= \begin{bmatrix} l_{11}^2 &amp; l_{11} L_{21}^T \\l_{11}L_{21}  &amp; L_{21} L_{21}^T + L_{22} L_{22}^T \end{bmatrix}\end{align*}\]</span></p><p>我们易得，</p><p><span class="math display">\[\begin{align*}l_{11} &amp;= \sqrt{a_{11}} \\[10pt]L_{21} &amp;= \frac{1}{l_{11}} \mathbf{A}_{21} \\[10pt]L_{22} L_{22}^T &amp;= \mathbf{A}_{22} - L_{21} L_{21}^T\end{align*}\]</span></p><p>其中<span class="math inline">\(l_{11}\)</span>和<span class="math inline">\(L_{21}\)</span>我们直接得到，对于<span class="math inline">\(L_{22}\)</span>，我们发现又是一个Cholesky分解的过程，递归求解即可。</p><p>我们根据公式可以发现，</p><p><span class="math display">\[(\mathbf{L}^\top)_{1,:}=\begin{bmatrix} l_{11} &amp; L^{\top}_{21} \end{bmatrix}= \frac{1}{\sqrt{a}} \begin{bmatrix} a_{11} &amp; \mathbf{A}^{\top}_{21}\end{bmatrix}= \frac{1}{\sqrt{a}}\mathbf{A}_{1,:}\]</span></p><p>所以我们证明了两个矩阵第一列的比例关系，根据分解的递推性和矩阵的对称性，我们就完整证明了这个结论。</p><h2 id="海森逆矩阵更新公式证明">海森逆矩阵更新公式证明</h2><p>首先我们需要证明<strong>在海森矩阵删除p行p列参数（不包括<span class="math inline">\(H_{pp}\)</span>）的时候，在海森矩阵上会发生类似的操作。</strong></p><p>首先，我们需要知道<span class="math inline">\((H^{-1})^\top=(H^T)^{-1}=H^{-1}\)</span>，即海森矩阵的逆矩阵是对称矩阵。</p><p><span class="math display">\[\begin{align*}\mathbf{H}= \begin{bmatrix} a &amp; \mathbf{z}^\top \\ \mathbf{z} &amp;\mathbf{A} \end{bmatrix}, \quad构造矩阵\mathbf{B}=\begin{bmatrix} \frac{1}{a} &amp; \mathbf{z}^\top \\\mathbf{z} &amp; \mathbf{A}^{-1} \end{bmatrix}\end{align*}\]</span></p><p>其中，<span class="math inline">\(\mathbf{H}\)</span>和<span class="math inline">\(\mathbf{B}\)</span>是n维的对称正定方阵，a<span class="math inline">\(是标量，\)</span><span class="math inline">\(z\)</span>为n-1维的零向量，那么</p><p><span class="math display">\[\mathbf{H} \times \mathbf{B}=\begin{bmatrix} a &amp; \mathbf{z}^\top \\ \mathbf{z} &amp; \mathbf{A}\end{bmatrix} \times\begin{bmatrix} \frac{1}{a} &amp; \mathbf{z}^\top \\ \mathbf{z} &amp;\mathbf{A}^{-1} \end{bmatrix}=\begin{bmatrix} 1 &amp; a\mathbf{z}^\top+\mathbf{z}^\top\mathbf{A}^{-1}\\ \frac{1}{a}\mathbf{z}+\mathbf{A}\mathbf{z} &amp;\mathbf{z}\mathbf{z}^\top+\mathbf{A}\mathbf{A}^{-1} \end{bmatrix}=\begin{bmatrix} 1 &amp; \mathbf{z}^\top \\ \mathbf{z} &amp;I_{n-1\times n-1} \end{bmatrix}=I_{n\times n}\]</span></p><p>说明，说明<span class="math inline">\(\mathbf{B}\)</span>就是<span class="math inline">\(\mathbf{H}\)</span>的逆矩阵，因此，得证。我们就可以直接对海森矩阵的逆矩阵进行删除参数即可。</p><p>接下来，我们需要证明删除行参数和列参数的公式。为了方便，假设我们有个n维的对称正定方阵<span class="math inline">\(\mathbf{A}\)</span>，令</p><p><span class="math display">\[\mathbf{B}=\mathbf{A}-\frac{1}{\mathbf{A}_{11}}\mathbf{A}_{:,1}\mathbf{A}_{1,:}\]</span></p><p>那么，</p><p><span class="math display">\[\mathbf{B}_{i,1}=\mathbf{A}_{i,1}-\frac{1}{\mathbf{A}_{11}}\mathbf{A}_{i1}\mathbf{A}_{11}=0,i\neq 1\]</span></p><p>所以<span class="math inline">\(\mathbf{B}\)</span>的第一列除了第一个元素全部为0，根据对称性，第一行也全部为0，即实现了对第一行第一列元素的删除。</p><blockquote><p>尽管这里推导的时候是针对第一行或者第一列的，但是很容易外推到任意位置。</p></blockquote><p>到此，我们证明了海森逆矩阵更新公式的正确性，分为两步：首先证明删除参数后海森逆矩阵的格式，然后根据格式证明了更新公式的正确性。</p><h3 id="lazy-batch-updates">Lazy Batch-Updates</h3><p>在量化某个参数矩阵的情况下，每次量化一个参数，其他所有未量化的参数都要按公式全都要更新一遍。如果每行的量化并行计算，那么每次更新过程就需要read + write 一次参数矩阵。如果参数矩阵的维度为<span class="math inline">\(k \timesk\)</span>，那么量化这个参数矩阵就需要读写 k 次参数，总共的 IO 量为<span class="math inline">\(k^3\)</span>个元素。当 k 比较大时（&gt;=4096），需要读写的元素就非常多了，运行时间大都被 IO 占据。</p><p><img src="/wiki/wiki/31769/1729848176700.png"></p><p><span class="math display">\[\Delta W_{:,p:} = -\frac{W_{:,p:} -\text{quant}(W_{:,p:})}{[\mathbf{H}_{p:,p:}^{-1}]_{0,0}} \left([\mathbf{H}_{p:,p:}^{-1}]_{:,0} \right)^\top=-\frac{\left( [\mathbf{H}_{p:,p:}^{-1}]_{:,0}\right)^\top}{[\mathbf{H}_{p:,p:}^{-1}]_{0,0}} ( W_{:,p:} -\text{quant}(W_{:,p:}))\]</span></p><p>将参数矩阵按每若干列划分为一个个 group，量化某一列时，group内的参数立即更新，而 group 后面的列只记录更新量，延迟更新。当一个 group的参数全部量化完成，再统一对后面的所有参数做一次更新。这就是 LazyBatch-Updates。根据更新公式我们可以发现，这个更新可以是可以合并同类项的，把一个group的第一项提出来求和，可以一次读写后面的所有group。</p><p><img src="/wiki/wiki/31769/1729848182654.png"></p><p>对应的算法伪代码为</p><p><img src="/wiki/wiki/31769/1729922822263.png"></p><h2 id="实验">实验</h2><p>1.小模型测试</p><p>首先在ResNet上面进行了测试。选择AdaQuant、基于贪心策略的OBQ在精度上保持持平。相比量化前的模型来说，性能也没有下降太多。</p><p><img src="/wiki/wiki/31769/1729943142457.png"></p><p>然后在小型语言模型上（这是OBQ能使用的最大的模型之一）进行了测试。在4bit得分比之前的方法稍微差一点，在3bit要低的多一点。</p><p><img src="/wiki/wiki/31769/1729943292624.png"></p><p>但是量化需要时间从1h降到1min。</p><p>2.测试了量化时间</p><p>提供了一个参考的标准：</p><p>"For reference, the straight-through based method ZeroQuant-LKD (Yaoet al., 2022) reports a 3 hour runtime (on the same hardware) for a 1.3Bmodel, which would linearly extrapolate to several hundred hours (a fewweeks) for 175B models."</p><p><img src="/wiki/wiki/31769/1729943743510.png"></p><p>3.测试了量化模型在文本生成任务的表现</p><p>和不进行优化的直接量化（RTN）进行了对比。</p><p><img src="/wiki/wiki/31769/1729944238282.png"></p><p>4.测试了OPT-175B文本生成的效率</p><p>开发了相关kernel（推理时权重会反量化），量化只针对权重，激活不量化。</p><p><img src="/wiki/wiki/31769/1729944960522.png"></p><p>5.零样本任务评测</p><p>测试了和基线相比，在零样本任务上的准确率。</p><p><img src="/wiki/wiki/31769/1729945727934.png"></p><p>该方法很容易和别的量化网络相结合。量化网络指的是量化的操作，是权重调整后的具体量化步骤，需要确定比如缩放因子，零点，量化的粒度等。</p><h2 id="代码解析">代码解析</h2><p>论文对应的代码仓库为：<a href="https://github.com/IST-DASLab/gptq">IST-DASLab/gptq: Code for theICLR 2023 paper "GPTQ: Accurate Post-training Quantization of GenerativePretrained Transformers".</a></p><p>基于改算法开发的工具包：<a href="https://github.com/AutoGPTQ/AutoGPTQ">AutoGPTQ/AutoGPTQ: Aneasy-to-use LLMs quantization package with user-friendly apis, based onGPTQ algorithm.</a></p><h2 id="参考资料">参考资料</h2><blockquote><ul><li><a href="https://zhuanlan.zhihu.com/p/646210009">QLoRA、GPTQ：模型量化概述- 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/690834228">LLM 推理加速技术 ——GPTQ 量化技术演进 - 知乎</a></li><li><a href="https://arxiv.org/abs/2210.17323">[2210.17323] GPTQ:Accurate Post-Training Quantization for Generative Pre-trainedTransformers</a></li><li><a href="https://github.com/AutoGPTQ/AutoGPTQ">AutoGPTQ/AutoGPTQ: Aneasy-to-use LLMs quantization package with user-friendly apis, based onGPTQ algorithm.</a></li><li><a href="https://github.com/IST-DASLab/gptq">IST-DASLab/gptq: Codefor the ICLR 2023 paper "GPTQ: Accurate Post-training Quantization ofGenerative Pretrained Transformers".</a></li><li><a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=17c0a7de3c17d31f79589d245852b57d083d386e">OptimalBrain Damage</a></li><li><a href="https://blog.csdn.net/xbinworld/article/details/104663481">三十分钟理解：矩阵Cholesky分解，及其在求解线性方程组、矩阵逆的应用_cholesky分解法求解线性方程组-CSDN博客</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;GPTQ算法的原理从数学公式出发，推导出权重的量化顺序和其余参数的调整值，然后根据这些值对block里的所有参数以列为单位进行量化，每次量化可以量化多个列，同时调整其余未量化的列的参数减小量化误差。&lt;/p&gt;
&lt;p&gt;GPTQ算法是只针对</summary>
      
    
    
    
    <category term="科研" scheme="https://bg51717.github.io/wiki/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文阅读" scheme="https://bg51717.github.io/wiki/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="量化" scheme="https://bg51717.github.io/wiki/tags/%E9%87%8F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>llm.int8</title>
    <link href="https://bg51717.github.io/wiki/21264/"/>
    <id>https://bg51717.github.io/wiki/21264/</id>
    <published>2024-10-24T07:39:30.000Z</published>
    <updated>2024-11-02T02:57:00.281Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>todo：本文还在施工中.......</p></blockquote><h2 id="介绍">介绍</h2><p><code>llm.int8</code>是第一批针对大模型进行量化的算法，并且其算法也被集成在<code>bitsandbytes</code>库中，该库也已经被<code>huggingface</code>集成到代码库当中作为最基本的量化算法之一。</p><p>论文地址为：<a href="https://arxiv.org/abs/2208.07339">[2208.07339]LLM.int8(): 8-bit Matrix Multiplication for Transformers atScale</a></p><p>对应的源码为：<a href="https://github.com/bitsandbytes-foundation/bitsandbytes">bitsandbytes-foundation/bitsandbytes:Accessible large language models via k-bit quantization forPyTorch.</a></p><h2 id="摘要">摘要</h2><p>LLM.in8()量化针对的是transformer的FNN模块和Attn模块的矩阵乘法。</p><p>该量化方法把矩阵中的异常值抽出来保持16位精度，对于其余值量化为8位精度，计算的时候恢复16精度。量化常数的选择是在向量维度进行的（矩阵最后维度）。</p><h2 id="介绍-1">介绍</h2><p>FFN和Attn的矩阵参数在模型总参数占比较高，且矩阵乘法占据了较多的计算资源。以往的量化方法降低了模型的表现并且需要量化后训练。同时没有对超过350M参数进行量化研究。这篇论文第一次提出针对百万级参数量的模型的量化方法且模型性能下降有限。应该也是第一篇把大模型量化到8比特的论文。</p><h2 id="量化前置知识">量化前置知识</h2><p>论文这里简单说明了非对称量化理论上能提供更高的精度，但是由于实际限制，用的最多的还是对称量化。</p><h3 id="对称量化">对称量化</h3><p>对于fp16格式的输入<span class="math inline">\(\mathbf{X}_{f16} \in\mathbb{R}^{s \times h}\)</span>，8比特的量化为：</p><p><span class="math display">\[\mathbf{X}_{i8} = \left\lfloor \frac{127 \cdot\mathbf{X}_{f16}}{\max_{ij} \left( |\mathbf{X}_{f16_{ij}}| \right)}\right\rceil= \left\lfloor \frac{127}{\|\mathbf{X}_{f16}\|_{\infty}}\mathbf{X}_{f16} \right\rceil= \left\lfloor s_{x_{f16}} \mathbf{X}_{f16} \right\rceil\]</span></p><p>其中<span class="math inline">\(\left\lfloor  \right\rceil\)</span>，代表四舍五入取整，即距离最近的整数。</p><h2 id="原理介绍">原理介绍</h2><p>这个量化的方法原理很简单，该方案先做了一个矩阵分解，对绝大部分权重和激活用8bit量化（<strong>vector</strong>-wise）。对离群特征的几个维度保留16bit，对其做高精度的矩阵乘法。</p><p><img src="/wiki/wiki/21264/1729756835272.png"></p><p>LLM.int8() 通过三个步骤完成矩阵乘法计算:</p><ol type="1"><li>从输入的隐含状态中，按列提取异常维度(离群特征，即大于某个阈值的值)。</li><li>对离群特征进行 FP16 矩阵运算，对非离群特征进行量化，做 INT8矩阵运算；</li><li>反量化非离群值的矩阵乘结果，并与离群值矩阵乘结果相加，获得最终的FP16 结果。</li></ol><h2 id="原理分析">原理分析</h2><p>这个方法能成功的一个重要原因是作者发现<strong>同一个序列的不同token异常维度集中在部分维度且异常维度数目较少</strong>。</p><p>首先，论文定义异常值的标准为：</p><ul><li>值需要大于等于6</li><li>影响25%的层</li><li>至少出现一个序列在6%的token中</li></ul><p>作者对此进行了解释：</p><ul><li>实验发现把大于等于6的值设置为异常值，困惑度退化就会停止</li><li>异常值特征是在大模型中系统出现的，要么出现在大多数层中，要么不出现；在小模型中概率出现。设置该阈值保证125M的最小模型中异常值只有一个（小模型中第二个出现最多的异常值仅仅出现2%的层）</li><li>使用和第二点相同的过程来选择异常值在序列维度的最小出现比例</li></ul><p>作者进行了实验，将满足条件的维度设置为0，比较最高可能性输出类别所分配的Softmax概率值。对比是随机选择相同数目的维度进行对比。</p><p><img src="/wiki/wiki/21264/1729962190147.png"></p><h2 id="实验">实验</h2><p>todo：量化效果</p><p><img src="/wiki/wiki/21264/1729961938672.png"></p><p>todo：加速效果</p><p>由于多余的反量化操作，在小模型上推理速度有所下降。</p><p><img src="/wiki/wiki/21264/1729962553598.png"></p><h2 id="bitsandbytes介绍">bitsandbytes介绍</h2><p>todo：介绍库的大致使用方法和部分疑点。</p><p><code>bitsandbytes</code>库在量化模型的时候，不需要数据首先直接把权重量化为8bit。在推理的时候，根据输入确定异常维度。输入的异常维度保持fp16，别的维度量化为int8。而量化后的权重，会把异常维度进行反量化为fp16（可能会有轻微损失），别的保持int8。之后的计算就和原理图保持一致。</p><h2 id="参考资料">参考资料</h2><blockquote><ul><li><a href="https://arxiv.org/abs/2208.07339">[2208.07339] LLM.int8():8-bit Matrix Multiplication for Transformers at Scale</a></li><li><a href="https://blog.csdn.net/scgaliguodong123_/article/details/136176382">大模型量化技术原理-LLM.int8()、GPTQ-CSDN博客</a></li><li><a href="https://github.com/bitsandbytes-foundation/bitsandbytes">bitsandbytes-foundation/bitsandbytes:Accessible large language models via k-bit quantization forPyTorch.</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;todo：本文还在施工中.......&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;llm.int8&lt;/code&gt;是第一批针对大模型进行量化的算法，并且其算法也被集成在
&lt;code&gt;bitsandb</summary>
      
    
    
    
    <category term="科研" scheme="https://bg51717.github.io/wiki/categories/%E7%A7%91%E7%A0%94/"/>
    
    <category term="论文阅读" scheme="https://bg51717.github.io/wiki/categories/%E7%A7%91%E7%A0%94/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="量化" scheme="https://bg51717.github.io/wiki/tags/%E9%87%8F%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>pytorch分布式-ddp</title>
    <link href="https://bg51717.github.io/wiki/45014/"/>
    <id>https://bg51717.github.io/wiki/45014/</id>
    <published>2024-10-09T05:30:13.000Z</published>
    <updated>2024-11-02T02:55:49.879Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍">介绍</h1><p>这篇博客主要是关于pytorch分布式ddp（DistributedDataParallel）的介绍和大概的食用（<del>这不是错别字</del>）教程。</p><p>数据并行DistributedDataParallel指的是在数据集层面进行多进程的切分，对于模型参数和训练状态等其他部分切分。</p><p>首先会介绍一下通信。在pytorch分布式ddp中，各个进程的代码是单独运行的。彼此之间在没有显式通信的时候，是不知道对方的的信息的。因此分布式的重点，所以了解通信的情况，也就了解了分布式的原理和使用的方法。</p><p>主要通信的方式有：</p><ul><li>环境变量</li><li>同步</li><li>tensor操作</li></ul><p>然后会介绍一下在数据并行下数据如何进行切分。</p><p>最后介绍整体pytorch分布式大概的流程和使用方法。</p><h1 id="通信">通信</h1><h2 id="环境变量">环境变量</h2><p>常用的环境变量有：</p><ul><li>WORLD_SIZE 全局进程数</li><li>RANK 当前进程全局标识符</li><li>LOCAL_RANK 在单个节点中的进程标识符</li><li>MASTER_ADDR 主节点IP地址</li><li>MASTER_PORT 主节点端口</li></ul><p>常用的为前三个，还有一些使用更加少的暂时没有罗列。</p><p>环境变量的获取可以：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"path"</span>]</span><br><span class="line">os.environ.get(<span class="string">'KEY_THAT_MIGHT_EXIST'</span>)</span><br><span class="line">os.getenv(<span class="string">'KEY_THAT_MIGHT_EXIST'</span>, default_value) <span class="comment"># 推荐</span></span><br></pre></td></tr></tbody></table></figure><h2 id="同步">同步</h2><p>引入pytorch分布式包</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br></pre></td></tr></tbody></table></figure><p>同步所有进程进度</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dist.barrier()</span><br></pre></td></tr></tbody></table></figure><p>在tensor通信的时候，也会起到同步进程的作用。很容易理解，不同步的话tensor的值都没有求得。</p><h2 id="tensor通信">tensor通信</h2><p>广播broadcast，收集gather，分发scatter，全收集all-gather，规约reduce，全规约all-reduce，全对称all-to-all，批量广播broadcast_object_list</p><h1 id="数据">数据</h1><p>在ddp中，只考虑的数据的剪切。那么对于某个进程，只需要计算部分数据即可。某个进程根据<strong>LOCAL_RANK</strong>获取自己所需的数据的方法有两种：</p><ol type="1"><li>数据集定义中加入offset，根据offset获取自己只需要的数据，那么进程只能看到自己的数据，比如<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, segment <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">open</span>(file)):</span><br><span class="line">  <span class="keyword">if</span> i % n_gpus != offset:</span><br><span class="line">    <span class="keyword">continue</span></span><br></pre></td></tr></tbody></table></figure></li><li>通过设置DataLoader中的sampler控制数据集采样实现数据切分，比如：<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, DistributedSampler</span><br><span class="line">sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>, sampler=sampler)</span><br></pre></td></tr></tbody></table></figure></li></ol><h1 id="使用方法">使用方法</h1><p>在通信前，需要进行初始化操作（如果init_process_group不指定部分参数，也会自动从环境变量中获取）：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化分布式进程</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">setup</span>():</span><br><span class="line">    rank = <span class="built_in">int</span>(os.environ[<span class="string">"RANK"</span>])</span><br><span class="line">    world_size = <span class="built_in">int</span>(os.environ[<span class="string">"WORLD_SIZE"</span>])</span><br><span class="line">    dist.init_process_group(<span class="string">"nccl"</span>, rank=rank, world_size=world_size)</span><br></pre></td></tr></tbody></table></figure><p>设置使用的GPU（可以灵活设置，比如每若干个进程共享GPU）:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rank = <span class="built_in">int</span>(os.environ[<span class="string">"RANK"</span>])</span><br><span class="line">torch.cuda.set_device(rank) <span class="comment"># 设置默认GPU</span></span><br><span class="line">device = torch.device(<span class="string">f"cuda:<span class="subst">{rank}</span>"</span>) <span class="comment"># 显式指定设备</span></span><br></pre></td></tr></tbody></table></figure><blockquote><p>使用的GPU还会收到环境变量CUDA_VISIBLE_DEVICES的限制</p><p>设置默认GPU可以让部分CUDA操作默认在该设备执行</p></blockquote><p>然后包装模型，隐式的进行<code>tensor</code>的同步和通信（在模型之外计算某些量（如精度、损失值等），可能需要同步）：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line">model = SimpleCNN().to(device)</span><br><span class="line">model = DDP(model, device_ids=[rank])</span><br></pre></td></tr></tbody></table></figure><blockquote><p>当然，这里也可以切换成别的过程，比如如果不是模型的训练和推理，也可以进行tensor别的计算方法，但是需要手动的进行通信等。</p></blockquote><p>对于一些多个进程只需要完成一次的操作，比如保存模型或者日志记录等，只需要一个进程一般是主进程完成即可：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> dist.get_rank() == <span class="number">0</span>:</span><br><span class="line">    torch.save(model.state_dict(), <span class="string">"model_checkpoint.pth"</span>)</span><br></pre></td></tr></tbody></table></figure><p>代码执行完需要进程组的销毁：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cleanup</span>():</span><br><span class="line">    dist.destroy_process_group()</span><br></pre></td></tr></tbody></table></figure><h1 id="代码执行">代码执行</h1><p>如果执行代码直接使用python，那么需要使用pytorch的包启动多进程：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.multiprocessing <span class="keyword">as</span> mp</span><br><span class="line">mp.spawn(train, nprocs=world_size, join=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure><p>如果直接使用 <code>torchrun</code>命令执行代码，则不需要使用<code>torch.multiprocessing</code>，但需要在命令里添加部分参数，等于调用<code>torch.multiprocessing</code>的任务交给<code>torchrun</code>完成：</p><figure class="highlight sh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchrun --nproc_per_node=4 your_script.py</span><br></pre></td></tr></tbody></table></figure><h1 id="参考资料">参考资料</h1><blockquote><ul><li><a href="https://pytorch.org/tutorials/distributed/home.html">Distributedand Parallel Training Tutorials — PyTorch Tutorials 2.4.0+cu121documentation</a></li><li><a href="https://blog.csdn.net/qq_39131062/article/details/109206804">“最全“PyTorch分布式训练教程来了！_pytorch训练-CSDN博客</a></li><li><a href="https://blog.csdn.net/qq_40185847/article/details/115074443">Pytorch中基于NCCL多GPU训练_pytorchnccl-CSDN博客</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;介绍&quot;&gt;介绍&lt;/h1&gt;
&lt;p&gt;这篇博客主要是关于pytorch分布式ddp（DistributedDataParallel）的介绍和大概的食用（&lt;del&gt;这不是错别字&lt;/del&gt;）教程。&lt;/p&gt;
&lt;p&gt;数据并行DistributedDataParallel指的是在</summary>
      
    
    
    
    <category term="模板" scheme="https://bg51717.github.io/wiki/categories/%E6%A8%A1%E6%9D%BF/"/>
    
    
    <category term="模板" scheme="https://bg51717.github.io/wiki/tags/%E6%A8%A1%E6%9D%BF/"/>
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="PyTorch" scheme="https://bg51717.github.io/wiki/tags/PyTorch/"/>
    
    <category term="分布式" scheme="https://bg51717.github.io/wiki/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    <category term="ddq" scheme="https://bg51717.github.io/wiki/tags/ddq/"/>
    
    <category term="数据并行" scheme="https://bg51717.github.io/wiki/tags/%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C/"/>
    
    <category term="Data Parallel" scheme="https://bg51717.github.io/wiki/tags/Data-Parallel/"/>
    
  </entry>
  
  <entry>
    <title>hexo博客2:双主题</title>
    <link href="https://bg51717.github.io/wiki/31680/"/>
    <id>https://bg51717.github.io/wiki/31680/</id>
    <published>2024-10-03T10:55:45.000Z</published>
    <updated>2024-11-02T05:32:24.133Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍">介绍</h1><p>后来发现单纯的wiki风格博客可能确实有些单调了（<del>绝对不是因为我想弄二次元风格的</del>），因此在考虑以后，决定搭建一个双主题的博客，外层是个华丽一点的主题<a href="https://github.com/blinkfox/hexo-theme-matery">matery</a>，内层是个wiki风格主题<a href="https://github.com/zthxxx/hexo-theme-Wikitten">Wikitten</a>。</p><p><img src="/wiki/wiki/31680/1727953154709.png"></p><p>同时可以通过外层的wiki标志点击进入内层的wiki主题。</p><p><img src="/wiki/wiki/31680/1727953220606.png"></p><p><img src="/wiki/wiki/31680/1727953171329.png"></p><h1 id="双主题安装">双主题安装</h1><p>参考文章：<a href="https://masantu.com/blog/2020-05-23/hello-hexo-wiki/">Hexo同时使用两种主题（博客与 wiki 页面实现统一管理） | 别院牧志(masantu.com)</a></p><ol type="1"><li><p>把原来wiki主题的 <code>_config.yml</code>复制并重命名为<code>_config_wiki.yml</code>（wiki主题下的站点文件</p></li><li><p>安装 <code>hexo-theme-matery</code>主题</p><figure class="highlight powershell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> your<span class="literal">-hexo-directory</span>/themes</span><br><span class="line">git clone https://github.com/blinkfox/hexo<span class="literal">-theme-matery</span></span><br></pre></td></tr></tbody></table></figure></li><li><p>按照 <code>matery</code>教程配置新的<code>_config_wiki.yml</code>文件</p></li><li><p>修改 <code>_config_wiki.yml</code>文件：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># URL</span></span><br><span class="line"><span class="comment">## Set your site url here. For example, if you use GitHub Page, set url as 'https://username.github.io/project'</span></span><br><span class="line"><span class="attr">url:</span> <span class="string">https://bg51717.github.io/wiki/</span></span><br><span class="line"><span class="attr">root:</span> <span class="string">/wiki/</span></span><br><span class="line"><span class="comment"># permalink: :year/:month/:day/:title/</span></span><br><span class="line"><span class="comment"># permalink_defaults:</span></span><br><span class="line"><span class="attr">permalink:</span> <span class="string">/:abbrlink/</span></span><br><span class="line"><span class="attr">abbrlink:</span></span><br><span class="line">    <span class="attr">alg:</span> <span class="string">crc16</span>   <span class="comment">#算法： crc16(default) and crc32</span></span><br><span class="line">    <span class="attr">rep:</span> <span class="string">dec</span>     <span class="comment">#进制： dec(default) and hex</span></span><br><span class="line"></span><br><span class="line"><span class="attr">permalink_defaults:</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># Directory</span></span><br><span class="line"><span class="attr">source_dir:</span> <span class="string">source</span></span><br><span class="line"><span class="attr">public_dir:</span> <span class="string">public/wiki/</span></span><br></pre></td></tr></tbody></table></figure></li></ol><h1 id="代码块行高问题">代码块行高问题</h1><p>在使用<code>prismjs</code>渲染代码块的时候，可能会遇到图示问题，行高不匹配，其实主要是位置没有对准：<img src="/wiki/wiki/31680/1730525332224.png"></p><p>可以参考：<a href="https://github.com/blinkfox/hexo-theme-matery/issues/928">hexo 7.3代码块显示问题 · Issue #928 · blinkfox/hexo-theme-matery</a></p><p>解决办法是把<code>themes\hexo-theme-matery\source\libs\prism\prism.min.css</code>文件里的：</p><figure class="highlight css"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-attr">[class*=language-]</span><span class="selector-class">.line-numbers</span>&gt;<span class="selector-tag">code</span>{<span class="attribute">position</span>:relative;<span class="attribute">white-space</span>:inherit}</span><br></pre></td></tr></tbody></table></figure><p>修改为：</p><figure class="highlight css"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-attr">[class*=language-]</span><span class="selector-class">.line-numbers</span>&gt;<span class="selector-tag">code</span>{<span class="attribute">position</span>:sticky;<span class="attribute">white-space</span>:inherit}</span><br></pre></td></tr></tbody></table></figure><h1 id="wiki图片链接问题">wiki图片链接问题</h1><p>由于渲染的站点 <code>url</code>和 <code>root</code>里都有<code>wiki</code>，因此会导致渲染出来的图片出现两次<code>/wiki</code>，因此需要代码进行处理，核心函数如下所示：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">post_html_file</span>(<span class="params">file_path</span>):</span><br><span class="line">    <span class="string">"""处理html文件，只修改图片链接中的重复/wiki/"""</span></span><br><span class="line">    <span class="comment"># 如果不是html文件, 返回</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> file_path.endswith(<span class="string">".html"</span>):</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读取文件内容</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">"r"</span>, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> file:</span><br><span class="line">        content = file.read()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义正则表达式来匹配图片链接，处理 &lt;img&gt; 标签中的 src 属性</span></span><br><span class="line">    img_pattern = <span class="string">r'&lt;img[^&gt;]+src="([^"]+)"'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查找所有匹配的图片链接</span></span><br><span class="line">    matches = re.findall(img_pattern, content)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历所有匹配的图片链接，替换掉重复的 /wiki/</span></span><br><span class="line">    modified_content = content</span><br><span class="line">    <span class="keyword">for</span> <span class="keyword">match</span> <span class="keyword">in</span> matches:</span><br><span class="line">        <span class="comment"># 如果链接中有重复的 /wiki/，替换为单一的 /wiki/</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">"/wiki/wiki/"</span> <span class="keyword">in</span> <span class="keyword">match</span>:</span><br><span class="line">            corrected_url = <span class="keyword">match</span>.replace(<span class="string">"/wiki/wiki/"</span>, <span class="string">"/wiki/"</span>)</span><br><span class="line">            <span class="comment"># 替换内容中的旧链接为新的链接</span></span><br><span class="line">            modified_content = modified_content.replace(<span class="keyword">match</span>, corrected_url)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将修改后的内容写回文件</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">"w"</span>, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> file:</span><br><span class="line">        file.write(modified_content)</span><br></pre></td></tr></tbody></table></figure><h1 id="图库api设置">图库APi设置</h1><p>对于外层主题，需要自己设置相关的图片，笔者从<a href="https://t.alcy.cc/">栗次元API-举个栗子图库API</a>爬取部分图片作为图片。也有别的食用方法可以参考该网站具体的教程。</p><h1 id="搜索链接跳转错误">搜索链接跳转错误</h1><p>由于把每篇博客链接到了一个数字，从而起到了缩短博客网址长度和增加搜索引擎爬取的概率，但是可能会导致原有的搜索跳转到随机IP。原因是：网址返回的是纯数字相对链接，比如<code>/7369</code>，完整的链接为<code>https://bg51717.github.io/7369/</code>，但是浏览器可能会把链接解析为ip地址，变成<code>0.0.28.201</code>，等同于256进制下的<code>7369</code>，从而导致跳转错误。因此需要调整搜索代码，每次返回完整链接，具体如下：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">if (data_url.indexOf('http') !== 0) {</span><br><span class="line">    // 生成绝对路径，确保基于网站根目录</span><br><span class="line">    data_url =  window.location.host +'/'+root+'/'+ data_url;</span><br><span class="line">    data_url = data_url.replace(/\/+/g, '/'); // 去掉连续的斜杠</span><br><span class="line">    data_url = data_url.replace(/^\//, '');   // 去掉开头的斜杠</span><br><span class="line">    data_url = window.location.protocol + '//' + data_url;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h1 id="渲染页面">渲染页面</h1><p>渲染部分的参考主要来自：<a href="https://masantu.com/blog/2020-05-23/hello-hexo-wiki/">Hexo同时使用两种主题（博客与 wiki 页面实现统一管理） | 别院牧志</a></p><ol type="1"><li><p>渲染主页面，生成到/public/</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br></pre></td></tr></tbody></table></figure></li><li><p>删除 db.json 及旧的 public/wiki</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo --config _config_wiki.yml clean</span><br></pre></td></tr></tbody></table></figure></li><li><p>渲染wiki页面</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo --config _config_wiki.yml g</span><br></pre></td></tr></tbody></table></figure></li><li><p>删除db.json</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm db.json</span><br></pre></td></tr></tbody></table></figure></li></ol><p>如果每次执行上述步骤都会稍显复杂，因此考虑整理脚本文件<code>run.py</code>，执行渲染时 <code>python run.py</code>即可：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> platform</span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_command</span>(<span class="params">command</span>):</span><br><span class="line">    <span class="string">"""实时执行命令并将输出打印到控制台"""</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># 使用 Popen 来执行命令并实时读取输出</span></span><br><span class="line">        process = subprocess.Popen(command, shell=<span class="literal">True</span>, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=<span class="literal">True</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 实时读取 stdout 和 stderr</span></span><br><span class="line">        <span class="keyword">for</span> stdout_line <span class="keyword">in</span> <span class="built_in">iter</span>(process.stdout.readline, <span class="string">""</span>):</span><br><span class="line">            <span class="built_in">print</span>(stdout_line, end=<span class="string">''</span>)  <span class="comment"># 实时输出 stdout</span></span><br><span class="line"></span><br><span class="line">        process.stdout.close()</span><br><span class="line">        return_code = process.wait()  <span class="comment"># 等待命令执行结束</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出 stderr（如果有）</span></span><br><span class="line">        <span class="keyword">if</span> return_code != <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">for</span> stderr_line <span class="keyword">in</span> <span class="built_in">iter</span>(process.stderr.readline, <span class="string">""</span>):</span><br><span class="line">                <span class="built_in">print</span>(stderr_line, end=<span class="string">''</span>)  <span class="comment"># 实时输出 stderr</span></span><br><span class="line">        process.stderr.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span> subprocess.CalledProcessError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"命令 '<span class="subst">{command}</span>' 执行失败，错误信息: <span class="subst">{e.stderr}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clean_generate_hexo</span>():</span><br><span class="line">    <span class="string">"""执行 hexo clean 和 hexo g 命令，以及相关的配置命令"""</span></span><br><span class="line">    os_type = platform.system()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据操作系统选择 rm 或 del</span></span><br><span class="line">    remove_command = <span class="string">"rm db.json"</span> <span class="keyword">if</span> os_type != <span class="string">"Windows"</span> <span class="keyword">else</span> <span class="string">"del db.json"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义需要执行的命令列表</span></span><br><span class="line">    commands = [</span><br><span class="line">        <span class="string">"python -m process.run_before"</span>,</span><br><span class="line">        <span class="string">"hexo clean"</span>,</span><br><span class="line">        <span class="string">"hexo g"</span>,</span><br><span class="line">        <span class="string">"hexo --config _config_wiki.yml clean"</span>,</span><br><span class="line">        <span class="string">"hexo --config _config_wiki.yml g"</span>,</span><br><span class="line">        remove_command,</span><br><span class="line">        <span class="string">"python -m process.run_post"</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 逐一执行命令</span></span><br><span class="line">    <span class="keyword">for</span> command <span class="keyword">in</span> commands:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"\n正在执行命令: <span class="subst">{command}</span>"</span>)</span><br><span class="line">        run_command(command)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    clean_generate_hexo()</span><br></pre></td></tr></tbody></table></figure><h1 id="参考资料">参考资料</h1><blockquote><ul><li><a href="https://github.com/blinkfox/hexo-theme-matery">blinkfox/hexo-theme-matery:A beautiful hexo blog theme with material design and responsivedesign.一个基于材料设计和响应式设计而成的全面、美观的Hexo主题。国内访问：http://blinkfox.com(github.com)</a></li><li><a href="https://github.com/zthxxx/hexo-theme-Wikitten">zthxxx/hexo-theme-Wikitten:A theme of Hexo for personal wiki which seems like Wikitten style.(github.com)</a></li><li><a href="https://masantu.com/blog/2020-05-23/hello-hexo-wiki/">Hexo同时使用两种主题（博客与 wiki 页面实现统一管理） | 别院牧志(masantu.com)</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;介绍&quot;&gt;介绍&lt;/h1&gt;
&lt;p&gt;后来发现单纯的wiki风格博客可能确实有些单调了（&lt;del&gt;绝对不是因为我想弄二次元风格的&lt;/del&gt;），因此在考虑以后，决定搭建一个双主题的博客，外层是个华丽一点的主题&lt;a href=&quot;https://github.com/blin</summary>
      
    
    
    
    <category term="SmallProjects" scheme="https://bg51717.github.io/wiki/categories/SmallProjects/"/>
    
    <category term="博客搭建" scheme="https://bg51717.github.io/wiki/categories/SmallProjects/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    
    
    <category term="博客" scheme="https://bg51717.github.io/wiki/tags/%E5%8D%9A%E5%AE%A2/"/>
    
    <category term="Hexo" scheme="https://bg51717.github.io/wiki/tags/Hexo/"/>
    
    <category term="Wiki" scheme="https://bg51717.github.io/wiki/tags/Wiki/"/>
    
  </entry>
  
  <entry>
    <title>使用dotbot快速同步Linux配置</title>
    <link href="https://bg51717.github.io/wiki/13162/"/>
    <id>https://bg51717.github.io/wiki/13162/</id>
    <published>2024-10-03T07:37:15.000Z</published>
    <updated>2024-11-02T02:55:49.875Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍">介绍</h1><p><strong>dotfiles</strong>指的是<code>.</code>开头的隐藏文件夹，一般是用户的配置或者软件信息。使用服务器或者Linux的时候，安装一些软件配置自己的使用环境是十分常见的场景。一个优秀的配置和各种软件不仅可以大幅提升工作效率，还可以美化工作环境，加强工作的动力。但是很多时候一个完整的配置是十分复杂且繁琐的，也难以去记忆每次配置时的信息。</p><p>因此很多人尝试收集配置文件，创建软连接，然后整理安装脚本，上传到github。实现难度较低。</p><p>这篇博客推荐使用基于Git的<a href="https://github.com/anishathalye/dotbot">dotbot</a>来管理dotfiles。自己编写管理脚本可能会导致脚本经常需要修改来使用不同的场合。框架在设计的时候会考虑到大部分场景，因此需要的修改和可能导致的错误较少。</p><h1 id="git子模块">Git子模块</h1><p>在介绍 <code>dotbot</code>之前需要了解一下<code>Git子模块(submodule)</code>的观念。当你的仓库依赖于别的仓库的时候，你可以添加一个链接指向被依赖的仓库的某个版本，而不需要去额外复制这些文件。</p><h2 id="基本操作">基本操作</h2><p>添加子模块：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git submodule add &lt;仓库地址&gt; &lt;子模块路径&gt;</span><br></pre></td></tr></tbody></table></figure><blockquote><p>&lt;仓库地址&gt;是仓库的网络地址，&lt;子模块路径&gt;指的是相当于这个仓库的路径<!--子模块路径--><!--仓库地址--></p><p>示例：git submodule add https://github.com/example/library.gitlibs/library</p></blockquote><p>初始化子模块，当你克隆具有子模块的仓库的时候，需要手动初始化并更新子模块：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git submodule init</span><br><span class="line">git submodule update</span><br></pre></td></tr></tbody></table></figure><blockquote><p>也可以一次更新</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --recurse-submodules &lt;仓库地址&gt;</span><br></pre></td></tr></tbody></table></figure></blockquote><p>子模块管理，子模块可以像正常的仓库一样进行管理：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> &lt;子模块路径&gt;</span><br><span class="line">git pull</span><br><span class="line">git add .</span><br><span class="line">git commit -m <span class="string">"Update submodule"</span></span><br><span class="line">git push</span><br></pre></td></tr></tbody></table></figure><blockquote><p>如果需要对子模块更新且不是子模块的作者的话，建议fork仓库作为子模块，fork仓库的管理此处不再阐述。</p></blockquote><p>删除子模块，删除子模块需要调整 <code>.gitmodules</code>和<code>.git/config</code>配置：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git submodule deinit &lt;子模块路径&gt;</span><br><span class="line">git <span class="built_in">rm</span> --cached &lt;子模块路径&gt;</span><br><span class="line"><span class="built_in">rm</span> -rf &lt;子模块路径&gt;</span><br></pre></td></tr></tbody></table></figure><p>修改子模块url：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git submodule set-url &lt;子模块路径&gt; &lt;新的URL&gt;</span><br></pre></td></tr></tbody></table></figure><p>修改子模块路径：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">mv</span> &lt;旧路径&gt; &lt;新路径&gt;</span><br><span class="line">git submodule <span class="built_in">sync</span></span><br></pre></td></tr></tbody></table></figure><h1 id="安装">安装</h1><p>首先你需要一个<code>dotfiles</code>文件夹（名字可以自定义），里面是你所有的配置文件。</p><p>之后进入这个文件夹，添加 <code>dotbot</code>作为子模块：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize Repository</span></span><br><span class="line">git init</span><br><span class="line">git submodule add https://github.com/anishathalye/dotbot</span><br><span class="line"><span class="built_in">cp</span> dotbot/tools/git-submodule/install .</span><br><span class="line"><span class="built_in">touch</span> install.config.yaml</span><br></pre></td></tr></tbody></table></figure><blockquote><p>也推荐把需要安装的软件作为子模块使用</p></blockquote><h1 id="配置">配置</h1><p>通过修改 <code>install.config.yaml</code>可以调整安装命令<code>./install</code>的工作。</p><p>默认的配置为：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="attr">defaults:</span></span><br><span class="line">    <span class="attr">link:</span></span><br><span class="line">      <span class="attr">relink:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">clean:</span> [<span class="string">'~'</span>]</span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">link:</span></span><br><span class="line">    <span class="string">~/.bashrc:</span> <span class="string">bashrc</span></span><br><span class="line">    <span class="string">~/.zshrc:</span> <span class="string">zshrc</span></span><br><span class="line">    <span class="string">~/.vimrc:</span> <span class="string">vimrc</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">shell:</span></span><br><span class="line">    <span class="bullet">-</span> [<span class="string">git</span> <span class="string">submodule</span> <span class="string">update</span> <span class="string">--init</span> <span class="string">--recursive</span>, <span class="string">Installing</span> <span class="string">submodules</span>]</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><blockquote><p>几个类别的顺序会影响命令执行时的顺序</p></blockquote><p>主要类别有：</p><ul><li>defaults：会对所有操作进行的设置</li><li>clean：哪些links会被检查是否dead，如果是dead links会被删除</li><li>link：创建软链接的源目录和目标目录</li><li>shell：希望运行的指令</li></ul><p>之后就可以简单的使用git来管理dotfile，并且使用<code>./install</code>安装即可。</p><h1 id="参考资料">参考资料</h1><blockquote><ul><li><a href="https://github.com/anishathalye/dotbot">anishathalye/dotbot: Atool that bootstraps your dotfiles ⚡️ (github.com)</a></li><li><a href="https://anishathalye.com/managing-your-dotfiles/">管理您的点文件(anishathalye.com)</a></li><li><a href="https://dotfiles.github.io/">GitHub does dotfiles -dotfiles.github.io</a></li><li>杰哥</li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;介绍&quot;&gt;介绍&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;dotfiles&lt;/strong&gt;指的是
&lt;code&gt;.&lt;/code&gt;开头的隐藏文件夹，一般是用户的配置或者软件信息。使用服务器或者Linux的时候，安装一些软件配置自己的使用环境是十分常见的场景。一个优秀的配置和各种</summary>
      
    
    
    
    <category term="工具" scheme="https://bg51717.github.io/wiki/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="dotfiles" scheme="https://bg51717.github.io/wiki/tags/dotfiles/"/>
    
    <category term="dotbot" scheme="https://bg51717.github.io/wiki/tags/dotbot/"/>
    
    <category term="Linux" scheme="https://bg51717.github.io/wiki/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>zsh+powerlevel10K优化终端使用体验</title>
    <link href="https://bg51717.github.io/wiki/13107/"/>
    <id>https://bg51717.github.io/wiki/13107/</id>
    <published>2024-09-29T13:09:00.000Z</published>
    <updated>2024-11-02T02:55:49.874Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍">介绍</h1><p><strong>ZSH</strong>（Zshell）类似Bash，是被广泛用于类Unix系统的命令行解释器。在具备Bash的基本功能的同时，还扩展了很多功能，同时对插件的支持和高度定制化使其成为了很多Linux用户的最佳选择。经常使用的功能有：自动补全，历史命令，语法高亮等。</p><p><strong>powerlevel10K</strong>是ZSH的主题之一，但是扩展了一些额外的功能，比如更多信息的显示，运行时间和当前时间的查看等。</p><p>通过灵活使用这两个工具，可以在美化你的终端页面的同时提升你的效率。</p><blockquote><p>以下操作默认使用的<strong>Ubuntu</strong>系统。</p></blockquote><h1 id="zsh">ZSH</h1><h2 id="安装">安装</h2><p>Ubuntu：</p><ul><li><p>安装zsh</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install zsh</span><br></pre></td></tr></tbody></table></figure></li><li><p>设置为默认shell</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo chsh -s /bin/zsh</span><br><span class="line"><span class="comment"># 为其他用户设置：sudo chsh -s /bin/zsh &lt;username&gt;</span></span><br></pre></td></tr></tbody></table></figure></li><li><p>安装git</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install git</span><br></pre></td></tr></tbody></table></figure></li><li><p>安装oh-my-zsh 手动安装:</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --depth=1 https://github.com/ohmyzsh/ohmyzsh.git ~/.oh-my-zsh</span><br><span class="line"><span class="built_in">cp</span> ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc</span><br></pre></td></tr></tbody></table></figure><p>或 自动安装:</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -c <span class="string">"<span class="subst">$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)</span>"</span></span><br></pre></td></tr></tbody></table></figure></li></ul><h2 id="zsh设置">ZSH设置</h2><p>通过修改配置文件 <code>~/.zshrc</code>可以修改终端设置。经常使用的设置应该有：</p><ul><li>ZSH_THEME：ZSH主题。</li><li>plugins：ZSH插件。</li></ul><p>同时一些希望终端启动时运行的命令也可以放在该文件里，比如conda的启动。</p><blockquote><p>也就是终端加载的时候会运行的命令，也可以<code>source ~/.zshrc</code>更新配置文件对终端的设置</p></blockquote><h2 id="插件推荐">插件推荐</h2><h3 id="sh-completions"><a href="https://github.com/zsh-users/zsh-completions">sh-completions</a></h3><p>这个插件提供了基础的补全功能。目前似乎已经被集成到ZSH项目中。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --depth=1 https://github.com/zsh-users/zsh-completions <span class="variable">${ZSH_CUSTOM:-<span class="variable">${ZSH:-~/.oh-my-zsh}</span>/custom}</span>/plugins/zsh-completions</span><br></pre></td></tr></tbody></table></figure><h3 id="zsh-autosuggestions"><a href="https://github.com/zsh-users/zsh-autosuggestions">zsh-autosuggestions</a></h3><p>这个插件可以提供自动补全的功能，比如输入 <code>git</code>，自动补全为<code>git status</code>。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --depth=1 https://github.com/zsh-users/zsh-autosuggestions.git <span class="variable">${ZSH_CUSTOM:-<span class="variable">${ZSH:-~/.oh-my-zsh}</span>/custom}</span>/plugins/zsh-autosuggestions</span><br></pre></td></tr></tbody></table></figure><h3 id="incremental-completion-on-zsh"><a href="https://mimosa-pudica.net/zsh-incremental.html">Incrementalcompletion on zsh</a></h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> <span class="variable">$ZSH_CUSTOM</span>/plugins/incr</span><br><span class="line">curl -fsSL https://mimosa-pudica.net/src/incr-0.2.zsh -o <span class="variable">$ZSH_CUSTOM</span>/plugins/incr/incr.zsh</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'source $ZSH_CUSTOM/plugins/incr/incr.zsh'</span> &gt;&gt; ~/.zshrc</span><br><span class="line"><span class="built_in">source</span> ~/.zshrc</span><br></pre></td></tr></tbody></table></figure><h3 id="zsh-syntax-highlighting"><a href="https://github.com/zsh-users/zsh-syntax-highlighting">zsh-syntax-highlighting</a></h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --depth=1 https://github.com/zsh-users/zsh-syntax-highlighting.git <span class="variable">${ZSH_CUSTOM:-~/.oh-my-zsh/custom}</span>/plugins/zsh-syntax-highlighting</span><br></pre></td></tr></tbody></table></figure><h1 id="powerlevel10k">powerlevel10K</h1><p>目前配置的终端可以显示当前的虚拟环境、路径、git情况，以及上个命令的执行时间等。基本需要都被涵盖。这写额外的功能和美化得归功于ZSH主题——<strong>powerlevel10K</strong>。具体效果图如下所示：<img src="/wiki/wiki/13107/1727664718688.png"></p><h2 id="安装-1">安装</h2><h3 id="字体">字体</h3><p>由于会显示一些额外的符号，所以需要安装新的字体。</p><blockquote><p>字体需要在显示终端的设备上安装。比如如果系统在本地，那么在本地安装。如果连接的远程，那么需要在远程上安装ZSH和主题，在本地安装字体。原理不难理解。</p></blockquote><p>字体链接：<a href="https://github.com/ryanoasis/nerd-fonts#font-installation">NerFont</a>字体链接里有完整的安装教程，可以根据自己的平台进行安装。如果是克隆源码安装的时候注意设置depth，建议从该仓库的releases里选择字体进行安装。根据平台情况安装然后后设置字体显示即可，一般需要在终端或者编辑器里设置字体选项。考虑到种类较多且教程丰富简单，此处不罗列详细步骤。</p><blockquote><p>Tips：如果设置的是Vscode平台，需要设置整体的字体： Editor:FontFamily，单独设置终端字体好像无法正常工作。 <img src="/wiki/wiki/13107/1727665397412.png"></p></blockquote><h3 id="主题">主题</h3><p>使用命令安装到 <code>~/.oh-my-zsh</code>目录下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --depth=1 https://github.com/romkatv/powerlevel10k.git <span class="variable">${ZSH_CUSTOM:-<span class="variable">$HOME</span>/.oh-my-zsh/custom}</span>/themes/powerlevel10k</span><br></pre></td></tr></tbody></table></figure><p>在 <code>~/.zshrc</code>下启动主题：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ZSH_THEME="powerlevel10k/powerlevel10k"</span><br></pre></td></tr></tbody></table></figure><h2 id="配置">配置</h2><p>重启终端或者<code>source ~/.zshrc</code>切换主题，之后会进入配置界面。（配置完成后也可以使用<code>p10k configure</code>重新进入配置向导）配置向导就是一些问题来判断你的字体情况和个人偏好，然后写入配置文件<code>~/.p10k.sh</code>中，大约有十几个问题。配置向导的详细介绍可以参考：<a href="https://juejin.cn/post/7293342627814244367">我的终端环境：与众不同的zsh 主题 - powerlevel10k本文介绍 zsh 主题 powerlevel10k - 掘金</a>在配置向导完成后，如果需要一些额外的配置。可以修改配置文件<code>~/.p10k.sh</code>。 比如，通过修改<code>POWERLEVEL9K_LEFT_PROMPT_ELEMENTS</code>和<code>POWERLEVEL9K_RIGHT_PROMPT_ELEMENTS</code>可以修改两侧显示的元素以及顺序等。</p><h1 id="参考资料">参考资料</h1><blockquote><ul><li><a href="https://sysin.org/blog/linux-zsh/">Linux Zsh 使用 oh-my-zsh打造高效便捷的 shell 环境 - sysin | SYStem INside |软件与技术分享</a></li><li><a href="https://juejin.cn/post/7293342627814244367">我的终端环境：与众不同的zsh 主题 - powerlevel10k本文介绍 zsh 主题 powerlevel10k - 掘金</a></li><li>杰哥</li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;介绍&quot;&gt;介绍&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;ZSH&lt;/strong&gt;（Z
shell）类似Bash，是被广泛用于类Unix系统的命令行解释器。在具备Bash的基本功能的同时，还扩展了很多功能，同时对插件的支持和高度定制化使其成为了很多Linux用户的最佳选择。经</summary>
      
    
    
    
    <category term="工具" scheme="https://bg51717.github.io/wiki/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="终端" scheme="https://bg51717.github.io/wiki/tags/%E7%BB%88%E7%AB%AF/"/>
    
    <category term="oh-my-zsh" scheme="https://bg51717.github.io/wiki/tags/oh-my-zsh/"/>
    
    <category term="powerlevel10k" scheme="https://bg51717.github.io/wiki/tags/powerlevel10k/"/>
    
  </entry>
  
  <entry>
    <title>安卓手机配置Google</title>
    <link href="https://bg51717.github.io/wiki/61294/"/>
    <id>https://bg51717.github.io/wiki/61294/</id>
    <published>2024-08-27T10:14:49.000Z</published>
    <updated>2024-11-02T02:55:49.875Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p>这篇博客主要记录了如何在安卓手机上配置谷歌三件套的服务。</p><p>对于非华为荣耀手机，可能仅仅需要简单的使用一些第三方的安装软件即可完成，比如<code>go安装助手</code>等，资源较大且获取难度较低。</p><p>而本篇博客主要介绍华为荣耀手机如何获取谷歌三件套的服务和配置支付信息等。</p><p>介绍两个并行的方法，当其中一个方法失效的时候，可以用另一个方法的部分替代。</p><p>方法是 <code>华谷套件</code>和<a href="https://github.com/to-alan/HarmonyOSInstallGMS?tab=readme-ov-file">to-alan/HarmonyOSInstallGMS:华为安装GMS教程</a> 。</p><p>博主前面的流程都使用的是华谷套件，该软件可以在每一步运行完后自动检测是否设置成功。在卸载MicroG后，转为使用方法二进行后续的处理。</p><p>目前手机谷歌三件套运行稳定，基本可以提供原生的谷歌三件套服务。</p><blockquote><p>支付方式的添加todo。</p></blockquote><p>参考资料</p><blockquote><ul><li><a href="https://github.com/to-alan/HarmonyOSInstallGMS?tab=readme-ov-file">to-alan/HarmonyOSInstallGMS:华为安装GMS教程</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;这篇博客主要记录了如何在安卓手机上配置谷歌三件套的服务。&lt;/p&gt;
&lt;p&gt;对于非华为荣耀手机，可能仅仅需要简单的使用一些第三方的安装软件即可完成，比如
&lt;code&gt;go安装助手&lt;/code&gt;等，资源较大且获取难度较低。&lt;/p&gt;
&lt;p&gt;而</summary>
      
    
    
    
    <category term="工具" scheme="https://bg51717.github.io/wiki/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
    <category term="安卓" scheme="https://bg51717.github.io/wiki/tags/%E5%AE%89%E5%8D%93/"/>
    
    <category term="Google" scheme="https://bg51717.github.io/wiki/tags/Google/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch代码转HF</title>
    <link href="https://bg51717.github.io/wiki/61054/"/>
    <id>https://bg51717.github.io/wiki/61054/</id>
    <published>2024-08-06T10:56:13.000Z</published>
    <updated>2024-11-02T02:55:49.879Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p>这篇博客主要介绍了怎么把一个已有的Pytorch代码转变成HF支持的格式，然后可以方便的放入HF代码流程中，并且使用一些HF的函数。代码转换主要涉及到以下几个方面：</p><ul><li>Config</li><li>Model</li><li>Trainer</li><li>Dataset</li></ul><p>因为ckpt里面的代码使用的会是相对导入，所以在转换的过程中，建议把<code>configuration_xxx.py</code>和<code>modeling_xxx.py</code>文件放在同一个目录下，并且添加<code>__init__.py</code>文件。</p><h2 id="config">Config</h2><p>参考：<a href="https://huggingface.co/docs/transformers/custom_models#building-custom-models">Buildingcustom models (huggingface.co)</a></p><p>示例：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> PretrainedConfig</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LtgBertConfig</span>(<span class="title class_ inherited__">PretrainedConfig</span>):</span><br><span class="line">    model_type = <span class="string">"LtgBert"</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"""Configuration class to store the configuration of a `LtgBertModel`.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 vocab_size_or_config_json_file=<span class="number">16384</span>,</span></span><br><span class="line"><span class="params">                 hidden_size=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">                 num_hidden_layers=<span class="number">12</span>,</span></span><br><span class="line"><span class="params">                 num_attention_heads=<span class="number">12</span>,</span></span><br><span class="line"><span class="params">                 intermediate_size=<span class="number">3072</span>,</span></span><br><span class="line"><span class="params">                 hidden_act=<span class="string">"gelu"</span>,</span></span><br><span class="line"><span class="params">                 hidden_dropout_prob=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                 attention_probs_dropout_prob=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                 max_position_embeddings=<span class="number">512</span>,</span></span><br><span class="line"><span class="params">                 type_vocab_size=<span class="number">2</span>,</span></span><br><span class="line"><span class="params">                 initializer_range=<span class="number">0.02</span>,</span></span><br><span class="line"><span class="params">                 output_all_encoded_layers=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 require_all_hidden_states=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                 batch_first=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                 **kwargs</span>):</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.num_hidden_layers = num_hidden_layers</span><br><span class="line">        self.num_attention_heads = num_attention_heads</span><br><span class="line">        self.hidden_act = hidden_act</span><br><span class="line">        self.intermediate_size = intermediate_size</span><br><span class="line">        self.hidden_dropout_prob = hidden_dropout_prob</span><br><span class="line">        self.attention_probs_dropout_prob = attention_probs_dropout_prob</span><br><span class="line">        self.max_position_embeddings = max_position_embeddings</span><br><span class="line">        self.type_vocab_size = type_vocab_size</span><br><span class="line">        self.initializer_range = initializer_range</span><br><span class="line">        self.output_all_encoded_layers = output_all_encoded_layers</span><br><span class="line">        self.require_all_hidden_states = require_all_hidden_states</span><br><span class="line">        self.batch_first=batch_first</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(vocab_size_or_config_json_file, <span class="built_in">str</span>) <span class="keyword">or</span> (sys.version_info[<span class="number">0</span>] == <span class="number">2</span></span><br><span class="line">                        <span class="keyword">and</span> <span class="built_in">isinstance</span>(vocab_size_or_config_json_file, unicode)):</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(vocab_size_or_config_json_file, <span class="string">"r"</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> reader:</span><br><span class="line">                json_config = json.loads(reader.read())</span><br><span class="line">            <span class="keyword">for</span> key, value <span class="keyword">in</span> json_config.items():</span><br><span class="line">                self.__dict__[key] = value</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(vocab_size_or_config_json_file, <span class="built_in">int</span>):</span><br><span class="line">            self.vocab_size = vocab_size_or_config_json_file</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"First argument must be either a vocabulary size (int)"</span></span><br><span class="line">                             <span class="string">"or the path to a pretrained model config file (str)"</span>)</span><br><span class="line">        <span class="built_in">super</span>(LtgBertConfig, self).__init__(**kwargs)</span><br></pre></td></tr></tbody></table></figure><p>必须满足：</p><ul><li>继承自 <code>PretrainedConfig</code></li><li><code>__init__</code>函数接受 <code>kwargs</code>，并且使用<code>super()).__init__</code>传递这些参数</li></ul><p><code>model_type</code>的作用是把模型注册到<code>AutoClass</code>中，建议设置。</p><h2 id="model">Model</h2><p>参考：<a href="https://huggingface.co/docs/transformers/custom_models#building-custom-models">Buildingcustom models (huggingface.co)</a></p><p>示例：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LtgBertForMaskedLM</span>(<span class="title class_ inherited__">PreTrainedModel</span>):</span><br><span class="line">    config_class=LtgBertConfig</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,config,activation_checkpointing=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(config)</span><br><span class="line"><span class="comment"># 这里可以把成员变成类的继承LtgBertForMaskedLM(Bert):</span></span><br><span class="line">        self.model=Bert(</span><br><span class="line">            config=config,</span><br><span class="line">            activation_checkpointing=activation_checkpointing</span><br><span class="line">        )</span><br><span class="line">        self.require_all_hidden_states=config.require_all_hidden_states</span><br><span class="line">        self.batch_first=config.batch_first</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask, masked_lm_labels=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> self.batch_first:</span><br><span class="line">            <span class="comment"># 模型把batch放在第二个维度</span></span><br><span class="line">            input_ids=input_ids.transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> masked_lm_labels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                masked_lm_labels=masked_lm_labels.transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">        subword_prediction=self.model(input_ids, attention_mask, masked_lm_labels=masked_lm_labels)</span><br><span class="line">        loss=<span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> masked_lm_labels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            target_ids = masked_lm_labels.flatten()</span><br><span class="line">            target_ids = target_ids[target_ids != -<span class="number">100</span>]</span><br><span class="line">            loss = F.cross_entropy(subword_prediction, target_ids)</span><br><span class="line">        all_hidden_states=<span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> self.require_all_hidden_states:</span><br><span class="line">            all_hidden_states=self.model.get_contextualized(input_ids=input_ids,attention_mask=attention_mask)</span><br><span class="line">        <span class="keyword">if</span> self.batch_first:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(subword_prediction.size())&gt;<span class="number">2</span>:</span><br><span class="line">                subword_prediction=subword_prediction.transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> all_hidden_states <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                all_hidden_states=[it.transpose(<span class="number">0</span>,<span class="number">1</span>) <span class="keyword">for</span> it <span class="keyword">in</span> all_hidden_states]</span><br><span class="line">        <span class="keyword">return</span> MaskedLMOutput(</span><br><span class="line">            loss=loss,</span><br><span class="line">            logits=subword_prediction,</span><br><span class="line">            hidden_states=all_hidden_states,</span><br><span class="line">            attentions=<span class="literal">None</span></span><br><span class="line">        )</span><br></pre></td></tr></tbody></table></figure><p>对于自定义模型，往往每个<code>AutoClass</code>上都会注册一个模型，因此往往要写多个自定义模型。</p><p><code>config_type</code>的作用是把模型注册到<code>AutoClass</code>中，建议设置。</p><p>由于简约性原则，官方要求<code>self.model</code>对应原来的模型，比如用Pytorch定义的模型。</p><p><code>forward</code>函数需要注意结果格式，<code>transformers.modeling_outputs</code>里定义了每种模型forward的结果格式。</p><p>其中对于每个特定的子任务都有个类似的模型，对于部分函数比如forward建议参考已有的代码进行操作，因为hf框架在使用特定子任务的模型的时候，可能会添加特殊的参数。比如，对于序列分类任务SequenceClassification，其中相关模型的forward为：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">def forward(</span><br><span class="line">        self,</span><br><span class="line">        input_ids: Optional[torch.Tensor] = None,</span><br><span class="line">        attention_mask: Optional[torch.Tensor] = None,</span><br><span class="line">        output_attentions: Optional[bool] = None,</span><br><span class="line">        output_hidden_states: Optional[bool] = None,</span><br><span class="line">        inputs_embeds: Optional[torch.Tensor] = None,</span><br><span class="line">        return_dict: Optional[bool] = None,</span><br><span class="line">        labels: Optional[torch.LongTensor] = None,</span><br><span class="line">    ) -&gt; Union[Tuple[torch.Tensor], SequenceClassifierOutput]:</span><br><span class="line">        if self.batch_first:</span><br><span class="line">            # 模型把batch放在第二个维度</span><br><span class="line">            input_ids=input_ids.transpose(0,1)</span><br><span class="line">        contextualized_embeddings=self.model.get_contextualized(input_ids, attention_mask)</span><br><span class="line">        if self.batch_first:</span><br><span class="line">            contextualized_embeddings=contextualized_embeddings.transpose(0,1)</span><br><span class="line">        logits = self.head(contextualized_embeddings[:, 0, :])</span><br><span class="line">        if labels is not None:</span><br><span class="line">            if self.config.problem_type is None:</span><br><span class="line">                if self.num_labels == 1:</span><br><span class="line">                    self.config.problem_type = "regression"</span><br><span class="line">                elif self.num_labels &gt; 1 and (labels.dtype == torch.long or labels.dtype == torch.int):</span><br><span class="line">                    self.config.problem_type = "single_label_classification"</span><br><span class="line">                else:</span><br><span class="line">                    self.config.problem_type = "multi_label_classification"</span><br><span class="line"></span><br><span class="line">    if self.config.problem_type == "regression":</span><br><span class="line">                loss_fct = nn.MSELoss()</span><br><span class="line">                if self.num_labels == 1:</span><br><span class="line">                    loss = loss_fct(logits.squeeze(), labels.squeeze())</span><br><span class="line">                else:</span><br><span class="line">                    loss = loss_fct(logits, labels)</span><br><span class="line">            elif self.config.problem_type == "single_label_classification":</span><br><span class="line">                loss_fct = nn.CrossEntropyLoss()</span><br><span class="line">                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))</span><br><span class="line">            elif self.config.problem_type == "multi_label_classification":</span><br><span class="line">                loss_fct = nn.BCEWithLogitsLoss()</span><br><span class="line">                loss = loss_fct(logits, labels)</span><br><span class="line"></span><br><span class="line">    assert output_attentions is None</span><br><span class="line">        assert output_hidden_states is None</span><br><span class="line">        return SequenceClassifierOutput(</span><br><span class="line">            loss=loss,</span><br><span class="line">            logits=logits,</span><br><span class="line">            hidden_states=contextualized_embeddings if output_hidden_states else None,</span><br><span class="line">            attentions=None</span><br><span class="line">        )</span><br></pre></td></tr></tbody></table></figure><p>这里hf框架会在配置中添加problem_type等内容。</p><h2 id="注册">注册</h2><p>如果在ckpt文件夹的 <code>config.json</code>里没有<code>auto_map</code>指明 <code>AutoClass</code>的注册：</p><figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">"auto_map"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">  <span class="attr">"AutoConfig"</span><span class="punctuation">:</span> <span class="string">"configuration_ltgbert.LtgBertConfig"</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">"AutoModelForMaskedLM"</span><span class="punctuation">:</span> <span class="string">"modeling_ltgbert.LtgBertForMaskedLM"</span></span><br><span class="line"><span class="punctuation">}</span></span><br></pre></td></tr></tbody></table></figure><p>那么需要手动添加，在读取ckpt的代码里添加：</p><figure class="highlight python-repl"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">AutoConfig.register("LtgBert", LtgBertConfig)</span><br><span class="line">AutoModelForMaskedLM.register(LtgBertConfig, LtgBertForMaskedLM)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>如果希望在保存模型的时候 <code>config.json</code>文件中自动包含<code>auto_map</code>，可以添加以下代码（如果模型是从ckpt里加载的就不需要添加）：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">LtgBertConfig.register_for_auto_class()</span><br><span class="line">LtgBertForMaskedLM.register_for_auto_class(<span class="string">"AutoModelForMaskedLM"</span>)</span><br></pre></td></tr></tbody></table></figure><p>后来发现只有注册可能会存在 <code>config.json</code>里<code>auto_map</code>不完整的情况（原因暂时没有调查），可以考虑直接在<code>config.__init__</code>里强制指定：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self,....):</span><br><span class="line">   ...</span><br><span class="line">   self.auto_map={</span><br><span class="line">    "AutoConfig": "configuration_ltgbert.LtgBertConfig",</span><br><span class="line">    "AutoModelForMaskedLM": "modeling_ltgbert.LtgBertForMaskedLM"</span><br><span class="line">   }</span><br></pre></td></tr></tbody></table></figure><h2 id="trainer">Trainer</h2><p>训练流程的转换主要设计HF的 <code>Trainer</code>类，可以参考<a href="https://huggingface.co/docs/transformers/main/zh/main_classes/trainer">Trainer(huggingface.co)</a>和<a href="https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments">Trainer(huggingface.co)</a>。</p><p><code>Trainer</code>把训练的流程分为几个过程，通过继承以及重写相关函数即可完成流程的定制，通过参数即可实现超参数的设置，细节阅读参考资料。</p><h2 id="dataset">Dataset</h2><p><code>dataset</code>可以继承自<code>torch.utils.data.dataset</code>，但是需要注意<code>__getitem__</code>，默认情况该函数返回的需要满足<code>dict</code>格式，从而实现参数的设置。</p><h2 id="参考资料">参考资料</h2><blockquote><ul><li><a href="https://huggingface.co/docs/transformers/custom_models#building-custom-models">Buildingcustom models (huggingface.co)</a></li><li><a href="https://huggingface.co/docs/transformers/main/zh/main_classes/trainer">Trainer(huggingface.co)</a></li><li><a href="https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments">Trainer(huggingface.co)</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;这篇博客主要介绍了怎么把一个已有的Pytorch代码转变成HF支持的格式，然后可以方便的放入HF代码流程中，并且使用一些HF的函数。代码转换主要涉及到以下几个方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Config&lt;/li&gt;
&lt;li&gt;Model</summary>
      
    
    
    
    <category term="模板" scheme="https://bg51717.github.io/wiki/categories/%E6%A8%A1%E6%9D%BF/"/>
    
    
    <category term="模板" scheme="https://bg51717.github.io/wiki/tags/%E6%A8%A1%E6%9D%BF/"/>
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="PyTorch" scheme="https://bg51717.github.io/wiki/tags/PyTorch/"/>
    
    <category term="HuggingFace" scheme="https://bg51717.github.io/wiki/tags/HuggingFace/"/>
    
    <category term="Trainer" scheme="https://bg51717.github.io/wiki/tags/Trainer/"/>
    
    <category term="config" scheme="https://bg51717.github.io/wiki/tags/config/"/>
    
    <category term="model" scheme="https://bg51717.github.io/wiki/tags/model/"/>
    
    <category term="dataset" scheme="https://bg51717.github.io/wiki/tags/dataset/"/>
    
  </entry>
  
  <entry>
    <title>随机数种子</title>
    <link href="https://bg51717.github.io/wiki/7369/"/>
    <id>https://bg51717.github.io/wiki/7369/</id>
    <published>2024-07-09T10:02:24.000Z</published>
    <updated>2024-11-02T02:55:49.881Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍">介绍</h1><p>在深度学习的实际项目中，为了减少随机性，增强项目的复现能力，设置固定随机数种子十分重要，因此这篇文章罗列了一些设置随机种子的方法和减少项目随机性的经验。</p><h1 id="通用函数">通用函数</h1><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">set_random_seed</span>(<span class="params">seed</span>):</span><br><span class="line">    <span class="string">"""Set random seeds."""</span></span><br><span class="line">    os.environ[<span class="string">'PYTHONHASHSEED'</span>] = <span class="built_in">str</span>(seed)</span><br><span class="line">    random.seed(seed)  <span class="comment"># 设置 Python 内置随机库的种子</span></span><br><span class="line">    np.random.seed(seed)  <span class="comment"># 设置 NumPy 随机库的种子</span></span><br><span class="line">    torch.manual_seed(seed)  <span class="comment"># 设置 PyTorch 随机库的种子</span></span><br><span class="line">    torch.cuda.manual_seed(seed)  <span class="comment"># 为当前 CUDA 设备设置种子</span></span><br><span class="line">    torch.cuda.manual_seed_all(seed)  <span class="comment"># 为所有 CUDA 设备设置种子</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h2 id="scikit-learn">Scikit-learn</h2><p>在 <code>Scikit-learn</code>中，部分算法需要设置<code>random_state</code>，比如聚类算法 <code>kmeans</code>。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KMeans(n_clusters=<span class="number">2</span>,random_state=<span class="number">42</span>)</span><br></pre></td></tr></tbody></table></figure><h1 id="工程经验">工程经验</h1><ol type="1"><li>由于部分原因，一些python数组或者python集合等，可能顺序也会影响结果的随机性。如果在无法确保顺序是固定的或者顺序是有要求的情况下，尝试对这些中间结果进行排序减少随机性。</li></ol><h1 id="参考资料">参考资料</h1><blockquote><ul><li><a href="https://blog.csdn.net/yangweipeng708/article/details/138793949">【Python】深度学习中随机数种子seed的种类和设置方式_seed设置-CSDN博客</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;介绍&quot;&gt;介绍&lt;/h1&gt;
&lt;p&gt;在深度学习的实际项目中，为了减少随机性，增强项目的复现能力，设置固定随机数种子十分重要，因此这篇文章罗列了一些设置随机种子的方法和减少项目随机性的经验。&lt;/p&gt;
&lt;h1 id=&quot;通用函数&quot;&gt;通用函数&lt;/h1&gt;
&lt;figure clas</summary>
      
    
    
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="工程细节" scheme="https://bg51717.github.io/wiki/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%B7%A5%E7%A8%8B%E7%BB%86%E8%8A%82/"/>
    
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="随机数" scheme="https://bg51717.github.io/wiki/tags/%E9%9A%8F%E6%9C%BA%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>vscode调试python</title>
    <link href="https://bg51717.github.io/wiki/30403/"/>
    <id>https://bg51717.github.io/wiki/30403/</id>
    <published>2024-04-19T14:47:51.000Z</published>
    <updated>2024-11-02T02:55:49.879Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p>在学习项目的过程中，很多时候需要通过调试来高效率的了解代码的执行过程，因此这里介绍下怎么使用vscode对python程序进行调试。</p><h2 id="方法一简单图标点击">方法一：简单图标点击</h2><p>vscode对一些简单的程序提供了一些可视化的调试方式，对于一些不需要指定参数等简单的调试功能，可以直接点击vscode左上角的几个图标进行debug过程。由于过于简单，此处不做介绍。</p><p><img src="/wiki/wiki/30403/1713538455137.png"></p><h2 id="方法二编辑launch.json文件">方法二：编辑launch.json文件</h2><p>在工作目录下的<code>./vscode/launch.json</code>文件里面，指定了各种debug和程序运行的参数、环境、解释器、目录等基本所有的环境配置。</p><p>可以在左下角的添加配置里面快速添加常见的选项。</p><p>比如下面所示：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">    // 使用 IntelliSense 了解相关属性。 </span><br><span class="line">    // 悬停以查看现有属性的描述。</span><br><span class="line">    // 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387</span><br><span class="line">    "version": "0.2.0",</span><br><span class="line">    "configurations": [</span><br><span class="line">        {</span><br><span class="line">            "name": "(gdb) 启动",</span><br><span class="line">            "type": "cppdbg",</span><br><span class="line">            "request": "launch",</span><br><span class="line">            "program": "${fileDirname}/${fileBasenameNoExtension}",</span><br><span class="line">            "args": [],</span><br><span class="line">            "stopAtEntry": false,</span><br><span class="line">            "cwd": "${fileDirname}",</span><br><span class="line">            "environment": [],</span><br><span class="line">            "externalConsole": false,</span><br><span class="line">            "MIMode": "gdb",</span><br><span class="line">            "setupCommands": [</span><br><span class="line">                {</span><br><span class="line">                    "description": "为 gdb 启用整齐打印",</span><br><span class="line">                    "text": "-enable-pretty-printing",</span><br><span class="line">                    "ignoreFailures": true</span><br><span class="line">                },</span><br><span class="line">                {</span><br><span class="line">                    "description": "将反汇编风格设置为 Intel",</span><br><span class="line">                    "text": "-gdb-set disassembly-flavor intel",</span><br><span class="line">                    "ignoreFailures": true</span><br><span class="line">                }</span><br><span class="line">            ]</span><br><span class="line">        },</span><br><span class="line">        {</span><br><span class="line">            "name": "py-dbg QLLM",</span><br><span class="line">            "type": "debugpy",</span><br><span class="line">            "request": "launch",</span><br><span class="line">            "python": "/home/bg51717/.conda/envs/QLLM/bin/python",</span><br><span class="line">            // "program": "/home/bg51717/project/QLLM/qllm/__main__.py",</span><br><span class="line">            "module": "qllm",</span><br><span class="line">            "console": "integratedTerminal",</span><br><span class="line">            "args":[</span><br><span class="line">                "--model=/home/bg51717/project/models/facebook/opt-350m",</span><br><span class="line">                "--method=gptq",</span><br><span class="line">                "--nsamples=64",</span><br><span class="line">                "--wbits=4",</span><br><span class="line">                "--groupsize=128",</span><br><span class="line">                "--save",</span><br><span class="line">                "/home/bg51717/project/QLLM/facebook/opt-350m_gptq4b",</span><br><span class="line">                "--export_onnx",</span><br><span class="line">                "/home/bg51717/project/QLLM/onnx_model/facebook/opt-350m_gptq4b"</span><br><span class="line">            ],</span><br><span class="line">            "env": {"PYTHONPATH": "/home/bg51717/project/QLLM/qllm"},</span><br><span class="line">            "cwd": "/home/bg51717/project/QLLM"</span><br><span class="line">        }</span><br><span class="line">    ]</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>这里参数非常多（建议使用的时候查询<code>gpt</code>、<code>搜索引擎</code>、<code>文档</code>等。</p><p>这里介绍几个常用的选项。此外，在编辑的时候可以类似<code>Linux</code>那样使用 <code>${fileDirname}</code>来引用<code>vscode</code>程序的当前变量比如工作目录</p><table><thead><tr><th>参数</th><th>含义</th><th>类型</th></tr></thead><tbody><tr><td>name</td><td>过程名字</td><td>str</td></tr><tr><td>type</td><td>过程类型</td><td>str</td></tr><tr><td>python</td><td>解释器（使用虚拟环境的时候需要注意指定</td><td>str</td></tr><tr><td>program</td><td>程序文件，按照脚本方式运行过程</td><td>str</td></tr><tr><td>module</td><td>模块名，按照模块方式运行过程</td><td>str</td></tr><tr><td>args</td><td>运行过程的参数</td><td>list</td></tr><tr><td>env</td><td>环境变量</td><td>dict</td></tr><tr><td>cwd</td><td>工作目录</td><td>str</td></tr></tbody></table><blockquote><p>此外，在使用的过程中python里面绝对引入、相对引入等。建议参考<a href="https://zhuanlan.zhihu.com/p/416867942">python相对导入常见问题和解决方案- 知乎 (zhihu.com)</a> 。</p><p>此处发现那里也有些没有提及的东西。</p><ul><li>解决方案错误一：ImportError: attempted <strong>relativeimport</strong> with no known parent package 里可以不修改代码，使用<code>python -m</code>命令+调整工作目录成功运行。（笔者当时遇到一个坑，当时没有注意的调试的是工程目录里的qllm文件还是env里装的py包</li></ul></blockquote><h2 id="方法三使用debug.py文件引入要调试的文件">方法三：使用debug.py文件引入要调试的文件</h2><p>如题，建立一个<code>debug.py</code>文件引入要调试的文件，类似于使用代理进行调试过程。</p><iframe src="https://player.bilibili.com/player.html?isOutside=true&amp;aid=656228835&amp;bvid=BV1ta4y1u78v&amp;cid=1129808210&amp;p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="width:100%;aspect-ratio:16/9;"></iframe><p>参考<a href="https://www.bilibili.com/video/BV1ta4y1u78v/?spm_id_from=333.337.search-card.all.click&amp;vd_source=6ca4c818110b061ac41dc3fcd0178f77">【Python入门】新手必会 vscode Debug 调试技巧_哔哩哔哩_bilibili</a></p><h2 id="参考资料">参考资料</h2><blockquote><ul><li><a href="https://zhuanlan.zhihu.com/p/416867942">python相对导入常见问题和解决方案- 知乎 (zhihu.com)</a></li><li><a href="https://www.bilibili.com/video/BV1ta4y1u78v/?spm_id_from=333.337.search-card.all.click&amp;vd_source=6ca4c818110b061ac41dc3fcd0178f77">【Python入门】新手必会 vscode Debug 调试技巧_哔哩哔哩_bilibili</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;在学习项目的过程中，很多时候需要通过调试来高效率的了解代码的执行过程，因此这里介绍下怎么使用vscode对python程序进行调试。&lt;/p&gt;
&lt;h2 id=&quot;方法一简单图标点击&quot;&gt;方法一：简单图标点击&lt;/h2&gt;
&lt;p&gt;vscode对一</summary>
      
    
    
    
    <category term="杂项" scheme="https://bg51717.github.io/wiki/categories/%E6%9D%82%E9%A1%B9/"/>
    
    
    <category term="vscode" scheme="https://bg51717.github.io/wiki/tags/vscode/"/>
    
    <category term="python" scheme="https://bg51717.github.io/wiki/tags/python/"/>
    
    <category term="调试" scheme="https://bg51717.github.io/wiki/tags/%E8%B0%83%E8%AF%95/"/>
    
    <category term="debug" scheme="https://bg51717.github.io/wiki/tags/debug/"/>
    
  </entry>
  
  <entry>
    <title>nlp常用排行榜</title>
    <link href="https://bg51717.github.io/wiki/63314/"/>
    <id>https://bg51717.github.io/wiki/63314/</id>
    <published>2024-04-05T06:56:57.000Z</published>
    <updated>2024-11-02T02:55:49.890Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p>在工作和学习的时候发现，很多时候挑选合适的模型和数据集等也是一个重要且麻烦的过程。发现有很多相关的评测的排行榜，根据这些实时更新的排行榜，可以辅助我们进行选择模型等前期工作。</p><p><a href="https://huggingface.co/spaces">Spaces - Hugging Face</a></p><p>这里罗列了许多关于ai的最新新闻，也能搜索到各种排行榜leaderboard。</p><h3 id="nlp任务">nlp任务</h3><p><a href="https://huggingface.co/spaces/mteb/leaderboard">MTEBLeaderboard - a Hugging Face Space by mteb</a></p><p>Massive Text Embedding Benchmark (MTEB)，是关于文本嵌入的排行榜，同时关注排行榜的like人数（从某种意义上反应排行榜的效用）。</p><h3 id="大模型评测">大模型评测</h3><p><a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">OpenLLM Leaderboard - a Hugging Face Space by HuggingFaceH4</a></p><p>这里提供了各种关于大模型在多维度的数据集上的表现能力，并且支持根据大模型的类型、精度等过滤大模型排行榜。</p><p><a href="https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard">BigCode Models Leaderboard - a Hugging Face Space by bigcode</a></p><p>这里提供了关于大模型code能力的排行榜。</p><p><a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard">LMSysChatbot Arena Leaderboard - a Hugging Face Space by lmsys</a></p><p>这里提供了关于大模型对话能力的排行榜（但是由于不知名原因暂时无法打开）。</p><p><a href="https://chat.lmsys.org/">Chat with Open Large LanguageModels (lmsys.org)</a></p><p>这里是关于大模型对话能力的测评网站，也提供了参考用的排行榜。</p><p><a href="https://huggingface.co/spaces/optimum/llm-perf-leaderboard">LLM-PerfLeaderboard - a Hugging Face Space by optimum</a></p><p>这里提供了大模型在给定硬件条件的训练资源后微调的性能排行榜。</p><p><a href="https://huggingface.co/spaces/logikon/open_cot_leaderboard">OpenCoT Leaderboard - a Hugging Face Space by logikon</a></p><p>这里提供了关于大模型CoT（Chain of Thought）的排行榜。</p><h3 id="数据集">数据集</h3><h2 id="参考资料">参考资料</h2><blockquote><ul><li><a href=""></a></li><li><a href=""></a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;在工作和学习的时候发现，很多时候挑选合适的模型和数据集等也是一个重要且麻烦的过程。发现有很多相关的评测的排行榜，根据这些实时更新的排行榜，可以辅助我们进行选择模型等前期工作。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://huggi</summary>
      
    
    
    
    <category term="深度学习" scheme="https://bg51717.github.io/wiki/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="自然语言处理" scheme="https://bg51717.github.io/wiki/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
  </entry>
  
</feed>
